{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "44bcd4ac7a054565a3c9fee1cf7d189c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9208bed5d90c4068bd46b7dd39ee74cd",
              "IPY_MODEL_1a05e99ede3848e0ac15c86d406abd74",
              "IPY_MODEL_44a67d33f414439f9cdb72618c82c167"
            ],
            "layout": "IPY_MODEL_fe053619003b485e8c0db9d47eae37a1"
          }
        },
        "9208bed5d90c4068bd46b7dd39ee74cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1497230283ef41b096d5937a37d92798",
            "placeholder": "​",
            "style": "IPY_MODEL_1f78f89fd3be4361aacaa2cec929af61",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "1a05e99ede3848e0ac15c86d406abd74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d6ca37da089425f845e6c95e01dda7b",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_280ca89e2ff846d79a4d4ae90841baa1",
            "value": 1042301
          }
        },
        "44a67d33f414439f9cdb72618c82c167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e126517b5214604ac4a409c8209ae78",
            "placeholder": "​",
            "style": "IPY_MODEL_a7f5a3b153264d8cba1ceabf16c6364d",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 13.7MB/s]"
          }
        },
        "fe053619003b485e8c0db9d47eae37a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1497230283ef41b096d5937a37d92798": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f78f89fd3be4361aacaa2cec929af61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d6ca37da089425f845e6c95e01dda7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "280ca89e2ff846d79a4d4ae90841baa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e126517b5214604ac4a409c8209ae78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7f5a3b153264d8cba1ceabf16c6364d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f8891e368fa416297f34e4fa6079239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8e652b704ee422ebf980876d07f787b",
              "IPY_MODEL_fbaac7e3917e49c98a2141253aeddd12",
              "IPY_MODEL_8d89e6db9d0e4f4d9905bd9e6a98039a"
            ],
            "layout": "IPY_MODEL_37671bd935984a84b8ec8efa2113196c"
          }
        },
        "b8e652b704ee422ebf980876d07f787b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a58bb6fe3f95490ebdef73b0ee35863f",
            "placeholder": "​",
            "style": "IPY_MODEL_4e99de75f48b4ccaa8c599c2e6131d9d",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "fbaac7e3917e49c98a2141253aeddd12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37a6ad6beafd4cd7b4e22cc07c70dee4",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47c253d72166426db32268b689d0117e",
            "value": 456318
          }
        },
        "8d89e6db9d0e4f4d9905bd9e6a98039a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b2ee4526bd64ed2bcfcd1c991b98b82",
            "placeholder": "​",
            "style": "IPY_MODEL_1e4e8b56a76f4bd28c6c96494ee7a068",
            "value": " 456k/456k [00:00&lt;00:00, 6.54MB/s]"
          }
        },
        "37671bd935984a84b8ec8efa2113196c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a58bb6fe3f95490ebdef73b0ee35863f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e99de75f48b4ccaa8c599c2e6131d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37a6ad6beafd4cd7b4e22cc07c70dee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47c253d72166426db32268b689d0117e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b2ee4526bd64ed2bcfcd1c991b98b82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e4e8b56a76f4bd28c6c96494ee7a068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf1f3a2f719f49618f5d640751936949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6958b6934f04a16bf14bd4243b7bd10",
              "IPY_MODEL_2e9554cc1ce0420cadbfdc746d3183d7",
              "IPY_MODEL_40b9b2ee7b084a3d8ca9c02cd937b8a3"
            ],
            "layout": "IPY_MODEL_8c8ba814baf742a58ccccc4991278c4e"
          }
        },
        "e6958b6934f04a16bf14bd4243b7bd10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1696fde975fc4496b44b43048976cba8",
            "placeholder": "​",
            "style": "IPY_MODEL_903f4aaf63c04c56ba012e83dacdb351",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "2e9554cc1ce0420cadbfdc746d3183d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b15ee5152c8491e84cd617be92a938e",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83acd5aadf5042889eac87de185743a3",
            "value": 665
          }
        },
        "40b9b2ee7b084a3d8ca9c02cd937b8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_838ec6ab80e041508dd829711aad2edb",
            "placeholder": "​",
            "style": "IPY_MODEL_6ccc5394d3aa4dc791fb69ffe746cad2",
            "value": " 665/665 [00:00&lt;00:00, 36.8kB/s]"
          }
        },
        "8c8ba814baf742a58ccccc4991278c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1696fde975fc4496b44b43048976cba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "903f4aaf63c04c56ba012e83dacdb351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b15ee5152c8491e84cd617be92a938e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83acd5aadf5042889eac87de185743a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "838ec6ab80e041508dd829711aad2edb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ccc5394d3aa4dc791fb69ffe746cad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xl4v7JZS2-YW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRSbllbH361U",
        "outputId": "d19beefd-1c88-437a-a3c1-619c284f367f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_a_data = pd.read_csv('/content/drive/MyDrive/Books/Ai Writting/Conversation/final_Chat_Data_GBC.csv')\n",
        "q_a_data = q_a_data.drop(\"sentiment\",axis=1)\n",
        "persona_data = pd.read_csv('/content/drive/MyDrive/Books/Ai Writting/Conversation/personality.csv')\n",
        "persona_data = persona_data.drop(\"Unnamed: 0\",axis = 1)"
      ],
      "metadata": {
        "id": "Z6_Q8NxB38OH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_a_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ba3lk-ON4fri",
        "outputId": "79c93458-3365-44bd-ae7d-a7c426842301"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              question  \\\n",
              "0               hi, how are you doing?   \n",
              "1        i'm fine. how about yourself?   \n",
              "2  i'm pretty good. thanks for asking.   \n",
              "3    no problem. so how have you been?   \n",
              "4     i've been great. what about you?   \n",
              "\n",
              "                                     answer  \n",
              "0             i'm fine. how about yourself?  \n",
              "1       i'm pretty good. thanks for asking.  \n",
              "2         no problem. so how have you been?  \n",
              "3          i've been great. what about you?  \n",
              "4  i've been good. i'm in school right now.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf6e9a65-9284-4085-9d1a-d1e27c296917\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hi, how are you doing?</td>\n",
              "      <td>i'm fine. how about yourself?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i'm fine. how about yourself?</td>\n",
              "      <td>i'm pretty good. thanks for asking.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i'm pretty good. thanks for asking.</td>\n",
              "      <td>no problem. so how have you been?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>no problem. so how have you been?</td>\n",
              "      <td>i've been great. what about you?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i've been great. what about you?</td>\n",
              "      <td>i've been good. i'm in school right now.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf6e9a65-9284-4085-9d1a-d1e27c296917')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bf6e9a65-9284-4085-9d1a-d1e27c296917 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bf6e9a65-9284-4085-9d1a-d1e27c296917');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "persona_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1DmaWQCo4iA5",
        "outputId": "e34a8693-d406-486c-f105-604e269b768b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Persona  \\\n",
              "0   i like to remodel homes. i like to go hunting...   \n",
              "1   my mom is my best friend. i have four sisters...   \n",
              "2   i had a gig at local theater last night. i wo...   \n",
              "3   i am very athletic. i wear contacts. i have b...   \n",
              "4   i am primarily a meat eater. i am a guitar pl...   \n",
              "\n",
              "                                                chat  \n",
              "0  hi , how are you doing ? i am getting ready to...  \n",
              "1  hi , how are you doing today ?\\ni am spending ...  \n",
              "2  we all live in a yellow submarine , a yellow s...  \n",
              "3  hi ! i work as a gourmet cook .\\ni do not like...  \n",
              "4  how are you doing today\\nwhat do you do for ca...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b4a564f-4582-4dd5-8771-b42ebfca97f1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Persona</th>\n",
              "      <th>chat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i like to remodel homes. i like to go hunting...</td>\n",
              "      <td>hi , how are you doing ? i am getting ready to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>my mom is my best friend. i have four sisters...</td>\n",
              "      <td>hi , how are you doing today ?\\ni am spending ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i had a gig at local theater last night. i wo...</td>\n",
              "      <td>we all live in a yellow submarine , a yellow s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i am very athletic. i wear contacts. i have b...</td>\n",
              "      <td>hi ! i work as a gourmet cook .\\ni do not like...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i am primarily a meat eater. i am a guitar pl...</td>\n",
              "      <td>how are you doing today\\nwhat do you do for ca...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b4a564f-4582-4dd5-8771-b42ebfca97f1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b4a564f-4582-4dd5-8771-b42ebfca97f1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b4a564f-4582-4dd5-8771-b42ebfca97f1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = q_a_data\n",
        "data2 = persona_data"
      ],
      "metadata": {
        "id": "PyavIm0E6WU9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqYs9bTc7ckT",
        "outputId": "49bb97a2-f4b4-4977-83e1-bc07bf97eabc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Persona    0\n",
              "chat       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Select relevant columns from the datasets\n",
        "data1_selected = data1[[\"question\", \"answer\"]]\n",
        "data2_selected = data2[[\"Persona\", \"chat\"]]\n",
        "\n",
        "# Join question and persona, answer and chat into single columns\n",
        "data1_selected[\"text\"] = data1_selected[\"question\"]\n",
        "data1_selected.loc[len(data1_selected)-1, \"text\"] += \" \" + data2_selected[\"Persona\"].values[0]\n",
        "data1_selected[\"response\"] = data1_selected[\"answer\"]\n",
        "data1_selected.loc[len(data1_selected)-1, \"response\"] += \" \" + data2_selected[\"chat\"].values[0]\n",
        "\n",
        "# Drop unnecessary columns\n",
        "data1_selected = data1_selected[[\"text\", \"response\"]]\n",
        "\n",
        "# Concatenate the datasets\n",
        "combined_data = pd.concat([data1_selected, data2_selected.iloc[1:]])\n",
        "\n",
        "# Reset the index of the combined dataset\n",
        "combined_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Replace NaN values with empty string\n",
        "combined_data.fillna(\"\", inplace=True)\n",
        "\n",
        "combined_data['User'] = combined_data['text'] + combined_data['Persona']\n",
        "combined_data['Reply'] = combined_data['response'] + combined_data['chat']"
      ],
      "metadata": {
        "id": "TjnD4xsY4ktG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_drop = [\"text\" ,\"response\", \"Persona\",\"chat\"]\n",
        "combined_data = combined_data.drop(cols_drop,axis=1)\n",
        "combined_data.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "B8SzE8U36aqm",
        "outputId": "8e4c9db5-b486-4ab7-ff08-f59658e34f69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    User  \\\n",
              "12658   my favorite food is fried chicken. i have a t...   \n",
              "12659   i love fast food. i am a stay at home mom. i ...   \n",
              "12660   my family and i go camping every month. i am ...   \n",
              "12661   i have red hair. i work at a retail store. i ...   \n",
              "12662   i m applying for publishing jobs. the only au...   \n",
              "\n",
              "                                                   Reply  \n",
              "12658  good evening , i have been having a terrible t...  \n",
              "12659  hi there how is work\\ni do not work these days...  \n",
              "12660  hello , i have a daughter who is in kindergart...  \n",
              "12661  hi . how are you doing ?\\ni am doing well . ju...  \n",
              "12662  hello ! how are you today ?\\nhello i am good ....  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90125d63-4c34-4040-b0dc-5767b1c03af1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Reply</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12658</th>\n",
              "      <td>my favorite food is fried chicken. i have a t...</td>\n",
              "      <td>good evening , i have been having a terrible t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12659</th>\n",
              "      <td>i love fast food. i am a stay at home mom. i ...</td>\n",
              "      <td>hi there how is work\\ni do not work these days...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12660</th>\n",
              "      <td>my family and i go camping every month. i am ...</td>\n",
              "      <td>hello , i have a daughter who is in kindergart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12661</th>\n",
              "      <td>i have red hair. i work at a retail store. i ...</td>\n",
              "      <td>hi . how are you doing ?\\ni am doing well . ju...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12662</th>\n",
              "      <td>i m applying for publishing jobs. the only au...</td>\n",
              "      <td>hello ! how are you today ?\\nhello i am good ....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90125d63-4c34-4040-b0dc-5767b1c03af1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90125d63-4c34-4040-b0dc-5767b1c03af1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90125d63-4c34-4040-b0dc-5767b1c03af1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.to_csv('Final_OUT_Conv_Data.csv')"
      ],
      "metadata": {
        "id": "SczQFQuv6tXZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre Processing Dataset"
      ],
      "metadata": {
        "id": "a7avCoYaCeUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Cleaning:** Remove any unnecessary characters, symbols, or special characters that do not contribute to the meaning of the text. This includes removing HTML tags, URLs, or any noise specific to your dataset.\n",
        "\n",
        "**Lowercasing:** Convert the text to lowercase to ensure consistency and avoid treating words with different casing as different entities.\n",
        "\n",
        "**Tokenization:** Split the text into individual tokens, which can be words or subword units. Tokenization helps the model understand the structure of the text and build its vocabulary.\n",
        "\n",
        "**Special Tokens:** Add special tokens to denote the beginning and end of sentences or prompts, as well as special tokens for padding or masking if necessary.\n",
        "\n",
        "**Sequence Length Limit:** Set a maximum sequence length for the input data. If a sequence exceeds this limit, it needs to be truncated or split into smaller segments to fit within the model's memory constraints.\n",
        "\n",
        "**Handling Long Texts:** For long texts that cannot fit into a single sequence, you can use techniques like sliding window or recursive encoding to process the text in smaller chunks.\n",
        "\n",
        "**Data Encoding:** Map the tokens to their corresponding integer IDs according to the model's vocabulary. This step transforms the text data into a numerical representation that can be processed by the model.\n",
        "\n",
        "**Data Formatting:** Organize the encoded data into input-output pairs suitable for the model. For example, for autoregressive models like GPT, the input sequence can be followed by the target sequence shifted by one position.\n",
        "\n",
        "**Masking:** Apply attention masks or padding masks to indicate which tokens are valid input and which should be ignored during training. This helps the model focus on relevant tokens and handle varying sequence lengths.\n",
        "\n",
        "**Data Batching:** Group the input-output pairs into batches for more efficient training. Batching allows parallel processing and improves training speed."
      ],
      "metadata": {
        "id": "xQH1SfxmCiYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnCp4rR2C6bl",
        "outputId": "8fa9eac5-2395-4f28-bad8-e45f7c041e3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer"
      ],
      "metadata": {
        "id": "HxcEKC327Wg0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = combined_data"
      ],
      "metadata": {
        "id": "tN-j7qsnEpMg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cleaning Text\n",
        "\n",
        "\n",
        "1. Removing leading and trailing spaces using the strip method.\n",
        "\n",
        "2. Removing HTML tags using a regular expression pattern (<.*?>) and the re.sub function.\n",
        "\n",
        "3. Removing URLs using a regular expression pattern (http\\S+|www\\S+|https\\S+) and the re.sub function.\n",
        "\n",
        "4. Removing special characters or symbols using a regular expression pattern ([^a-zA-Z0-9\\s]) and the re.sub function.\n",
        "\n",
        "5. Normalizing whitespace using a regular expression pattern (\\s+) and the re.sub function.\n",
        "\n",
        "6. Replacing multiple consecutive punctuation marks (e.g., \"!!!\" or \"??\") with a single one using a regular expression pattern (([!?.]){2,}) and the re.sub function.\n",
        "\n",
        "7. Replacing numbers with a special token (e.g., \"NUM\") using a regular expression pattern (\\d+) and the re.sub function.\n",
        "\n",
        "8. Lowering the text to small letters\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nmSw75WmDuGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove leading/trailing spaces\n",
        "    text = text.strip()\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    # Remove special characters or symbols\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    # Replace multiple consecutive punctuation marks with a single one\n",
        "    text = re.sub(r\"([!?.]){2,}\", r\"\\1\", text)\n",
        "    # Replace numbers with a special token\n",
        "    text = re.sub(r\"\\d+\", \"NUM\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning to \"User\" and \"Reply\" columns\n",
        "data[\"User\"] = data[\"User\"].apply(clean_text)\n",
        "data[\"Reply\"] = data[\"Reply\"].apply(clean_text)\n",
        "\n",
        "# Step 2: Lowercasing\n",
        "data[\"User\"] = data[\"User\"].str.lower()\n",
        "data[\"Reply\"] = data[\"Reply\"].str.lower()\n"
      ],
      "metadata": {
        "id": "qYJtIlYDDtzJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ],
      "metadata": {
        "id": "rBKFufhGE_6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "data[\"User_tokens\"] = data[\"User\"].apply(tokenizer.tokenize)\n",
        "data[\"Reply_tokens\"] = data[\"Reply\"].apply(tokenizer.tokenize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "44bcd4ac7a054565a3c9fee1cf7d189c",
            "9208bed5d90c4068bd46b7dd39ee74cd",
            "1a05e99ede3848e0ac15c86d406abd74",
            "44a67d33f414439f9cdb72618c82c167",
            "fe053619003b485e8c0db9d47eae37a1",
            "1497230283ef41b096d5937a37d92798",
            "1f78f89fd3be4361aacaa2cec929af61",
            "2d6ca37da089425f845e6c95e01dda7b",
            "280ca89e2ff846d79a4d4ae90841baa1",
            "4e126517b5214604ac4a409c8209ae78",
            "a7f5a3b153264d8cba1ceabf16c6364d",
            "5f8891e368fa416297f34e4fa6079239",
            "b8e652b704ee422ebf980876d07f787b",
            "fbaac7e3917e49c98a2141253aeddd12",
            "8d89e6db9d0e4f4d9905bd9e6a98039a",
            "37671bd935984a84b8ec8efa2113196c",
            "a58bb6fe3f95490ebdef73b0ee35863f",
            "4e99de75f48b4ccaa8c599c2e6131d9d",
            "37a6ad6beafd4cd7b4e22cc07c70dee4",
            "47c253d72166426db32268b689d0117e",
            "8b2ee4526bd64ed2bcfcd1c991b98b82",
            "1e4e8b56a76f4bd28c6c96494ee7a068",
            "bf1f3a2f719f49618f5d640751936949",
            "e6958b6934f04a16bf14bd4243b7bd10",
            "2e9554cc1ce0420cadbfdc746d3183d7",
            "40b9b2ee7b084a3d8ca9c02cd937b8a3",
            "8c8ba814baf742a58ccccc4991278c4e",
            "1696fde975fc4496b44b43048976cba8",
            "903f4aaf63c04c56ba012e83dacdb351",
            "0b15ee5152c8491e84cd617be92a938e",
            "83acd5aadf5042889eac87de185743a3",
            "838ec6ab80e041508dd829711aad2edb",
            "6ccc5394d3aa4dc791fb69ffe746cad2"
          ]
        },
        "id": "DSdCG5IPDZCR",
        "outputId": "55b5ba4b-ebab-46d3-c2fa-b9dc2227258c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44bcd4ac7a054565a3c9fee1cf7d189c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f8891e368fa416297f34e4fa6079239"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf1f3a2f719f49618f5d640751936949"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Special Tokens\n",
        "special_tokens = {\n",
        "    \"bos_token\": \"<bos>\",\n",
        "    \"eos_token\": \"<eos>\",\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"mask_token\": \"<mask>\"\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5h_4Us-DZOc",
        "outputId": "0b00f792-9ff8-4b14-e062-4ddc43c86a51"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Data Encoding\n",
        "data[\"User_ids\"] = data[\"User_tokens\"].apply(tokenizer.convert_tokens_to_ids)\n",
        "data[\"Reply_ids\"] = data[\"Reply_tokens\"].apply(tokenizer.convert_tokens_to_ids)"
      ],
      "metadata": {
        "id": "JdfXwpgDFQYA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Data Formatting\n",
        "formatted_data = [(user_ids, reply_ids) for user_ids, reply_ids in zip(data[\"User_ids\"], data[\"Reply_ids\"])]"
      ],
      "metadata": {
        "id": "2_fnIh1RF562"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Masking\n",
        "attention_mask = [[1] * len(user_ids) for user_ids in data[\"User_ids\"]]"
      ],
      "metadata": {
        "id": "ByxOxMC_F8xa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Data Batching\n",
        "batch_size = 16  # Set the desired batch size"
      ],
      "metadata": {
        "id": "hMoUTNMxGAZf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences to a fixed length\n",
        "padded_input_ids = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids, _ in formatted_data], batch_first=True)\n",
        "padded_attention_mask = torch.nn.utils.rnn.pad_sequence([torch.tensor(mask) for _, mask in zip(data[\"User_ids\"], attention_mask)], batch_first=True)"
      ],
      "metadata": {
        "id": "7bUm2ZlSGXUL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the padded sequences and attention masks into PyTorch tensors\n",
        "input_ids = torch.tensor(padded_input_ids)\n",
        "attention_mask = torch.tensor(padded_attention_mask)\n",
        "\n",
        "# Create data batches\n",
        "data_batches = torch.utils.data.TensorDataset(input_ids, attention_mask)\n",
        "data_loader = torch.utils.data.DataLoader(data_batches, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGRAQYRsGEum",
        "outputId": "27c7b71d-d352-4d27-caa2-c440081c920c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-c5607bf1755d>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_ids = torch.tensor(padded_input_ids)\n",
            "<ipython-input-19-c5607bf1755d>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  attention_mask = torch.tensor(padded_attention_mask)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary Code"
      ],
      "metadata": {
        "id": "cJnUOdViHiFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Step 1: Text Cleaning\n",
        "# Step 2: Lowercasing\n",
        "# Step 3: Tokenization\n",
        "# Step 4: Special Tokens\n",
        "# Step 5: Sequence Length Limit\n",
        "# Step 6: Data Encoding\n",
        "formatted_data = []\n",
        "attention_mask = []\n",
        "\n",
        "for user_text, reply_text in zip(data[\"User\"], data[\"Reply\"]):\n",
        "    # Text Cleaning\n",
        "    user_text = clean_text(user_text)\n",
        "    reply_text = clean_text(reply_text)\n",
        "\n",
        "    # Lowercasing\n",
        "    user_text = user_text.lower()\n",
        "    reply_text = reply_text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    user_tokens = tokenizer.tokenize(user_text)\n",
        "    reply_tokens = tokenizer.tokenize(reply_text)\n",
        "\n",
        "    # Special Tokens\n",
        "    user_tokens = [tokenizer.bos_token] + user_tokens + [tokenizer.eos_token]\n",
        "    reply_tokens = [tokenizer.bos_token] + reply_tokens + [tokenizer.eos_token]\n",
        "\n",
        "    # Sequence Length Limit\n",
        "    max_sequence_length = 128  # Set the maximum sequence length\n",
        "    user_tokens = user_tokens[:max_sequence_length]\n",
        "    reply_tokens = reply_tokens[:max_sequence_length]\n",
        "\n",
        "    # Data Encoding\n",
        "    user_ids = tokenizer.convert_tokens_to_ids(user_tokens)\n",
        "    reply_ids = tokenizer.convert_tokens_to_ids(reply_tokens)\n",
        "\n",
        "    formatted_data.append((user_ids, reply_ids))\n",
        "    attention_mask.append([1] * len(user_ids))\n",
        "\n",
        "# # Step 7: Data Formatting\n",
        "# input_ids = pad_sequence([torch.tensor(user_ids) for user_ids, _ in formatted_data], batch_first=True)\n",
        "# target_ids = pad_sequence([torch.tensor(reply_ids) for _, reply_ids in formatted_data], batch_first=True)\n",
        "# attention_mask = pad_sequence([torch.tensor(mask) for mask in attention_mask], batch_first=True)\n",
        "\n",
        "# Step 7: Data Formatting\n",
        "input_ids = pad_sequence([torch.tensor(user_ids) for user_ids, _ in formatted_data], batch_first=True, padding_value=0.0)  # Set padding value to 0.0\n",
        "target_ids = pad_sequence([torch.tensor(reply_ids) for _, reply_ids in formatted_data], batch_first=True, padding_value=0.0)  # Set padding value to 0.0\n",
        "attention_mask = pad_sequence([torch.tensor(mask) for mask in attention_mask], batch_first=True)\n",
        "\n",
        "\n",
        "# Step 8: Masking\n",
        "attention_mask = attention_mask.bool()\n",
        "\n",
        "# Step 9: Data Batching\n",
        "batch_size = 16  # Set the desired batch size\n",
        "\n",
        "# Create data batches\n",
        "data_batches = torch.utils.data.TensorDataset(input_ids, attention_mask, target_ids)\n",
        "data_loader = torch.utils.data.DataLoader(data_batches, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Z5oQ7b-lDZiu"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Building"
      ],
      "metadata": {
        "id": "Zqzz9LPQIFVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "VCAvfTeFINbJ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "# Set the device for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Set the model to the desired device\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "IAnb_29vHUwA"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 3"
      ],
      "metadata": {
        "id": "xgoV3lbbITnz"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set optimizer and learning rate scheduler\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
      ],
      "metadata": {
        "id": "9xp5w_JlIXzs"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    for batch_idx, (input_ids, attention_mask, target_ids) in enumerate(data_loader):\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    average_loss = total_loss / total_batches\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {average_loss}\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save_pretrained(\"trained_model\")\n",
        "tokenizer.save_pretrained(\"trained_model\")"
      ],
      "metadata": {
        "id": "EMXNkP6gIXrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "average_loss_p = []\n",
        "\n",
        "# Initialize the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Step 1: Text Cleaning\n",
        "# Step 2: Lowercasing\n",
        "# Step 3: Tokenization\n",
        "# Step 4: Special Tokens\n",
        "# Step 5: Sequence Length Limit\n",
        "# Step 6: Data Encoding\n",
        "formatted_data = []\n",
        "attention_mask = []\n",
        "\n",
        "for user_text, reply_text in zip(data[\"User\"], data[\"Reply\"]):\n",
        "    # Text Cleaning\n",
        "    user_text = clean_text(user_text)\n",
        "    reply_text = clean_text(reply_text)\n",
        "\n",
        "    # Lowercasing\n",
        "    user_text = user_text.lower()\n",
        "    reply_text = reply_text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    user_tokens = tokenizer.tokenize(user_text)\n",
        "    reply_tokens = tokenizer.tokenize(reply_text)\n",
        "\n",
        "    # Special Tokens\n",
        "    user_tokens = [tokenizer.bos_token] + user_tokens + [tokenizer.eos_token]\n",
        "    reply_tokens = [tokenizer.bos_token] + reply_tokens + [tokenizer.eos_token]\n",
        "\n",
        "    # Sequence Length Limit\n",
        "    max_sequence_length = 60 # Set the maximum sequence length\n",
        "    user_tokens = user_tokens[:max_sequence_length]\n",
        "    reply_tokens = reply_tokens[:max_sequence_length]\n",
        "\n",
        "    # Data Encoding\n",
        "    user_ids = tokenizer.convert_tokens_to_ids(user_tokens)\n",
        "    reply_ids = tokenizer.convert_tokens_to_ids(reply_tokens)\n",
        "\n",
        "    formatted_data.append((user_ids, reply_ids))\n",
        "    attention_mask.append([1] * len(user_ids))\n",
        "\n",
        "# Step 7: Data Formatting\n",
        "input_ids = pad_sequence([torch.tensor(user_ids) for user_ids, _ in formatted_data], batch_first=True, padding_value=0.0)  # Set padding value to 0.0\n",
        "target_ids = pad_sequence([torch.tensor(reply_ids) for _, reply_ids in formatted_data], batch_first=True, padding_value=0.0)  # Set padding value to 0.0\n",
        "attention_mask = pad_sequence([torch.tensor(mask) for mask in attention_mask], batch_first=True)\n",
        "\n",
        "# Step 8: Masking\n",
        "attention_mask = attention_mask.bool()\n",
        "\n",
        "# Filter out incomplete batches\n",
        "complete_batches = [idx for idx, (input_batch, target_batch) in enumerate(zip(input_ids, target_ids)) if input_batch.size(0) == target_batch.size(0)]\n",
        "input_ids = input_ids[complete_batches]\n",
        "target_ids = target_ids[complete_batches]\n",
        "attention_mask = attention_mask[complete_batches]\n",
        "\n",
        "# Step 9: Data Batching\n",
        "batch_size = 64  # Set the desired batch size\n",
        "\n",
        "if len(input_ids) > 0:\n",
        "    # Create data batches\n",
        "    data_batches = torch.utils.data.TensorDataset(input_ids, attention_mask, target_ids)\n",
        "    data_loader = torch.utils.data.DataLoader(data_batches, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Load the pre-trained GPT-2 model and tokenizer\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    # Set the device for training\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Set the model to the desired device\n",
        "    model = model.to(device)\n",
        "    # Set hyperparameters\n",
        "    learning_rate = 1e-4\n",
        "    num_epochs = 50\n",
        "    # Set optimizer and learning rate scheduler\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "\n",
        "   # Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    for batch_idx, (input_ids, attention_mask, target_ids) in enumerate(data_loader):\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx+1}/{len(data_loader)} | Loss: {loss.item()}\")\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    average_loss = total_loss / total_batches\n",
        "    average_loss_p.append(average_loss)\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Average Loss: {average_loss}\")\n",
        "\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save_pretrained(\"trained_model\")\n",
        "    tokenizer.save_pretrained(\"trained_model\")\n",
        "else:\n",
        "    print(\"No complete batches remaining after filtering.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VPu7kQhIXkH",
        "outputId": "e63b6f87-12a0-415a-df0c-c349598a0a24"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 25/50 | Batch 176/198 | Loss: 1.9287289381027222\n",
            "Epoch 25/50 | Batch 177/198 | Loss: 1.6140964031219482\n",
            "Epoch 25/50 | Batch 178/198 | Loss: 2.109032154083252\n",
            "Epoch 25/50 | Batch 179/198 | Loss: 1.8018094301223755\n",
            "Epoch 25/50 | Batch 180/198 | Loss: 2.0459835529327393\n",
            "Epoch 25/50 | Batch 181/198 | Loss: 1.8262004852294922\n",
            "Epoch 25/50 | Batch 182/198 | Loss: 1.6854921579360962\n",
            "Epoch 25/50 | Batch 183/198 | Loss: 1.6793545484542847\n",
            "Epoch 25/50 | Batch 184/198 | Loss: 1.9061262607574463\n",
            "Epoch 25/50 | Batch 185/198 | Loss: 2.0613205432891846\n",
            "Epoch 25/50 | Batch 186/198 | Loss: 1.9465641975402832\n",
            "Epoch 25/50 | Batch 187/198 | Loss: 1.816077709197998\n",
            "Epoch 25/50 | Batch 188/198 | Loss: 1.9491405487060547\n",
            "Epoch 25/50 | Batch 189/198 | Loss: 1.8865643739700317\n",
            "Epoch 25/50 | Batch 190/198 | Loss: 1.8258165121078491\n",
            "Epoch 25/50 | Batch 191/198 | Loss: 1.6907621622085571\n",
            "Epoch 25/50 | Batch 192/198 | Loss: 1.7629475593566895\n",
            "Epoch 25/50 | Batch 193/198 | Loss: 1.8357821702957153\n",
            "Epoch 25/50 | Batch 194/198 | Loss: 1.8504880666732788\n",
            "Epoch 25/50 | Batch 195/198 | Loss: 1.6689456701278687\n",
            "Epoch 25/50 | Batch 196/198 | Loss: 1.8366999626159668\n",
            "Epoch 25/50 | Batch 197/198 | Loss: 1.776334285736084\n",
            "Epoch 25/50 | Batch 198/198 | Loss: 1.836360216140747\n",
            "Epoch 25/50 | Average Loss: 1.8180645872848202\n",
            "Epoch 26/50 | Batch 1/198 | Loss: 1.812587022781372\n",
            "Epoch 26/50 | Batch 2/198 | Loss: 1.7139129638671875\n",
            "Epoch 26/50 | Batch 3/198 | Loss: 1.7218724489212036\n",
            "Epoch 26/50 | Batch 4/198 | Loss: 1.7358180284500122\n",
            "Epoch 26/50 | Batch 5/198 | Loss: 1.8010996580123901\n",
            "Epoch 26/50 | Batch 6/198 | Loss: 1.5861375331878662\n",
            "Epoch 26/50 | Batch 7/198 | Loss: 1.8070672750473022\n",
            "Epoch 26/50 | Batch 8/198 | Loss: 1.733532190322876\n",
            "Epoch 26/50 | Batch 9/198 | Loss: 1.7500180006027222\n",
            "Epoch 26/50 | Batch 10/198 | Loss: 1.630949854850769\n",
            "Epoch 26/50 | Batch 11/198 | Loss: 1.7822967767715454\n",
            "Epoch 26/50 | Batch 12/198 | Loss: 1.7430161237716675\n",
            "Epoch 26/50 | Batch 13/198 | Loss: 1.7437081336975098\n",
            "Epoch 26/50 | Batch 14/198 | Loss: 1.8567779064178467\n",
            "Epoch 26/50 | Batch 15/198 | Loss: 1.7346866130828857\n",
            "Epoch 26/50 | Batch 16/198 | Loss: 1.568902850151062\n",
            "Epoch 26/50 | Batch 17/198 | Loss: 1.5829143524169922\n",
            "Epoch 26/50 | Batch 18/198 | Loss: 1.5982954502105713\n",
            "Epoch 26/50 | Batch 19/198 | Loss: 1.8075766563415527\n",
            "Epoch 26/50 | Batch 20/198 | Loss: 1.85101318359375\n",
            "Epoch 26/50 | Batch 21/198 | Loss: 1.9558228254318237\n",
            "Epoch 26/50 | Batch 22/198 | Loss: 1.5520529747009277\n",
            "Epoch 26/50 | Batch 23/198 | Loss: 1.6202973127365112\n",
            "Epoch 26/50 | Batch 24/198 | Loss: 1.5718311071395874\n",
            "Epoch 26/50 | Batch 25/198 | Loss: 1.7573628425598145\n",
            "Epoch 26/50 | Batch 26/198 | Loss: 1.7601286172866821\n",
            "Epoch 26/50 | Batch 27/198 | Loss: 1.837063193321228\n",
            "Epoch 26/50 | Batch 28/198 | Loss: 1.768858551979065\n",
            "Epoch 26/50 | Batch 29/198 | Loss: 1.8144510984420776\n",
            "Epoch 26/50 | Batch 30/198 | Loss: 1.6124444007873535\n",
            "Epoch 26/50 | Batch 31/198 | Loss: 1.857102394104004\n",
            "Epoch 26/50 | Batch 32/198 | Loss: 2.162445306777954\n",
            "Epoch 26/50 | Batch 33/198 | Loss: 1.9500566720962524\n",
            "Epoch 26/50 | Batch 34/198 | Loss: 1.8464593887329102\n",
            "Epoch 26/50 | Batch 35/198 | Loss: 1.9455952644348145\n",
            "Epoch 26/50 | Batch 36/198 | Loss: 1.6073449850082397\n",
            "Epoch 26/50 | Batch 37/198 | Loss: 1.6645472049713135\n",
            "Epoch 26/50 | Batch 38/198 | Loss: 1.6373673677444458\n",
            "Epoch 26/50 | Batch 39/198 | Loss: 1.8615620136260986\n",
            "Epoch 26/50 | Batch 40/198 | Loss: 1.7976689338684082\n",
            "Epoch 26/50 | Batch 41/198 | Loss: 1.6091814041137695\n",
            "Epoch 26/50 | Batch 42/198 | Loss: 1.882924199104309\n",
            "Epoch 26/50 | Batch 43/198 | Loss: 1.72194242477417\n",
            "Epoch 26/50 | Batch 44/198 | Loss: 1.5692697763442993\n",
            "Epoch 26/50 | Batch 45/198 | Loss: 1.7818280458450317\n",
            "Epoch 26/50 | Batch 46/198 | Loss: 1.7538174390792847\n",
            "Epoch 26/50 | Batch 47/198 | Loss: 1.5935977697372437\n",
            "Epoch 26/50 | Batch 48/198 | Loss: 1.8409119844436646\n",
            "Epoch 26/50 | Batch 49/198 | Loss: 1.5083516836166382\n",
            "Epoch 26/50 | Batch 50/198 | Loss: 1.9763482809066772\n",
            "Epoch 26/50 | Batch 51/198 | Loss: 1.7328832149505615\n",
            "Epoch 26/50 | Batch 52/198 | Loss: 1.7686721086502075\n",
            "Epoch 26/50 | Batch 53/198 | Loss: 1.850935459136963\n",
            "Epoch 26/50 | Batch 54/198 | Loss: 1.9686903953552246\n",
            "Epoch 26/50 | Batch 55/198 | Loss: 1.4045995473861694\n",
            "Epoch 26/50 | Batch 56/198 | Loss: 1.6660064458847046\n",
            "Epoch 26/50 | Batch 57/198 | Loss: 1.7470703125\n",
            "Epoch 26/50 | Batch 58/198 | Loss: 1.4244935512542725\n",
            "Epoch 26/50 | Batch 59/198 | Loss: 1.7140024900436401\n",
            "Epoch 26/50 | Batch 60/198 | Loss: 1.718950867652893\n",
            "Epoch 26/50 | Batch 61/198 | Loss: 1.644356608390808\n",
            "Epoch 26/50 | Batch 62/198 | Loss: 1.9330891370773315\n",
            "Epoch 26/50 | Batch 63/198 | Loss: 1.7109220027923584\n",
            "Epoch 26/50 | Batch 64/198 | Loss: 1.6705050468444824\n",
            "Epoch 26/50 | Batch 65/198 | Loss: 1.5464690923690796\n",
            "Epoch 26/50 | Batch 66/198 | Loss: 1.7759385108947754\n",
            "Epoch 26/50 | Batch 67/198 | Loss: 1.8967839479446411\n",
            "Epoch 26/50 | Batch 68/198 | Loss: 2.0981857776641846\n",
            "Epoch 26/50 | Batch 69/198 | Loss: 1.8485736846923828\n",
            "Epoch 26/50 | Batch 70/198 | Loss: 1.811157464981079\n",
            "Epoch 26/50 | Batch 71/198 | Loss: 1.809484601020813\n",
            "Epoch 26/50 | Batch 72/198 | Loss: 1.8285549879074097\n",
            "Epoch 26/50 | Batch 73/198 | Loss: 1.79496431350708\n",
            "Epoch 26/50 | Batch 74/198 | Loss: 1.8207296133041382\n",
            "Epoch 26/50 | Batch 75/198 | Loss: 1.6206318140029907\n",
            "Epoch 26/50 | Batch 76/198 | Loss: 1.8959314823150635\n",
            "Epoch 26/50 | Batch 77/198 | Loss: 1.665950059890747\n",
            "Epoch 26/50 | Batch 78/198 | Loss: 1.9090709686279297\n",
            "Epoch 26/50 | Batch 79/198 | Loss: 1.7900570631027222\n",
            "Epoch 26/50 | Batch 80/198 | Loss: 1.8687739372253418\n",
            "Epoch 26/50 | Batch 81/198 | Loss: 1.8184977769851685\n",
            "Epoch 26/50 | Batch 82/198 | Loss: 1.6741890907287598\n",
            "Epoch 26/50 | Batch 83/198 | Loss: 1.8388376235961914\n",
            "Epoch 26/50 | Batch 84/198 | Loss: 1.596992015838623\n",
            "Epoch 26/50 | Batch 85/198 | Loss: 1.889540672302246\n",
            "Epoch 26/50 | Batch 86/198 | Loss: 1.8489553928375244\n",
            "Epoch 26/50 | Batch 87/198 | Loss: 1.8667621612548828\n",
            "Epoch 26/50 | Batch 88/198 | Loss: 1.6528106927871704\n",
            "Epoch 26/50 | Batch 89/198 | Loss: 2.1476292610168457\n",
            "Epoch 26/50 | Batch 90/198 | Loss: 1.7224864959716797\n",
            "Epoch 26/50 | Batch 91/198 | Loss: 2.0596702098846436\n",
            "Epoch 26/50 | Batch 92/198 | Loss: 1.4861371517181396\n",
            "Epoch 26/50 | Batch 93/198 | Loss: 1.7481013536453247\n",
            "Epoch 26/50 | Batch 94/198 | Loss: 1.6069639921188354\n",
            "Epoch 26/50 | Batch 95/198 | Loss: 1.626771092414856\n",
            "Epoch 26/50 | Batch 96/198 | Loss: 1.9294347763061523\n",
            "Epoch 26/50 | Batch 97/198 | Loss: 1.7679309844970703\n",
            "Epoch 26/50 | Batch 98/198 | Loss: 1.8617396354675293\n",
            "Epoch 26/50 | Batch 99/198 | Loss: 1.7883102893829346\n",
            "Epoch 26/50 | Batch 100/198 | Loss: 1.8505696058273315\n",
            "Epoch 26/50 | Batch 101/198 | Loss: 1.9591435194015503\n",
            "Epoch 26/50 | Batch 102/198 | Loss: 1.793228268623352\n",
            "Epoch 26/50 | Batch 103/198 | Loss: 1.8918356895446777\n",
            "Epoch 26/50 | Batch 104/198 | Loss: 2.065492630004883\n",
            "Epoch 26/50 | Batch 105/198 | Loss: 1.6638543605804443\n",
            "Epoch 26/50 | Batch 106/198 | Loss: 1.8695068359375\n",
            "Epoch 26/50 | Batch 107/198 | Loss: 1.8076413869857788\n",
            "Epoch 26/50 | Batch 108/198 | Loss: 1.8785500526428223\n",
            "Epoch 26/50 | Batch 109/198 | Loss: 1.8518245220184326\n",
            "Epoch 26/50 | Batch 110/198 | Loss: 1.7087435722351074\n",
            "Epoch 26/50 | Batch 111/198 | Loss: 1.9336801767349243\n",
            "Epoch 26/50 | Batch 112/198 | Loss: 1.7463607788085938\n",
            "Epoch 26/50 | Batch 113/198 | Loss: 1.6569074392318726\n",
            "Epoch 26/50 | Batch 114/198 | Loss: 1.8903753757476807\n",
            "Epoch 26/50 | Batch 115/198 | Loss: 1.7110810279846191\n",
            "Epoch 26/50 | Batch 116/198 | Loss: 1.7219161987304688\n",
            "Epoch 26/50 | Batch 117/198 | Loss: 1.8528019189834595\n",
            "Epoch 26/50 | Batch 118/198 | Loss: 1.8778753280639648\n",
            "Epoch 26/50 | Batch 119/198 | Loss: 1.9082955121994019\n",
            "Epoch 26/50 | Batch 120/198 | Loss: 1.8341890573501587\n",
            "Epoch 26/50 | Batch 121/198 | Loss: 1.7374500036239624\n",
            "Epoch 26/50 | Batch 122/198 | Loss: 1.6627575159072876\n",
            "Epoch 26/50 | Batch 123/198 | Loss: 1.7915021181106567\n",
            "Epoch 26/50 | Batch 124/198 | Loss: 2.0010290145874023\n",
            "Epoch 26/50 | Batch 125/198 | Loss: 1.620818018913269\n",
            "Epoch 26/50 | Batch 126/198 | Loss: 1.743836522102356\n",
            "Epoch 26/50 | Batch 127/198 | Loss: 1.6210020780563354\n",
            "Epoch 26/50 | Batch 128/198 | Loss: 1.7312170267105103\n",
            "Epoch 26/50 | Batch 129/198 | Loss: 1.6998381614685059\n",
            "Epoch 26/50 | Batch 130/198 | Loss: 1.7966798543930054\n",
            "Epoch 26/50 | Batch 131/198 | Loss: 1.8919857740402222\n",
            "Epoch 26/50 | Batch 132/198 | Loss: 1.613438367843628\n",
            "Epoch 26/50 | Batch 133/198 | Loss: 1.7312908172607422\n",
            "Epoch 26/50 | Batch 134/198 | Loss: 1.7334386110305786\n",
            "Epoch 26/50 | Batch 135/198 | Loss: 1.8858850002288818\n",
            "Epoch 26/50 | Batch 136/198 | Loss: 2.0010275840759277\n",
            "Epoch 26/50 | Batch 137/198 | Loss: 1.8779362440109253\n",
            "Epoch 26/50 | Batch 138/198 | Loss: 1.74103844165802\n",
            "Epoch 26/50 | Batch 139/198 | Loss: 1.6532639265060425\n",
            "Epoch 26/50 | Batch 140/198 | Loss: 1.8032304048538208\n",
            "Epoch 26/50 | Batch 141/198 | Loss: 1.8390707969665527\n",
            "Epoch 26/50 | Batch 142/198 | Loss: 1.786834716796875\n",
            "Epoch 26/50 | Batch 143/198 | Loss: 1.9401127099990845\n",
            "Epoch 26/50 | Batch 144/198 | Loss: 1.7528595924377441\n",
            "Epoch 26/50 | Batch 145/198 | Loss: 1.7145650386810303\n",
            "Epoch 26/50 | Batch 146/198 | Loss: 1.6305180788040161\n",
            "Epoch 26/50 | Batch 147/198 | Loss: 1.8233356475830078\n",
            "Epoch 26/50 | Batch 148/198 | Loss: 1.7708996534347534\n",
            "Epoch 26/50 | Batch 149/198 | Loss: 1.6780744791030884\n",
            "Epoch 26/50 | Batch 150/198 | Loss: 1.6582566499710083\n",
            "Epoch 26/50 | Batch 151/198 | Loss: 1.7940458059310913\n",
            "Epoch 26/50 | Batch 152/198 | Loss: 1.6730380058288574\n",
            "Epoch 26/50 | Batch 153/198 | Loss: 1.8812040090560913\n",
            "Epoch 26/50 | Batch 154/198 | Loss: 1.6762480735778809\n",
            "Epoch 26/50 | Batch 155/198 | Loss: 2.035120964050293\n",
            "Epoch 26/50 | Batch 156/198 | Loss: 1.9005743265151978\n",
            "Epoch 26/50 | Batch 157/198 | Loss: 1.5302289724349976\n",
            "Epoch 26/50 | Batch 158/198 | Loss: 1.6749277114868164\n",
            "Epoch 26/50 | Batch 159/198 | Loss: 1.6717166900634766\n",
            "Epoch 26/50 | Batch 160/198 | Loss: 1.867182970046997\n",
            "Epoch 26/50 | Batch 161/198 | Loss: 1.9652764797210693\n",
            "Epoch 26/50 | Batch 162/198 | Loss: 1.9285075664520264\n",
            "Epoch 26/50 | Batch 163/198 | Loss: 1.5970879793167114\n",
            "Epoch 26/50 | Batch 164/198 | Loss: 1.7972462177276611\n",
            "Epoch 26/50 | Batch 165/198 | Loss: 1.7850933074951172\n",
            "Epoch 26/50 | Batch 166/198 | Loss: 1.623862862586975\n",
            "Epoch 26/50 | Batch 167/198 | Loss: 1.877372145652771\n",
            "Epoch 26/50 | Batch 168/198 | Loss: 1.6708707809448242\n",
            "Epoch 26/50 | Batch 169/198 | Loss: 1.6352894306182861\n",
            "Epoch 26/50 | Batch 170/198 | Loss: 1.9214462041854858\n",
            "Epoch 26/50 | Batch 171/198 | Loss: 1.7646733522415161\n",
            "Epoch 26/50 | Batch 172/198 | Loss: 1.5374755859375\n",
            "Epoch 26/50 | Batch 173/198 | Loss: 1.8667871952056885\n",
            "Epoch 26/50 | Batch 174/198 | Loss: 1.7505937814712524\n",
            "Epoch 26/50 | Batch 175/198 | Loss: 1.7640812397003174\n",
            "Epoch 26/50 | Batch 176/198 | Loss: 1.6139990091323853\n",
            "Epoch 26/50 | Batch 177/198 | Loss: 1.7650201320648193\n",
            "Epoch 26/50 | Batch 178/198 | Loss: 1.7691855430603027\n",
            "Epoch 26/50 | Batch 179/198 | Loss: 1.4036298990249634\n",
            "Epoch 26/50 | Batch 180/198 | Loss: 1.7809098958969116\n",
            "Epoch 26/50 | Batch 181/198 | Loss: 1.9748345613479614\n",
            "Epoch 26/50 | Batch 182/198 | Loss: 1.814505696296692\n",
            "Epoch 26/50 | Batch 183/198 | Loss: 1.9561272859573364\n",
            "Epoch 26/50 | Batch 184/198 | Loss: 1.497214674949646\n",
            "Epoch 26/50 | Batch 185/198 | Loss: 1.5165237188339233\n",
            "Epoch 26/50 | Batch 186/198 | Loss: 1.8119333982467651\n",
            "Epoch 26/50 | Batch 187/198 | Loss: 1.752708911895752\n",
            "Epoch 26/50 | Batch 188/198 | Loss: 1.8422385454177856\n",
            "Epoch 26/50 | Batch 189/198 | Loss: 1.6433517932891846\n",
            "Epoch 26/50 | Batch 190/198 | Loss: 1.7613272666931152\n",
            "Epoch 26/50 | Batch 191/198 | Loss: 1.8215806484222412\n",
            "Epoch 26/50 | Batch 192/198 | Loss: 1.9088064432144165\n",
            "Epoch 26/50 | Batch 193/198 | Loss: 1.7398356199264526\n",
            "Epoch 26/50 | Batch 194/198 | Loss: 1.7910794019699097\n",
            "Epoch 26/50 | Batch 195/198 | Loss: 1.796800136566162\n",
            "Epoch 26/50 | Batch 196/198 | Loss: 1.7861146926879883\n",
            "Epoch 26/50 | Batch 197/198 | Loss: 1.8069586753845215\n",
            "Epoch 26/50 | Batch 198/198 | Loss: 1.9487379789352417\n",
            "Epoch 26/50 | Average Loss: 1.7694416937201913\n",
            "Epoch 27/50 | Batch 1/198 | Loss: 1.6214025020599365\n",
            "Epoch 27/50 | Batch 2/198 | Loss: 1.85068678855896\n",
            "Epoch 27/50 | Batch 3/198 | Loss: 1.6889252662658691\n",
            "Epoch 27/50 | Batch 4/198 | Loss: 1.6796765327453613\n",
            "Epoch 27/50 | Batch 5/198 | Loss: 1.7341805696487427\n",
            "Epoch 27/50 | Batch 6/198 | Loss: 1.5923948287963867\n",
            "Epoch 27/50 | Batch 7/198 | Loss: 1.8176023960113525\n",
            "Epoch 27/50 | Batch 8/198 | Loss: 1.8130245208740234\n",
            "Epoch 27/50 | Batch 9/198 | Loss: 1.521872878074646\n",
            "Epoch 27/50 | Batch 10/198 | Loss: 1.8004904985427856\n",
            "Epoch 27/50 | Batch 11/198 | Loss: 1.8012945652008057\n",
            "Epoch 27/50 | Batch 12/198 | Loss: 1.7997715473175049\n",
            "Epoch 27/50 | Batch 13/198 | Loss: 1.8606897592544556\n",
            "Epoch 27/50 | Batch 14/198 | Loss: 1.6897456645965576\n",
            "Epoch 27/50 | Batch 15/198 | Loss: 1.9544860124588013\n",
            "Epoch 27/50 | Batch 16/198 | Loss: 2.024585247039795\n",
            "Epoch 27/50 | Batch 17/198 | Loss: 1.7759424448013306\n",
            "Epoch 27/50 | Batch 18/198 | Loss: 1.6847865581512451\n",
            "Epoch 27/50 | Batch 19/198 | Loss: 1.700575828552246\n",
            "Epoch 27/50 | Batch 20/198 | Loss: 1.7420034408569336\n",
            "Epoch 27/50 | Batch 21/198 | Loss: 1.6854106187820435\n",
            "Epoch 27/50 | Batch 22/198 | Loss: 1.8132532835006714\n",
            "Epoch 27/50 | Batch 23/198 | Loss: 1.443855881690979\n",
            "Epoch 27/50 | Batch 24/198 | Loss: 1.5665943622589111\n",
            "Epoch 27/50 | Batch 25/198 | Loss: 1.9291739463806152\n",
            "Epoch 27/50 | Batch 26/198 | Loss: 1.6440932750701904\n",
            "Epoch 27/50 | Batch 27/198 | Loss: 1.8680264949798584\n",
            "Epoch 27/50 | Batch 28/198 | Loss: 1.495219349861145\n",
            "Epoch 27/50 | Batch 29/198 | Loss: 1.8104498386383057\n",
            "Epoch 27/50 | Batch 30/198 | Loss: 1.7223896980285645\n",
            "Epoch 27/50 | Batch 31/198 | Loss: 1.8942105770111084\n",
            "Epoch 27/50 | Batch 32/198 | Loss: 1.5661355257034302\n",
            "Epoch 27/50 | Batch 33/198 | Loss: 1.8405368328094482\n",
            "Epoch 27/50 | Batch 34/198 | Loss: 1.3843204975128174\n",
            "Epoch 27/50 | Batch 35/198 | Loss: 1.6651142835617065\n",
            "Epoch 27/50 | Batch 36/198 | Loss: 1.7001760005950928\n",
            "Epoch 27/50 | Batch 37/198 | Loss: 1.4891788959503174\n",
            "Epoch 27/50 | Batch 38/198 | Loss: 1.704872727394104\n",
            "Epoch 27/50 | Batch 39/198 | Loss: 1.7592041492462158\n",
            "Epoch 27/50 | Batch 40/198 | Loss: 1.5961111783981323\n",
            "Epoch 27/50 | Batch 41/198 | Loss: 1.7363661527633667\n",
            "Epoch 27/50 | Batch 42/198 | Loss: 1.6709494590759277\n",
            "Epoch 27/50 | Batch 43/198 | Loss: 1.8691143989562988\n",
            "Epoch 27/50 | Batch 44/198 | Loss: 1.7096037864685059\n",
            "Epoch 27/50 | Batch 45/198 | Loss: 1.9844480752944946\n",
            "Epoch 27/50 | Batch 46/198 | Loss: 1.6940900087356567\n",
            "Epoch 27/50 | Batch 47/198 | Loss: 1.812337875366211\n",
            "Epoch 27/50 | Batch 48/198 | Loss: 1.8839750289916992\n",
            "Epoch 27/50 | Batch 49/198 | Loss: 1.679374098777771\n",
            "Epoch 27/50 | Batch 50/198 | Loss: 1.4846419095993042\n",
            "Epoch 27/50 | Batch 51/198 | Loss: 1.7466241121292114\n",
            "Epoch 27/50 | Batch 52/198 | Loss: 1.8880411386489868\n",
            "Epoch 27/50 | Batch 53/198 | Loss: 2.045921802520752\n",
            "Epoch 27/50 | Batch 54/198 | Loss: 1.577270746231079\n",
            "Epoch 27/50 | Batch 55/198 | Loss: 1.5554769039154053\n",
            "Epoch 27/50 | Batch 56/198 | Loss: 1.5750792026519775\n",
            "Epoch 27/50 | Batch 57/198 | Loss: 1.719117283821106\n",
            "Epoch 27/50 | Batch 58/198 | Loss: 1.6479138135910034\n",
            "Epoch 27/50 | Batch 59/198 | Loss: 1.9685393571853638\n",
            "Epoch 27/50 | Batch 60/198 | Loss: 1.5487171411514282\n",
            "Epoch 27/50 | Batch 61/198 | Loss: 1.4706226587295532\n",
            "Epoch 27/50 | Batch 62/198 | Loss: 1.7634780406951904\n",
            "Epoch 27/50 | Batch 63/198 | Loss: 1.3720765113830566\n",
            "Epoch 27/50 | Batch 64/198 | Loss: 1.7927114963531494\n",
            "Epoch 27/50 | Batch 65/198 | Loss: 1.7356318235397339\n",
            "Epoch 27/50 | Batch 66/198 | Loss: 1.7957576513290405\n",
            "Epoch 27/50 | Batch 67/198 | Loss: 1.7534047365188599\n",
            "Epoch 27/50 | Batch 68/198 | Loss: 1.4860464334487915\n",
            "Epoch 27/50 | Batch 69/198 | Loss: 1.8862228393554688\n",
            "Epoch 27/50 | Batch 70/198 | Loss: 1.9260801076889038\n",
            "Epoch 27/50 | Batch 71/198 | Loss: 1.7771350145339966\n",
            "Epoch 27/50 | Batch 72/198 | Loss: 1.6091784238815308\n",
            "Epoch 27/50 | Batch 73/198 | Loss: 1.6462167501449585\n",
            "Epoch 27/50 | Batch 74/198 | Loss: 1.7906818389892578\n",
            "Epoch 27/50 | Batch 75/198 | Loss: 1.7658249139785767\n",
            "Epoch 27/50 | Batch 76/198 | Loss: 1.6607134342193604\n",
            "Epoch 27/50 | Batch 77/198 | Loss: 1.8645069599151611\n",
            "Epoch 27/50 | Batch 78/198 | Loss: 1.7885072231292725\n",
            "Epoch 27/50 | Batch 79/198 | Loss: 1.8445920944213867\n",
            "Epoch 27/50 | Batch 80/198 | Loss: 1.9157060384750366\n",
            "Epoch 27/50 | Batch 81/198 | Loss: 1.504517912864685\n",
            "Epoch 27/50 | Batch 82/198 | Loss: 1.7357856035232544\n",
            "Epoch 27/50 | Batch 83/198 | Loss: 1.6925839185714722\n",
            "Epoch 27/50 | Batch 84/198 | Loss: 1.7728930711746216\n",
            "Epoch 27/50 | Batch 85/198 | Loss: 1.9727222919464111\n",
            "Epoch 27/50 | Batch 86/198 | Loss: 1.6820430755615234\n",
            "Epoch 27/50 | Batch 87/198 | Loss: 1.810325264930725\n",
            "Epoch 27/50 | Batch 88/198 | Loss: 1.653989315032959\n",
            "Epoch 27/50 | Batch 89/198 | Loss: 1.5629764795303345\n",
            "Epoch 27/50 | Batch 90/198 | Loss: 1.7582488059997559\n",
            "Epoch 27/50 | Batch 91/198 | Loss: 1.567699670791626\n",
            "Epoch 27/50 | Batch 92/198 | Loss: 1.9980233907699585\n",
            "Epoch 27/50 | Batch 93/198 | Loss: 1.9030853509902954\n",
            "Epoch 27/50 | Batch 94/198 | Loss: 1.6903949975967407\n",
            "Epoch 27/50 | Batch 95/198 | Loss: 1.7413169145584106\n",
            "Epoch 27/50 | Batch 96/198 | Loss: 1.9231630563735962\n",
            "Epoch 27/50 | Batch 97/198 | Loss: 1.742904543876648\n",
            "Epoch 27/50 | Batch 98/198 | Loss: 1.6677132844924927\n",
            "Epoch 27/50 | Batch 99/198 | Loss: 1.8881040811538696\n",
            "Epoch 27/50 | Batch 100/198 | Loss: 1.5426214933395386\n",
            "Epoch 27/50 | Batch 101/198 | Loss: 1.9034361839294434\n",
            "Epoch 27/50 | Batch 102/198 | Loss: 1.539625883102417\n",
            "Epoch 27/50 | Batch 103/198 | Loss: 1.546326756477356\n",
            "Epoch 27/50 | Batch 104/198 | Loss: 1.8210420608520508\n",
            "Epoch 27/50 | Batch 105/198 | Loss: 1.627530813217163\n",
            "Epoch 27/50 | Batch 106/198 | Loss: 1.8114756345748901\n",
            "Epoch 27/50 | Batch 107/198 | Loss: 1.6399359703063965\n",
            "Epoch 27/50 | Batch 108/198 | Loss: 1.8552496433258057\n",
            "Epoch 27/50 | Batch 109/198 | Loss: 1.6195858716964722\n",
            "Epoch 27/50 | Batch 110/198 | Loss: 1.796181082725525\n",
            "Epoch 27/50 | Batch 111/198 | Loss: 1.6747543811798096\n",
            "Epoch 27/50 | Batch 112/198 | Loss: 1.7503501176834106\n",
            "Epoch 27/50 | Batch 113/198 | Loss: 1.7918872833251953\n",
            "Epoch 27/50 | Batch 114/198 | Loss: 1.5984917879104614\n",
            "Epoch 27/50 | Batch 115/198 | Loss: 1.7550760507583618\n",
            "Epoch 27/50 | Batch 116/198 | Loss: 1.912519931793213\n",
            "Epoch 27/50 | Batch 117/198 | Loss: 2.0545923709869385\n",
            "Epoch 27/50 | Batch 118/198 | Loss: 1.6913247108459473\n",
            "Epoch 27/50 | Batch 119/198 | Loss: 1.710947871208191\n",
            "Epoch 27/50 | Batch 120/198 | Loss: 1.7787444591522217\n",
            "Epoch 27/50 | Batch 121/198 | Loss: 1.8254972696304321\n",
            "Epoch 27/50 | Batch 122/198 | Loss: 1.5777039527893066\n",
            "Epoch 27/50 | Batch 123/198 | Loss: 1.7269268035888672\n",
            "Epoch 27/50 | Batch 124/198 | Loss: 1.6535340547561646\n",
            "Epoch 27/50 | Batch 125/198 | Loss: 1.761314034461975\n",
            "Epoch 27/50 | Batch 126/198 | Loss: 1.7842813730239868\n",
            "Epoch 27/50 | Batch 127/198 | Loss: 1.7503398656845093\n",
            "Epoch 27/50 | Batch 128/198 | Loss: 1.6360769271850586\n",
            "Epoch 27/50 | Batch 129/198 | Loss: 1.6740856170654297\n",
            "Epoch 27/50 | Batch 130/198 | Loss: 1.7956703901290894\n",
            "Epoch 27/50 | Batch 131/198 | Loss: 1.5670133829116821\n",
            "Epoch 27/50 | Batch 132/198 | Loss: 1.5912871360778809\n",
            "Epoch 27/50 | Batch 133/198 | Loss: 1.68126380443573\n",
            "Epoch 27/50 | Batch 134/198 | Loss: 1.70919930934906\n",
            "Epoch 27/50 | Batch 135/198 | Loss: 1.6288931369781494\n",
            "Epoch 27/50 | Batch 136/198 | Loss: 1.695694923400879\n",
            "Epoch 27/50 | Batch 137/198 | Loss: 1.6423131227493286\n",
            "Epoch 27/50 | Batch 138/198 | Loss: 1.5234074592590332\n",
            "Epoch 27/50 | Batch 139/198 | Loss: 1.727874994277954\n",
            "Epoch 27/50 | Batch 140/198 | Loss: 1.8898297548294067\n",
            "Epoch 27/50 | Batch 141/198 | Loss: 1.6713334321975708\n",
            "Epoch 27/50 | Batch 142/198 | Loss: 1.716823697090149\n",
            "Epoch 27/50 | Batch 143/198 | Loss: 1.8141018152236938\n",
            "Epoch 27/50 | Batch 144/198 | Loss: 1.775278925895691\n",
            "Epoch 27/50 | Batch 145/198 | Loss: 1.842151403427124\n",
            "Epoch 27/50 | Batch 146/198 | Loss: 1.6623517274856567\n",
            "Epoch 27/50 | Batch 147/198 | Loss: 1.7388458251953125\n",
            "Epoch 27/50 | Batch 148/198 | Loss: 1.611648440361023\n",
            "Epoch 27/50 | Batch 149/198 | Loss: 1.5875109434127808\n",
            "Epoch 27/50 | Batch 150/198 | Loss: 1.8136731386184692\n",
            "Epoch 27/50 | Batch 151/198 | Loss: 1.670530915260315\n",
            "Epoch 27/50 | Batch 152/198 | Loss: 1.6125620603561401\n",
            "Epoch 27/50 | Batch 153/198 | Loss: 1.4603909254074097\n",
            "Epoch 27/50 | Batch 154/198 | Loss: 1.7344852685928345\n",
            "Epoch 27/50 | Batch 155/198 | Loss: 1.7128853797912598\n",
            "Epoch 27/50 | Batch 156/198 | Loss: 1.8007489442825317\n",
            "Epoch 27/50 | Batch 157/198 | Loss: 1.7017604112625122\n",
            "Epoch 27/50 | Batch 158/198 | Loss: 1.8062459230422974\n",
            "Epoch 27/50 | Batch 159/198 | Loss: 1.7034786939620972\n",
            "Epoch 27/50 | Batch 160/198 | Loss: 1.8240875005722046\n",
            "Epoch 27/50 | Batch 161/198 | Loss: 1.524092435836792\n",
            "Epoch 27/50 | Batch 162/198 | Loss: 1.4623545408248901\n",
            "Epoch 27/50 | Batch 163/198 | Loss: 1.7415028810501099\n",
            "Epoch 27/50 | Batch 164/198 | Loss: 1.4727857112884521\n",
            "Epoch 27/50 | Batch 165/198 | Loss: 1.7003414630889893\n",
            "Epoch 27/50 | Batch 166/198 | Loss: 1.8060256242752075\n",
            "Epoch 27/50 | Batch 167/198 | Loss: 1.7951009273529053\n",
            "Epoch 27/50 | Batch 168/198 | Loss: 1.6655406951904297\n",
            "Epoch 27/50 | Batch 169/198 | Loss: 1.801888108253479\n",
            "Epoch 27/50 | Batch 170/198 | Loss: 1.990875482559204\n",
            "Epoch 27/50 | Batch 171/198 | Loss: 1.757368803024292\n",
            "Epoch 27/50 | Batch 172/198 | Loss: 1.891585111618042\n",
            "Epoch 27/50 | Batch 173/198 | Loss: 1.4907258749008179\n",
            "Epoch 27/50 | Batch 174/198 | Loss: 1.8930491209030151\n",
            "Epoch 27/50 | Batch 175/198 | Loss: 1.7321735620498657\n",
            "Epoch 27/50 | Batch 176/198 | Loss: 1.7117090225219727\n",
            "Epoch 27/50 | Batch 177/198 | Loss: 1.8015865087509155\n",
            "Epoch 27/50 | Batch 178/198 | Loss: 1.808345913887024\n",
            "Epoch 27/50 | Batch 179/198 | Loss: 1.6263903379440308\n",
            "Epoch 27/50 | Batch 180/198 | Loss: 1.7786610126495361\n",
            "Epoch 27/50 | Batch 181/198 | Loss: 1.7930676937103271\n",
            "Epoch 27/50 | Batch 182/198 | Loss: 1.7076141834259033\n",
            "Epoch 27/50 | Batch 183/198 | Loss: 1.619962453842163\n",
            "Epoch 27/50 | Batch 184/198 | Loss: 1.732871174812317\n",
            "Epoch 27/50 | Batch 185/198 | Loss: 1.6629544496536255\n",
            "Epoch 27/50 | Batch 186/198 | Loss: 1.962962031364441\n",
            "Epoch 27/50 | Batch 187/198 | Loss: 1.6569902896881104\n",
            "Epoch 27/50 | Batch 188/198 | Loss: 1.7211499214172363\n",
            "Epoch 27/50 | Batch 189/198 | Loss: 1.7298290729522705\n",
            "Epoch 27/50 | Batch 190/198 | Loss: 1.7371273040771484\n",
            "Epoch 27/50 | Batch 191/198 | Loss: 1.695622205734253\n",
            "Epoch 27/50 | Batch 192/198 | Loss: 1.7072407007217407\n",
            "Epoch 27/50 | Batch 193/198 | Loss: 1.8625754117965698\n",
            "Epoch 27/50 | Batch 194/198 | Loss: 1.8766193389892578\n",
            "Epoch 27/50 | Batch 195/198 | Loss: 1.5691707134246826\n",
            "Epoch 27/50 | Batch 196/198 | Loss: 1.9483075141906738\n",
            "Epoch 27/50 | Batch 197/198 | Loss: 1.6567368507385254\n",
            "Epoch 27/50 | Batch 198/198 | Loss: 1.6885656118392944\n",
            "Epoch 27/50 | Average Loss: 1.726009730738823\n",
            "Epoch 28/50 | Batch 1/198 | Loss: 1.5603452920913696\n",
            "Epoch 28/50 | Batch 2/198 | Loss: 1.584259033203125\n",
            "Epoch 28/50 | Batch 3/198 | Loss: 1.4206483364105225\n",
            "Epoch 28/50 | Batch 4/198 | Loss: 1.6454849243164062\n",
            "Epoch 28/50 | Batch 5/198 | Loss: 1.8237980604171753\n",
            "Epoch 28/50 | Batch 6/198 | Loss: 1.6031469106674194\n",
            "Epoch 28/50 | Batch 7/198 | Loss: 1.5691226720809937\n",
            "Epoch 28/50 | Batch 8/198 | Loss: 1.4753469228744507\n",
            "Epoch 28/50 | Batch 9/198 | Loss: 1.8898015022277832\n",
            "Epoch 28/50 | Batch 10/198 | Loss: 1.7027949094772339\n",
            "Epoch 28/50 | Batch 11/198 | Loss: 1.5671387910842896\n",
            "Epoch 28/50 | Batch 12/198 | Loss: 1.9380536079406738\n",
            "Epoch 28/50 | Batch 13/198 | Loss: 1.630249261856079\n",
            "Epoch 28/50 | Batch 14/198 | Loss: 1.7014552354812622\n",
            "Epoch 28/50 | Batch 15/198 | Loss: 1.6489923000335693\n",
            "Epoch 28/50 | Batch 16/198 | Loss: 1.8271844387054443\n",
            "Epoch 28/50 | Batch 17/198 | Loss: 1.639591932296753\n",
            "Epoch 28/50 | Batch 18/198 | Loss: 1.6980228424072266\n",
            "Epoch 28/50 | Batch 19/198 | Loss: 1.7014633417129517\n",
            "Epoch 28/50 | Batch 20/198 | Loss: 1.8348157405853271\n",
            "Epoch 28/50 | Batch 21/198 | Loss: 1.5249199867248535\n",
            "Epoch 28/50 | Batch 22/198 | Loss: 1.6671340465545654\n",
            "Epoch 28/50 | Batch 23/198 | Loss: 1.8185702562332153\n",
            "Epoch 28/50 | Batch 24/198 | Loss: 1.6694400310516357\n",
            "Epoch 28/50 | Batch 25/198 | Loss: 1.7922723293304443\n",
            "Epoch 28/50 | Batch 26/198 | Loss: 1.4896763563156128\n",
            "Epoch 28/50 | Batch 27/198 | Loss: 1.6156960725784302\n",
            "Epoch 28/50 | Batch 28/198 | Loss: 1.678861141204834\n",
            "Epoch 28/50 | Batch 29/198 | Loss: 1.5856069326400757\n",
            "Epoch 28/50 | Batch 30/198 | Loss: 1.8921303749084473\n",
            "Epoch 28/50 | Batch 31/198 | Loss: 2.0044400691986084\n",
            "Epoch 28/50 | Batch 32/198 | Loss: 1.4708386659622192\n",
            "Epoch 28/50 | Batch 33/198 | Loss: 1.6589921712875366\n",
            "Epoch 28/50 | Batch 34/198 | Loss: 1.6720927953720093\n",
            "Epoch 28/50 | Batch 35/198 | Loss: 1.4358160495758057\n",
            "Epoch 28/50 | Batch 36/198 | Loss: 1.5318597555160522\n",
            "Epoch 28/50 | Batch 37/198 | Loss: 1.8485318422317505\n",
            "Epoch 28/50 | Batch 38/198 | Loss: 1.653055191040039\n",
            "Epoch 28/50 | Batch 39/198 | Loss: 1.532840371131897\n",
            "Epoch 28/50 | Batch 40/198 | Loss: 1.6932021379470825\n",
            "Epoch 28/50 | Batch 41/198 | Loss: 1.8924267292022705\n",
            "Epoch 28/50 | Batch 42/198 | Loss: 1.6390552520751953\n",
            "Epoch 28/50 | Batch 43/198 | Loss: 1.6449904441833496\n",
            "Epoch 28/50 | Batch 44/198 | Loss: 1.781711220741272\n",
            "Epoch 28/50 | Batch 45/198 | Loss: 1.5805937051773071\n",
            "Epoch 28/50 | Batch 46/198 | Loss: 1.5300489664077759\n",
            "Epoch 28/50 | Batch 47/198 | Loss: 1.9681978225708008\n",
            "Epoch 28/50 | Batch 48/198 | Loss: 1.6568620204925537\n",
            "Epoch 28/50 | Batch 49/198 | Loss: 1.6560477018356323\n",
            "Epoch 28/50 | Batch 50/198 | Loss: 1.6673314571380615\n",
            "Epoch 28/50 | Batch 51/198 | Loss: 1.505647897720337\n",
            "Epoch 28/50 | Batch 52/198 | Loss: 1.756902813911438\n",
            "Epoch 28/50 | Batch 53/198 | Loss: 1.5382053852081299\n",
            "Epoch 28/50 | Batch 54/198 | Loss: 1.802289366722107\n",
            "Epoch 28/50 | Batch 55/198 | Loss: 1.6596070528030396\n",
            "Epoch 28/50 | Batch 56/198 | Loss: 1.8373651504516602\n",
            "Epoch 28/50 | Batch 57/198 | Loss: 1.5600085258483887\n",
            "Epoch 28/50 | Batch 58/198 | Loss: 1.8269861936569214\n",
            "Epoch 28/50 | Batch 59/198 | Loss: 1.9186056852340698\n",
            "Epoch 28/50 | Batch 60/198 | Loss: 1.7361130714416504\n",
            "Epoch 28/50 | Batch 61/198 | Loss: 1.7212015390396118\n",
            "Epoch 28/50 | Batch 62/198 | Loss: 1.6827259063720703\n",
            "Epoch 28/50 | Batch 63/198 | Loss: 1.5178393125534058\n",
            "Epoch 28/50 | Batch 64/198 | Loss: 1.8064454793930054\n",
            "Epoch 28/50 | Batch 65/198 | Loss: 1.4044804573059082\n",
            "Epoch 28/50 | Batch 66/198 | Loss: 1.4220502376556396\n",
            "Epoch 28/50 | Batch 67/198 | Loss: 1.6630083322525024\n",
            "Epoch 28/50 | Batch 68/198 | Loss: 1.8363893032073975\n",
            "Epoch 28/50 | Batch 69/198 | Loss: 1.7727465629577637\n",
            "Epoch 28/50 | Batch 70/198 | Loss: 1.6563693284988403\n",
            "Epoch 28/50 | Batch 71/198 | Loss: 1.7347043752670288\n",
            "Epoch 28/50 | Batch 72/198 | Loss: 1.714455485343933\n",
            "Epoch 28/50 | Batch 73/198 | Loss: 1.5596904754638672\n",
            "Epoch 28/50 | Batch 74/198 | Loss: 1.7945809364318848\n",
            "Epoch 28/50 | Batch 75/198 | Loss: 1.940341830253601\n",
            "Epoch 28/50 | Batch 76/198 | Loss: 1.6621067523956299\n",
            "Epoch 28/50 | Batch 77/198 | Loss: 1.682497262954712\n",
            "Epoch 28/50 | Batch 78/198 | Loss: 1.5060927867889404\n",
            "Epoch 28/50 | Batch 79/198 | Loss: 1.846964716911316\n",
            "Epoch 28/50 | Batch 80/198 | Loss: 1.7961305379867554\n",
            "Epoch 28/50 | Batch 81/198 | Loss: 1.7095057964324951\n",
            "Epoch 28/50 | Batch 82/198 | Loss: 1.5837385654449463\n",
            "Epoch 28/50 | Batch 83/198 | Loss: 1.8317219018936157\n",
            "Epoch 28/50 | Batch 84/198 | Loss: 1.9344311952590942\n",
            "Epoch 28/50 | Batch 85/198 | Loss: 1.5445674657821655\n",
            "Epoch 28/50 | Batch 86/198 | Loss: 1.8923006057739258\n",
            "Epoch 28/50 | Batch 87/198 | Loss: 1.387687087059021\n",
            "Epoch 28/50 | Batch 88/198 | Loss: 1.4433332681655884\n",
            "Epoch 28/50 | Batch 89/198 | Loss: 1.5822432041168213\n",
            "Epoch 28/50 | Batch 90/198 | Loss: 1.6135975122451782\n",
            "Epoch 28/50 | Batch 91/198 | Loss: 1.599609136581421\n",
            "Epoch 28/50 | Batch 92/198 | Loss: 1.5623289346694946\n",
            "Epoch 28/50 | Batch 93/198 | Loss: 1.7387570142745972\n",
            "Epoch 28/50 | Batch 94/198 | Loss: 1.9272323846817017\n",
            "Epoch 28/50 | Batch 95/198 | Loss: 1.7774513959884644\n",
            "Epoch 28/50 | Batch 96/198 | Loss: 1.9481430053710938\n",
            "Epoch 28/50 | Batch 97/198 | Loss: 1.6397532224655151\n",
            "Epoch 28/50 | Batch 98/198 | Loss: 1.6828311681747437\n",
            "Epoch 28/50 | Batch 99/198 | Loss: 1.5627330541610718\n",
            "Epoch 28/50 | Batch 100/198 | Loss: 1.6905078887939453\n",
            "Epoch 28/50 | Batch 101/198 | Loss: 1.6292212009429932\n",
            "Epoch 28/50 | Batch 102/198 | Loss: 1.5570942163467407\n",
            "Epoch 28/50 | Batch 103/198 | Loss: 1.6168944835662842\n",
            "Epoch 28/50 | Batch 104/198 | Loss: 1.7217411994934082\n",
            "Epoch 28/50 | Batch 105/198 | Loss: 1.7786026000976562\n",
            "Epoch 28/50 | Batch 106/198 | Loss: 1.5723298788070679\n",
            "Epoch 28/50 | Batch 107/198 | Loss: 1.6878708600997925\n",
            "Epoch 28/50 | Batch 108/198 | Loss: 1.5379468202590942\n",
            "Epoch 28/50 | Batch 109/198 | Loss: 1.5946283340454102\n",
            "Epoch 28/50 | Batch 110/198 | Loss: 1.3612314462661743\n",
            "Epoch 28/50 | Batch 111/198 | Loss: 1.829044222831726\n",
            "Epoch 28/50 | Batch 112/198 | Loss: 1.787428855895996\n",
            "Epoch 28/50 | Batch 113/198 | Loss: 1.6051050424575806\n",
            "Epoch 28/50 | Batch 114/198 | Loss: 1.6540008783340454\n",
            "Epoch 28/50 | Batch 115/198 | Loss: 1.5551280975341797\n",
            "Epoch 28/50 | Batch 116/198 | Loss: 1.8719580173492432\n",
            "Epoch 28/50 | Batch 117/198 | Loss: 1.6482969522476196\n",
            "Epoch 28/50 | Batch 118/198 | Loss: 1.7051770687103271\n",
            "Epoch 28/50 | Batch 119/198 | Loss: 1.7497632503509521\n",
            "Epoch 28/50 | Batch 120/198 | Loss: 1.611856460571289\n",
            "Epoch 28/50 | Batch 121/198 | Loss: 1.62355375289917\n",
            "Epoch 28/50 | Batch 122/198 | Loss: 1.608131766319275\n",
            "Epoch 28/50 | Batch 123/198 | Loss: 1.7056939601898193\n",
            "Epoch 28/50 | Batch 124/198 | Loss: 1.8595396280288696\n",
            "Epoch 28/50 | Batch 125/198 | Loss: 1.656854510307312\n",
            "Epoch 28/50 | Batch 126/198 | Loss: 1.5856120586395264\n",
            "Epoch 28/50 | Batch 127/198 | Loss: 1.5377389192581177\n",
            "Epoch 28/50 | Batch 128/198 | Loss: 1.66780424118042\n",
            "Epoch 28/50 | Batch 129/198 | Loss: 1.724701166152954\n",
            "Epoch 28/50 | Batch 130/198 | Loss: 1.7241300344467163\n",
            "Epoch 28/50 | Batch 131/198 | Loss: 1.5528380870819092\n",
            "Epoch 28/50 | Batch 132/198 | Loss: 1.851032018661499\n",
            "Epoch 28/50 | Batch 133/198 | Loss: 1.7231104373931885\n",
            "Epoch 28/50 | Batch 134/198 | Loss: 1.7204289436340332\n",
            "Epoch 28/50 | Batch 135/198 | Loss: 1.8451918363571167\n",
            "Epoch 28/50 | Batch 136/198 | Loss: 1.8430758714675903\n",
            "Epoch 28/50 | Batch 137/198 | Loss: 1.4311788082122803\n",
            "Epoch 28/50 | Batch 138/198 | Loss: 1.8905292749404907\n",
            "Epoch 28/50 | Batch 139/198 | Loss: 1.8296350240707397\n",
            "Epoch 28/50 | Batch 140/198 | Loss: 1.7929717302322388\n",
            "Epoch 28/50 | Batch 141/198 | Loss: 1.8680877685546875\n",
            "Epoch 28/50 | Batch 142/198 | Loss: 1.8571804761886597\n",
            "Epoch 28/50 | Batch 143/198 | Loss: 1.8251464366912842\n",
            "Epoch 28/50 | Batch 144/198 | Loss: 1.4949736595153809\n",
            "Epoch 28/50 | Batch 145/198 | Loss: 1.528161644935608\n",
            "Epoch 28/50 | Batch 146/198 | Loss: 1.5405614376068115\n",
            "Epoch 28/50 | Batch 147/198 | Loss: 1.6930792331695557\n",
            "Epoch 28/50 | Batch 148/198 | Loss: 1.7305535078048706\n",
            "Epoch 28/50 | Batch 149/198 | Loss: 1.7436209917068481\n",
            "Epoch 28/50 | Batch 150/198 | Loss: 1.4598435163497925\n",
            "Epoch 28/50 | Batch 151/198 | Loss: 1.836601734161377\n",
            "Epoch 28/50 | Batch 152/198 | Loss: 1.6539655923843384\n",
            "Epoch 28/50 | Batch 153/198 | Loss: 1.6853605508804321\n",
            "Epoch 28/50 | Batch 154/198 | Loss: 1.8092522621154785\n",
            "Epoch 28/50 | Batch 155/198 | Loss: 1.6342504024505615\n",
            "Epoch 28/50 | Batch 156/198 | Loss: 1.6886920928955078\n",
            "Epoch 28/50 | Batch 157/198 | Loss: 1.6536110639572144\n",
            "Epoch 28/50 | Batch 158/198 | Loss: 1.5431228876113892\n",
            "Epoch 28/50 | Batch 159/198 | Loss: 1.6399353742599487\n",
            "Epoch 28/50 | Batch 160/198 | Loss: 1.7344415187835693\n",
            "Epoch 28/50 | Batch 161/198 | Loss: 1.5412570238113403\n",
            "Epoch 28/50 | Batch 162/198 | Loss: 1.5513445138931274\n",
            "Epoch 28/50 | Batch 163/198 | Loss: 1.739216685295105\n",
            "Epoch 28/50 | Batch 164/198 | Loss: 1.806933045387268\n",
            "Epoch 28/50 | Batch 165/198 | Loss: 1.8108981847763062\n",
            "Epoch 28/50 | Batch 166/198 | Loss: 1.4975788593292236\n",
            "Epoch 28/50 | Batch 167/198 | Loss: 1.520167350769043\n",
            "Epoch 28/50 | Batch 168/198 | Loss: 1.9959825277328491\n",
            "Epoch 28/50 | Batch 169/198 | Loss: 1.815049171447754\n",
            "Epoch 28/50 | Batch 170/198 | Loss: 1.5543818473815918\n",
            "Epoch 28/50 | Batch 171/198 | Loss: 1.658341407775879\n",
            "Epoch 28/50 | Batch 172/198 | Loss: 1.7361507415771484\n",
            "Epoch 28/50 | Batch 173/198 | Loss: 1.6160674095153809\n",
            "Epoch 28/50 | Batch 174/198 | Loss: 1.5644232034683228\n",
            "Epoch 28/50 | Batch 175/198 | Loss: 1.6449676752090454\n",
            "Epoch 28/50 | Batch 176/198 | Loss: 1.7340031862258911\n",
            "Epoch 28/50 | Batch 177/198 | Loss: 1.9548439979553223\n",
            "Epoch 28/50 | Batch 178/198 | Loss: 1.5352476835250854\n",
            "Epoch 28/50 | Batch 179/198 | Loss: 1.7538846731185913\n",
            "Epoch 28/50 | Batch 180/198 | Loss: 1.4631056785583496\n",
            "Epoch 28/50 | Batch 181/198 | Loss: 1.7654023170471191\n",
            "Epoch 28/50 | Batch 182/198 | Loss: 1.6297216415405273\n",
            "Epoch 28/50 | Batch 183/198 | Loss: 1.9886972904205322\n",
            "Epoch 28/50 | Batch 184/198 | Loss: 1.810449242591858\n",
            "Epoch 28/50 | Batch 185/198 | Loss: 1.7161133289337158\n",
            "Epoch 28/50 | Batch 186/198 | Loss: 1.778882384300232\n",
            "Epoch 28/50 | Batch 187/198 | Loss: 1.67414391040802\n",
            "Epoch 28/50 | Batch 188/198 | Loss: 1.8101210594177246\n",
            "Epoch 28/50 | Batch 189/198 | Loss: 1.8222575187683105\n",
            "Epoch 28/50 | Batch 190/198 | Loss: 1.6709516048431396\n",
            "Epoch 28/50 | Batch 191/198 | Loss: 1.7751330137252808\n",
            "Epoch 28/50 | Batch 192/198 | Loss: 1.7886766195297241\n",
            "Epoch 28/50 | Batch 193/198 | Loss: 1.8725520372390747\n",
            "Epoch 28/50 | Batch 194/198 | Loss: 1.558390736579895\n",
            "Epoch 28/50 | Batch 195/198 | Loss: 2.014915704727173\n",
            "Epoch 28/50 | Batch 196/198 | Loss: 1.4123975038528442\n",
            "Epoch 28/50 | Batch 197/198 | Loss: 1.598617434501648\n",
            "Epoch 28/50 | Batch 198/198 | Loss: 1.696860432624817\n",
            "Epoch 28/50 | Average Loss: 1.686755693320072\n",
            "Epoch 29/50 | Batch 1/198 | Loss: 1.6466280221939087\n",
            "Epoch 29/50 | Batch 2/198 | Loss: 1.8986728191375732\n",
            "Epoch 29/50 | Batch 3/198 | Loss: 1.5390478372573853\n",
            "Epoch 29/50 | Batch 4/198 | Loss: 1.798354148864746\n",
            "Epoch 29/50 | Batch 5/198 | Loss: 1.7717151641845703\n",
            "Epoch 29/50 | Batch 6/198 | Loss: 1.4696440696716309\n",
            "Epoch 29/50 | Batch 7/198 | Loss: 1.7078155279159546\n",
            "Epoch 29/50 | Batch 8/198 | Loss: 1.7905956506729126\n",
            "Epoch 29/50 | Batch 9/198 | Loss: 1.8281649351119995\n",
            "Epoch 29/50 | Batch 10/198 | Loss: 1.8194752931594849\n",
            "Epoch 29/50 | Batch 11/198 | Loss: 1.754023551940918\n",
            "Epoch 29/50 | Batch 12/198 | Loss: 1.6395354270935059\n",
            "Epoch 29/50 | Batch 13/198 | Loss: 1.7119685411453247\n",
            "Epoch 29/50 | Batch 14/198 | Loss: 1.4637564420700073\n",
            "Epoch 29/50 | Batch 15/198 | Loss: 1.7407904863357544\n",
            "Epoch 29/50 | Batch 16/198 | Loss: 1.5281636714935303\n",
            "Epoch 29/50 | Batch 17/198 | Loss: 1.357670545578003\n",
            "Epoch 29/50 | Batch 18/198 | Loss: 1.5132884979248047\n",
            "Epoch 29/50 | Batch 19/198 | Loss: 1.6520551443099976\n",
            "Epoch 29/50 | Batch 20/198 | Loss: 1.8092105388641357\n",
            "Epoch 29/50 | Batch 21/198 | Loss: 1.8004257678985596\n",
            "Epoch 29/50 | Batch 22/198 | Loss: 1.7303104400634766\n",
            "Epoch 29/50 | Batch 23/198 | Loss: 1.9242174625396729\n",
            "Epoch 29/50 | Batch 24/198 | Loss: 1.7321933507919312\n",
            "Epoch 29/50 | Batch 25/198 | Loss: 1.7154409885406494\n",
            "Epoch 29/50 | Batch 26/198 | Loss: 1.6489429473876953\n",
            "Epoch 29/50 | Batch 27/198 | Loss: 1.424648642539978\n",
            "Epoch 29/50 | Batch 28/198 | Loss: 1.5310312509536743\n",
            "Epoch 29/50 | Batch 29/198 | Loss: 1.778925895690918\n",
            "Epoch 29/50 | Batch 30/198 | Loss: 1.7483919858932495\n",
            "Epoch 29/50 | Batch 31/198 | Loss: 1.9250167608261108\n",
            "Epoch 29/50 | Batch 32/198 | Loss: 1.677837610244751\n",
            "Epoch 29/50 | Batch 33/198 | Loss: 1.405967354774475\n",
            "Epoch 29/50 | Batch 34/198 | Loss: 1.5971804857254028\n",
            "Epoch 29/50 | Batch 35/198 | Loss: 1.7897135019302368\n",
            "Epoch 29/50 | Batch 36/198 | Loss: 1.598224401473999\n",
            "Epoch 29/50 | Batch 37/198 | Loss: 1.8342270851135254\n",
            "Epoch 29/50 | Batch 38/198 | Loss: 1.4094722270965576\n",
            "Epoch 29/50 | Batch 39/198 | Loss: 1.6404412984848022\n",
            "Epoch 29/50 | Batch 40/198 | Loss: 1.8746135234832764\n",
            "Epoch 29/50 | Batch 41/198 | Loss: 1.54664146900177\n",
            "Epoch 29/50 | Batch 42/198 | Loss: 1.454776406288147\n",
            "Epoch 29/50 | Batch 43/198 | Loss: 1.82249915599823\n",
            "Epoch 29/50 | Batch 44/198 | Loss: 1.6702828407287598\n",
            "Epoch 29/50 | Batch 45/198 | Loss: 1.194101333618164\n",
            "Epoch 29/50 | Batch 46/198 | Loss: 1.669279932975769\n",
            "Epoch 29/50 | Batch 47/198 | Loss: 1.5037397146224976\n",
            "Epoch 29/50 | Batch 48/198 | Loss: 1.6179667711257935\n",
            "Epoch 29/50 | Batch 49/198 | Loss: 1.7302700281143188\n",
            "Epoch 29/50 | Batch 50/198 | Loss: 1.5346418619155884\n",
            "Epoch 29/50 | Batch 51/198 | Loss: 1.7764265537261963\n",
            "Epoch 29/50 | Batch 52/198 | Loss: 1.5178298950195312\n",
            "Epoch 29/50 | Batch 53/198 | Loss: 1.8504705429077148\n",
            "Epoch 29/50 | Batch 54/198 | Loss: 1.7456073760986328\n",
            "Epoch 29/50 | Batch 55/198 | Loss: 1.7206844091415405\n",
            "Epoch 29/50 | Batch 56/198 | Loss: 1.5873841047286987\n",
            "Epoch 29/50 | Batch 57/198 | Loss: 1.6735185384750366\n",
            "Epoch 29/50 | Batch 58/198 | Loss: 1.5719422101974487\n",
            "Epoch 29/50 | Batch 59/198 | Loss: 1.8020689487457275\n",
            "Epoch 29/50 | Batch 60/198 | Loss: 1.6731984615325928\n",
            "Epoch 29/50 | Batch 61/198 | Loss: 1.6827607154846191\n",
            "Epoch 29/50 | Batch 62/198 | Loss: 1.4312002658843994\n",
            "Epoch 29/50 | Batch 63/198 | Loss: 1.5939098596572876\n",
            "Epoch 29/50 | Batch 64/198 | Loss: 1.5961393117904663\n",
            "Epoch 29/50 | Batch 65/198 | Loss: 1.6657984256744385\n",
            "Epoch 29/50 | Batch 66/198 | Loss: 1.6827226877212524\n",
            "Epoch 29/50 | Batch 67/198 | Loss: 1.6348623037338257\n",
            "Epoch 29/50 | Batch 68/198 | Loss: 1.6993224620819092\n",
            "Epoch 29/50 | Batch 69/198 | Loss: 1.621024489402771\n",
            "Epoch 29/50 | Batch 70/198 | Loss: 1.67827570438385\n",
            "Epoch 29/50 | Batch 71/198 | Loss: 1.778132438659668\n",
            "Epoch 29/50 | Batch 72/198 | Loss: 1.5483427047729492\n",
            "Epoch 29/50 | Batch 73/198 | Loss: 1.8693621158599854\n",
            "Epoch 29/50 | Batch 74/198 | Loss: 1.8533883094787598\n",
            "Epoch 29/50 | Batch 75/198 | Loss: 1.6011358499526978\n",
            "Epoch 29/50 | Batch 76/198 | Loss: 1.8396497964859009\n",
            "Epoch 29/50 | Batch 77/198 | Loss: 1.4126447439193726\n",
            "Epoch 29/50 | Batch 78/198 | Loss: 1.5190012454986572\n",
            "Epoch 29/50 | Batch 79/198 | Loss: 1.5828936100006104\n",
            "Epoch 29/50 | Batch 80/198 | Loss: 1.6115131378173828\n",
            "Epoch 29/50 | Batch 81/198 | Loss: 2.065156936645508\n",
            "Epoch 29/50 | Batch 82/198 | Loss: 1.3932430744171143\n",
            "Epoch 29/50 | Batch 83/198 | Loss: 1.6765717267990112\n",
            "Epoch 29/50 | Batch 84/198 | Loss: 1.8601831197738647\n",
            "Epoch 29/50 | Batch 85/198 | Loss: 1.5642648935317993\n",
            "Epoch 29/50 | Batch 86/198 | Loss: 1.5262880325317383\n",
            "Epoch 29/50 | Batch 87/198 | Loss: 1.7589706182479858\n",
            "Epoch 29/50 | Batch 88/198 | Loss: 1.7574193477630615\n",
            "Epoch 29/50 | Batch 89/198 | Loss: 1.5896562337875366\n",
            "Epoch 29/50 | Batch 90/198 | Loss: 1.5721940994262695\n",
            "Epoch 29/50 | Batch 91/198 | Loss: 1.3820208311080933\n",
            "Epoch 29/50 | Batch 92/198 | Loss: 1.6118264198303223\n",
            "Epoch 29/50 | Batch 93/198 | Loss: 1.7836507558822632\n",
            "Epoch 29/50 | Batch 94/198 | Loss: 1.363938570022583\n",
            "Epoch 29/50 | Batch 95/198 | Loss: 1.6705293655395508\n",
            "Epoch 29/50 | Batch 96/198 | Loss: 1.6205195188522339\n",
            "Epoch 29/50 | Batch 97/198 | Loss: 1.7774169445037842\n",
            "Epoch 29/50 | Batch 98/198 | Loss: 1.7192010879516602\n",
            "Epoch 29/50 | Batch 99/198 | Loss: 1.709702730178833\n",
            "Epoch 29/50 | Batch 100/198 | Loss: 1.5321340560913086\n",
            "Epoch 29/50 | Batch 101/198 | Loss: 1.5801587104797363\n",
            "Epoch 29/50 | Batch 102/198 | Loss: 1.83207106590271\n",
            "Epoch 29/50 | Batch 103/198 | Loss: 1.6877198219299316\n",
            "Epoch 29/50 | Batch 104/198 | Loss: 1.6762551069259644\n",
            "Epoch 29/50 | Batch 105/198 | Loss: 1.6431783437728882\n",
            "Epoch 29/50 | Batch 106/198 | Loss: 1.751711130142212\n",
            "Epoch 29/50 | Batch 107/198 | Loss: 1.4828286170959473\n",
            "Epoch 29/50 | Batch 108/198 | Loss: 1.481870174407959\n",
            "Epoch 29/50 | Batch 109/198 | Loss: 1.7943782806396484\n",
            "Epoch 29/50 | Batch 110/198 | Loss: 1.5358167886734009\n",
            "Epoch 29/50 | Batch 111/198 | Loss: 1.6949876546859741\n",
            "Epoch 29/50 | Batch 112/198 | Loss: 1.43482506275177\n",
            "Epoch 29/50 | Batch 113/198 | Loss: 1.547839641571045\n",
            "Epoch 29/50 | Batch 114/198 | Loss: 1.523862361907959\n",
            "Epoch 29/50 | Batch 115/198 | Loss: 1.5426563024520874\n",
            "Epoch 29/50 | Batch 116/198 | Loss: 1.632734775543213\n",
            "Epoch 29/50 | Batch 117/198 | Loss: 1.5793166160583496\n",
            "Epoch 29/50 | Batch 118/198 | Loss: 1.6934921741485596\n",
            "Epoch 29/50 | Batch 119/198 | Loss: 1.6713491678237915\n",
            "Epoch 29/50 | Batch 120/198 | Loss: 1.5503093004226685\n",
            "Epoch 29/50 | Batch 121/198 | Loss: 1.6687637567520142\n",
            "Epoch 29/50 | Batch 122/198 | Loss: 1.3595643043518066\n",
            "Epoch 29/50 | Batch 123/198 | Loss: 1.625710129737854\n",
            "Epoch 29/50 | Batch 124/198 | Loss: 2.0729665756225586\n",
            "Epoch 29/50 | Batch 125/198 | Loss: 1.9264159202575684\n",
            "Epoch 29/50 | Batch 126/198 | Loss: 1.606561303138733\n",
            "Epoch 29/50 | Batch 127/198 | Loss: 1.701475977897644\n",
            "Epoch 29/50 | Batch 128/198 | Loss: 1.7903668880462646\n",
            "Epoch 29/50 | Batch 129/198 | Loss: 1.5710368156433105\n",
            "Epoch 29/50 | Batch 130/198 | Loss: 1.4019834995269775\n",
            "Epoch 29/50 | Batch 131/198 | Loss: 1.8878916501998901\n",
            "Epoch 29/50 | Batch 132/198 | Loss: 1.50570547580719\n",
            "Epoch 29/50 | Batch 133/198 | Loss: 1.6331934928894043\n",
            "Epoch 29/50 | Batch 134/198 | Loss: 1.7052770853042603\n",
            "Epoch 29/50 | Batch 135/198 | Loss: 1.6088181734085083\n",
            "Epoch 29/50 | Batch 136/198 | Loss: 1.631373405456543\n",
            "Epoch 29/50 | Batch 137/198 | Loss: 1.6018294095993042\n",
            "Epoch 29/50 | Batch 138/198 | Loss: 1.70992910861969\n",
            "Epoch 29/50 | Batch 139/198 | Loss: 1.5569407939910889\n",
            "Epoch 29/50 | Batch 140/198 | Loss: 1.8080412149429321\n",
            "Epoch 29/50 | Batch 141/198 | Loss: 1.5619323253631592\n",
            "Epoch 29/50 | Batch 142/198 | Loss: 1.6602963209152222\n",
            "Epoch 29/50 | Batch 143/198 | Loss: 1.5905485153198242\n",
            "Epoch 29/50 | Batch 144/198 | Loss: 1.7203868627548218\n",
            "Epoch 29/50 | Batch 145/198 | Loss: 1.9135146141052246\n",
            "Epoch 29/50 | Batch 146/198 | Loss: 1.4926155805587769\n",
            "Epoch 29/50 | Batch 147/198 | Loss: 1.5335397720336914\n",
            "Epoch 29/50 | Batch 148/198 | Loss: 1.9683105945587158\n",
            "Epoch 29/50 | Batch 149/198 | Loss: 1.705725073814392\n",
            "Epoch 29/50 | Batch 150/198 | Loss: 1.5140427350997925\n",
            "Epoch 29/50 | Batch 151/198 | Loss: 1.4452290534973145\n",
            "Epoch 29/50 | Batch 152/198 | Loss: 1.5228053331375122\n",
            "Epoch 29/50 | Batch 153/198 | Loss: 1.4986766576766968\n",
            "Epoch 29/50 | Batch 154/198 | Loss: 1.700769305229187\n",
            "Epoch 29/50 | Batch 155/198 | Loss: 1.6425544023513794\n",
            "Epoch 29/50 | Batch 156/198 | Loss: 1.8195120096206665\n",
            "Epoch 29/50 | Batch 157/198 | Loss: 1.7648463249206543\n",
            "Epoch 29/50 | Batch 158/198 | Loss: 1.4936050176620483\n",
            "Epoch 29/50 | Batch 159/198 | Loss: 1.8086986541748047\n",
            "Epoch 29/50 | Batch 160/198 | Loss: 1.6679983139038086\n",
            "Epoch 29/50 | Batch 161/198 | Loss: 1.7387696504592896\n",
            "Epoch 29/50 | Batch 162/198 | Loss: 1.633057951927185\n",
            "Epoch 29/50 | Batch 163/198 | Loss: 1.483652949333191\n",
            "Epoch 29/50 | Batch 164/198 | Loss: 1.6593596935272217\n",
            "Epoch 29/50 | Batch 165/198 | Loss: 1.505371332168579\n",
            "Epoch 29/50 | Batch 166/198 | Loss: 1.7278227806091309\n",
            "Epoch 29/50 | Batch 167/198 | Loss: 1.573691725730896\n",
            "Epoch 29/50 | Batch 168/198 | Loss: 1.7854933738708496\n",
            "Epoch 29/50 | Batch 169/198 | Loss: 1.5433027744293213\n",
            "Epoch 29/50 | Batch 170/198 | Loss: 1.632885217666626\n",
            "Epoch 29/50 | Batch 171/198 | Loss: 1.4690293073654175\n",
            "Epoch 29/50 | Batch 172/198 | Loss: 1.5775529146194458\n",
            "Epoch 29/50 | Batch 173/198 | Loss: 1.7449753284454346\n",
            "Epoch 29/50 | Batch 174/198 | Loss: 1.8880983591079712\n",
            "Epoch 29/50 | Batch 175/198 | Loss: 1.6275254487991333\n",
            "Epoch 29/50 | Batch 176/198 | Loss: 1.6505793333053589\n",
            "Epoch 29/50 | Batch 177/198 | Loss: 1.3933703899383545\n",
            "Epoch 29/50 | Batch 178/198 | Loss: 1.4678326845169067\n",
            "Epoch 29/50 | Batch 179/198 | Loss: 1.7304331064224243\n",
            "Epoch 29/50 | Batch 180/198 | Loss: 1.5763027667999268\n",
            "Epoch 29/50 | Batch 181/198 | Loss: 1.7261608839035034\n",
            "Epoch 29/50 | Batch 182/198 | Loss: 1.6475632190704346\n",
            "Epoch 29/50 | Batch 183/198 | Loss: 1.8459789752960205\n",
            "Epoch 29/50 | Batch 184/198 | Loss: 1.766742467880249\n",
            "Epoch 29/50 | Batch 185/198 | Loss: 1.5932127237319946\n",
            "Epoch 29/50 | Batch 186/198 | Loss: 1.56667160987854\n",
            "Epoch 29/50 | Batch 187/198 | Loss: 1.4230029582977295\n",
            "Epoch 29/50 | Batch 188/198 | Loss: 1.3239914178848267\n",
            "Epoch 29/50 | Batch 189/198 | Loss: 1.7197086811065674\n",
            "Epoch 29/50 | Batch 190/198 | Loss: 1.7242395877838135\n",
            "Epoch 29/50 | Batch 191/198 | Loss: 1.711294174194336\n",
            "Epoch 29/50 | Batch 192/198 | Loss: 1.7235488891601562\n",
            "Epoch 29/50 | Batch 193/198 | Loss: 1.6501492261886597\n",
            "Epoch 29/50 | Batch 194/198 | Loss: 1.8822575807571411\n",
            "Epoch 29/50 | Batch 195/198 | Loss: 1.6131964921951294\n",
            "Epoch 29/50 | Batch 196/198 | Loss: 1.7407153844833374\n",
            "Epoch 29/50 | Batch 197/198 | Loss: 1.8152388334274292\n",
            "Epoch 29/50 | Batch 198/198 | Loss: 1.6795270442962646\n",
            "Epoch 29/50 | Average Loss: 1.651652622102487\n",
            "Epoch 30/50 | Batch 1/198 | Loss: 1.4842259883880615\n",
            "Epoch 30/50 | Batch 2/198 | Loss: 1.7691454887390137\n",
            "Epoch 30/50 | Batch 3/198 | Loss: 1.3715702295303345\n",
            "Epoch 30/50 | Batch 4/198 | Loss: 1.5389748811721802\n",
            "Epoch 30/50 | Batch 5/198 | Loss: 1.426147222518921\n",
            "Epoch 30/50 | Batch 6/198 | Loss: 1.8262146711349487\n",
            "Epoch 30/50 | Batch 7/198 | Loss: 1.588092565536499\n",
            "Epoch 30/50 | Batch 8/198 | Loss: 1.7076520919799805\n",
            "Epoch 30/50 | Batch 9/198 | Loss: 1.515146255493164\n",
            "Epoch 30/50 | Batch 10/198 | Loss: 1.7387984991073608\n",
            "Epoch 30/50 | Batch 11/198 | Loss: 1.4926793575286865\n",
            "Epoch 30/50 | Batch 12/198 | Loss: 1.6213257312774658\n",
            "Epoch 30/50 | Batch 13/198 | Loss: 1.465698003768921\n",
            "Epoch 30/50 | Batch 14/198 | Loss: 1.5291138887405396\n",
            "Epoch 30/50 | Batch 15/198 | Loss: 1.4624391794204712\n",
            "Epoch 30/50 | Batch 16/198 | Loss: 1.6185623407363892\n",
            "Epoch 30/50 | Batch 17/198 | Loss: 1.7609364986419678\n",
            "Epoch 30/50 | Batch 18/198 | Loss: 1.6787408590316772\n",
            "Epoch 30/50 | Batch 19/198 | Loss: 1.6023504734039307\n",
            "Epoch 30/50 | Batch 20/198 | Loss: 1.4262537956237793\n",
            "Epoch 30/50 | Batch 21/198 | Loss: 1.4864777326583862\n",
            "Epoch 30/50 | Batch 22/198 | Loss: 1.5816621780395508\n",
            "Epoch 30/50 | Batch 23/198 | Loss: 1.7365249395370483\n",
            "Epoch 30/50 | Batch 24/198 | Loss: 1.82999587059021\n",
            "Epoch 30/50 | Batch 25/198 | Loss: 1.6456308364868164\n",
            "Epoch 30/50 | Batch 26/198 | Loss: 1.7649534940719604\n",
            "Epoch 30/50 | Batch 27/198 | Loss: 1.6306334733963013\n",
            "Epoch 30/50 | Batch 28/198 | Loss: 2.0423097610473633\n",
            "Epoch 30/50 | Batch 29/198 | Loss: 1.8688311576843262\n",
            "Epoch 30/50 | Batch 30/198 | Loss: 1.6070603132247925\n",
            "Epoch 30/50 | Batch 31/198 | Loss: 1.6723227500915527\n",
            "Epoch 30/50 | Batch 32/198 | Loss: 1.5640063285827637\n",
            "Epoch 30/50 | Batch 33/198 | Loss: 1.604014277458191\n",
            "Epoch 30/50 | Batch 34/198 | Loss: 1.8306771516799927\n",
            "Epoch 30/50 | Batch 35/198 | Loss: 1.5738886594772339\n",
            "Epoch 30/50 | Batch 36/198 | Loss: 1.7535669803619385\n",
            "Epoch 30/50 | Batch 37/198 | Loss: 1.82475745677948\n",
            "Epoch 30/50 | Batch 38/198 | Loss: 1.566760778427124\n",
            "Epoch 30/50 | Batch 39/198 | Loss: 1.6039366722106934\n",
            "Epoch 30/50 | Batch 40/198 | Loss: 1.55466628074646\n",
            "Epoch 30/50 | Batch 41/198 | Loss: 1.764784336090088\n",
            "Epoch 30/50 | Batch 42/198 | Loss: 1.6310051679611206\n",
            "Epoch 30/50 | Batch 43/198 | Loss: 1.5619451999664307\n",
            "Epoch 30/50 | Batch 44/198 | Loss: 1.6122050285339355\n",
            "Epoch 30/50 | Batch 45/198 | Loss: 1.556294322013855\n",
            "Epoch 30/50 | Batch 46/198 | Loss: 1.6682618856430054\n",
            "Epoch 30/50 | Batch 47/198 | Loss: 1.6216155290603638\n",
            "Epoch 30/50 | Batch 48/198 | Loss: 1.6646437644958496\n",
            "Epoch 30/50 | Batch 49/198 | Loss: 1.6918013095855713\n",
            "Epoch 30/50 | Batch 50/198 | Loss: 1.7054286003112793\n",
            "Epoch 30/50 | Batch 51/198 | Loss: 1.6963131427764893\n",
            "Epoch 30/50 | Batch 52/198 | Loss: 1.582000494003296\n",
            "Epoch 30/50 | Batch 53/198 | Loss: 1.653383731842041\n",
            "Epoch 30/50 | Batch 54/198 | Loss: 1.5996941328048706\n",
            "Epoch 30/50 | Batch 55/198 | Loss: 1.6670101881027222\n",
            "Epoch 30/50 | Batch 56/198 | Loss: 1.5291255712509155\n",
            "Epoch 30/50 | Batch 57/198 | Loss: 1.9017224311828613\n",
            "Epoch 30/50 | Batch 58/198 | Loss: 1.61520254611969\n",
            "Epoch 30/50 | Batch 59/198 | Loss: 1.656064510345459\n",
            "Epoch 30/50 | Batch 60/198 | Loss: 1.793319582939148\n",
            "Epoch 30/50 | Batch 61/198 | Loss: 1.419426441192627\n",
            "Epoch 30/50 | Batch 62/198 | Loss: 1.7149794101715088\n",
            "Epoch 30/50 | Batch 63/198 | Loss: 1.5367491245269775\n",
            "Epoch 30/50 | Batch 64/198 | Loss: 1.7674511671066284\n",
            "Epoch 30/50 | Batch 65/198 | Loss: 1.6880018711090088\n",
            "Epoch 30/50 | Batch 66/198 | Loss: 1.5102894306182861\n",
            "Epoch 30/50 | Batch 67/198 | Loss: 1.5928994417190552\n",
            "Epoch 30/50 | Batch 68/198 | Loss: 1.4344463348388672\n",
            "Epoch 30/50 | Batch 69/198 | Loss: 1.7523777484893799\n",
            "Epoch 30/50 | Batch 70/198 | Loss: 1.644814133644104\n",
            "Epoch 30/50 | Batch 71/198 | Loss: 1.7016054391860962\n",
            "Epoch 30/50 | Batch 72/198 | Loss: 1.6143229007720947\n",
            "Epoch 30/50 | Batch 73/198 | Loss: 1.6186785697937012\n",
            "Epoch 30/50 | Batch 74/198 | Loss: 1.5670603513717651\n",
            "Epoch 30/50 | Batch 75/198 | Loss: 1.526403546333313\n",
            "Epoch 30/50 | Batch 76/198 | Loss: 1.8557825088500977\n",
            "Epoch 30/50 | Batch 77/198 | Loss: 1.5962752103805542\n",
            "Epoch 30/50 | Batch 78/198 | Loss: 1.5506998300552368\n",
            "Epoch 30/50 | Batch 79/198 | Loss: 1.4262362718582153\n",
            "Epoch 30/50 | Batch 80/198 | Loss: 1.4837298393249512\n",
            "Epoch 30/50 | Batch 81/198 | Loss: 1.4281378984451294\n",
            "Epoch 30/50 | Batch 82/198 | Loss: 1.6361044645309448\n",
            "Epoch 30/50 | Batch 83/198 | Loss: 1.7215937376022339\n",
            "Epoch 30/50 | Batch 84/198 | Loss: 1.6650758981704712\n",
            "Epoch 30/50 | Batch 85/198 | Loss: 1.694502830505371\n",
            "Epoch 30/50 | Batch 86/198 | Loss: 1.5299806594848633\n",
            "Epoch 30/50 | Batch 87/198 | Loss: 1.6263421773910522\n",
            "Epoch 30/50 | Batch 88/198 | Loss: 1.579898476600647\n",
            "Epoch 30/50 | Batch 89/198 | Loss: 1.5990170240402222\n",
            "Epoch 30/50 | Batch 90/198 | Loss: 1.7640541791915894\n",
            "Epoch 30/50 | Batch 91/198 | Loss: 1.5199109315872192\n",
            "Epoch 30/50 | Batch 92/198 | Loss: 1.548633098602295\n",
            "Epoch 30/50 | Batch 93/198 | Loss: 1.7030308246612549\n",
            "Epoch 30/50 | Batch 94/198 | Loss: 1.511042594909668\n",
            "Epoch 30/50 | Batch 95/198 | Loss: 1.4467501640319824\n",
            "Epoch 30/50 | Batch 96/198 | Loss: 1.709298849105835\n",
            "Epoch 30/50 | Batch 97/198 | Loss: 1.7954840660095215\n",
            "Epoch 30/50 | Batch 98/198 | Loss: 1.7848684787750244\n",
            "Epoch 30/50 | Batch 99/198 | Loss: 1.5844786167144775\n",
            "Epoch 30/50 | Batch 100/198 | Loss: 1.8568669557571411\n",
            "Epoch 30/50 | Batch 101/198 | Loss: 1.4667093753814697\n",
            "Epoch 30/50 | Batch 102/198 | Loss: 1.5654529333114624\n",
            "Epoch 30/50 | Batch 103/198 | Loss: 1.6777921915054321\n",
            "Epoch 30/50 | Batch 104/198 | Loss: 1.506385326385498\n",
            "Epoch 30/50 | Batch 105/198 | Loss: 1.5677273273468018\n",
            "Epoch 30/50 | Batch 106/198 | Loss: 1.3715474605560303\n",
            "Epoch 30/50 | Batch 107/198 | Loss: 1.4649397134780884\n",
            "Epoch 30/50 | Batch 108/198 | Loss: 1.585385799407959\n",
            "Epoch 30/50 | Batch 109/198 | Loss: 1.3655505180358887\n",
            "Epoch 30/50 | Batch 110/198 | Loss: 1.8788418769836426\n",
            "Epoch 30/50 | Batch 111/198 | Loss: 1.5010173320770264\n",
            "Epoch 30/50 | Batch 112/198 | Loss: 1.7901629209518433\n",
            "Epoch 30/50 | Batch 113/198 | Loss: 1.5472244024276733\n",
            "Epoch 30/50 | Batch 114/198 | Loss: 1.7250906229019165\n",
            "Epoch 30/50 | Batch 115/198 | Loss: 1.5411001443862915\n",
            "Epoch 30/50 | Batch 116/198 | Loss: 1.3599807024002075\n",
            "Epoch 30/50 | Batch 117/198 | Loss: 1.6916673183441162\n",
            "Epoch 30/50 | Batch 118/198 | Loss: 1.755275011062622\n",
            "Epoch 30/50 | Batch 119/198 | Loss: 1.7519822120666504\n",
            "Epoch 30/50 | Batch 120/198 | Loss: 1.5208563804626465\n",
            "Epoch 30/50 | Batch 121/198 | Loss: 1.5538089275360107\n",
            "Epoch 30/50 | Batch 122/198 | Loss: 1.5118732452392578\n",
            "Epoch 30/50 | Batch 123/198 | Loss: 1.74295973777771\n",
            "Epoch 30/50 | Batch 124/198 | Loss: 1.6847425699234009\n",
            "Epoch 30/50 | Batch 125/198 | Loss: 1.482468605041504\n",
            "Epoch 30/50 | Batch 126/198 | Loss: 1.7131191492080688\n",
            "Epoch 30/50 | Batch 127/198 | Loss: 1.3659701347351074\n",
            "Epoch 30/50 | Batch 128/198 | Loss: 1.7200922966003418\n",
            "Epoch 30/50 | Batch 129/198 | Loss: 1.748069405555725\n",
            "Epoch 30/50 | Batch 130/198 | Loss: 1.5986547470092773\n",
            "Epoch 30/50 | Batch 131/198 | Loss: 1.6360646486282349\n",
            "Epoch 30/50 | Batch 132/198 | Loss: 1.6935207843780518\n",
            "Epoch 30/50 | Batch 133/198 | Loss: 1.5485578775405884\n",
            "Epoch 30/50 | Batch 134/198 | Loss: 1.5112601518630981\n",
            "Epoch 30/50 | Batch 135/198 | Loss: 1.6046327352523804\n",
            "Epoch 30/50 | Batch 136/198 | Loss: 1.5941798686981201\n",
            "Epoch 30/50 | Batch 137/198 | Loss: 1.148506760597229\n",
            "Epoch 30/50 | Batch 138/198 | Loss: 1.625274658203125\n",
            "Epoch 30/50 | Batch 139/198 | Loss: 1.6904022693634033\n",
            "Epoch 30/50 | Batch 140/198 | Loss: 1.5017080307006836\n",
            "Epoch 30/50 | Batch 141/198 | Loss: 1.6641207933425903\n",
            "Epoch 30/50 | Batch 142/198 | Loss: 1.846571922302246\n",
            "Epoch 30/50 | Batch 143/198 | Loss: 1.6582207679748535\n",
            "Epoch 30/50 | Batch 144/198 | Loss: 1.7445660829544067\n",
            "Epoch 30/50 | Batch 145/198 | Loss: 1.3857121467590332\n",
            "Epoch 30/50 | Batch 146/198 | Loss: 1.5964000225067139\n",
            "Epoch 30/50 | Batch 147/198 | Loss: 1.8363099098205566\n",
            "Epoch 30/50 | Batch 148/198 | Loss: 1.3636553287506104\n",
            "Epoch 30/50 | Batch 149/198 | Loss: 1.5795568227767944\n",
            "Epoch 30/50 | Batch 150/198 | Loss: 1.677852988243103\n",
            "Epoch 30/50 | Batch 151/198 | Loss: 1.5543900728225708\n",
            "Epoch 30/50 | Batch 152/198 | Loss: 1.7056362628936768\n",
            "Epoch 30/50 | Batch 153/198 | Loss: 1.7217493057250977\n",
            "Epoch 30/50 | Batch 154/198 | Loss: 1.4882394075393677\n",
            "Epoch 30/50 | Batch 155/198 | Loss: 1.5879212617874146\n",
            "Epoch 30/50 | Batch 156/198 | Loss: 1.5356625318527222\n",
            "Epoch 30/50 | Batch 157/198 | Loss: 1.2472448348999023\n",
            "Epoch 30/50 | Batch 158/198 | Loss: 1.6606814861297607\n",
            "Epoch 30/50 | Batch 159/198 | Loss: 1.6523990631103516\n",
            "Epoch 30/50 | Batch 160/198 | Loss: 1.9110701084136963\n",
            "Epoch 30/50 | Batch 161/198 | Loss: 1.5487242937088013\n",
            "Epoch 30/50 | Batch 162/198 | Loss: 1.620727777481079\n",
            "Epoch 30/50 | Batch 163/198 | Loss: 1.6473561525344849\n",
            "Epoch 30/50 | Batch 164/198 | Loss: 1.5683685541152954\n",
            "Epoch 30/50 | Batch 165/198 | Loss: 1.3766863346099854\n",
            "Epoch 30/50 | Batch 166/198 | Loss: 1.5031862258911133\n",
            "Epoch 30/50 | Batch 167/198 | Loss: 1.7278621196746826\n",
            "Epoch 30/50 | Batch 168/198 | Loss: 1.8781315088272095\n",
            "Epoch 30/50 | Batch 169/198 | Loss: 1.4484484195709229\n",
            "Epoch 30/50 | Batch 170/198 | Loss: 1.6738944053649902\n",
            "Epoch 30/50 | Batch 171/198 | Loss: 1.666884422302246\n",
            "Epoch 30/50 | Batch 172/198 | Loss: 1.6688084602355957\n",
            "Epoch 30/50 | Batch 173/198 | Loss: 1.749283790588379\n",
            "Epoch 30/50 | Batch 174/198 | Loss: 1.4380072355270386\n",
            "Epoch 30/50 | Batch 175/198 | Loss: 1.4198007583618164\n",
            "Epoch 30/50 | Batch 176/198 | Loss: 1.679641604423523\n",
            "Epoch 30/50 | Batch 177/198 | Loss: 1.5174469947814941\n",
            "Epoch 30/50 | Batch 178/198 | Loss: 1.5634949207305908\n",
            "Epoch 30/50 | Batch 179/198 | Loss: 1.8457884788513184\n",
            "Epoch 30/50 | Batch 180/198 | Loss: 1.6140180826187134\n",
            "Epoch 30/50 | Batch 181/198 | Loss: 1.6186691522598267\n",
            "Epoch 30/50 | Batch 182/198 | Loss: 1.363433837890625\n",
            "Epoch 30/50 | Batch 183/198 | Loss: 1.4451417922973633\n",
            "Epoch 30/50 | Batch 184/198 | Loss: 1.6379094123840332\n",
            "Epoch 30/50 | Batch 185/198 | Loss: 1.685386300086975\n",
            "Epoch 30/50 | Batch 186/198 | Loss: 1.7905299663543701\n",
            "Epoch 30/50 | Batch 187/198 | Loss: 1.6418906450271606\n",
            "Epoch 30/50 | Batch 188/198 | Loss: 1.7182221412658691\n",
            "Epoch 30/50 | Batch 189/198 | Loss: 1.4315871000289917\n",
            "Epoch 30/50 | Batch 190/198 | Loss: 1.8112380504608154\n",
            "Epoch 30/50 | Batch 191/198 | Loss: 1.6485379934310913\n",
            "Epoch 30/50 | Batch 192/198 | Loss: 1.5652934312820435\n",
            "Epoch 30/50 | Batch 193/198 | Loss: 1.7121220827102661\n",
            "Epoch 30/50 | Batch 194/198 | Loss: 1.801072120666504\n",
            "Epoch 30/50 | Batch 195/198 | Loss: 1.6765104532241821\n",
            "Epoch 30/50 | Batch 196/198 | Loss: 1.7349594831466675\n",
            "Epoch 30/50 | Batch 197/198 | Loss: 1.7594720125198364\n",
            "Epoch 30/50 | Batch 198/198 | Loss: 1.6217273473739624\n",
            "Epoch 30/50 | Average Loss: 1.6201749961785596\n",
            "Epoch 31/50 | Batch 1/198 | Loss: 1.5840144157409668\n",
            "Epoch 31/50 | Batch 2/198 | Loss: 1.4385592937469482\n",
            "Epoch 31/50 | Batch 3/198 | Loss: 1.6243971586227417\n",
            "Epoch 31/50 | Batch 4/198 | Loss: 1.4222289323806763\n",
            "Epoch 31/50 | Batch 5/198 | Loss: 1.6315313577651978\n",
            "Epoch 31/50 | Batch 6/198 | Loss: 1.6562669277191162\n",
            "Epoch 31/50 | Batch 7/198 | Loss: 1.3641996383666992\n",
            "Epoch 31/50 | Batch 8/198 | Loss: 1.5399761199951172\n",
            "Epoch 31/50 | Batch 9/198 | Loss: 1.6642053127288818\n",
            "Epoch 31/50 | Batch 10/198 | Loss: 1.6056716442108154\n",
            "Epoch 31/50 | Batch 11/198 | Loss: 1.6307690143585205\n",
            "Epoch 31/50 | Batch 12/198 | Loss: 1.685763955116272\n",
            "Epoch 31/50 | Batch 13/198 | Loss: 1.455656886100769\n",
            "Epoch 31/50 | Batch 14/198 | Loss: 1.3467448949813843\n",
            "Epoch 31/50 | Batch 15/198 | Loss: 1.586959958076477\n",
            "Epoch 31/50 | Batch 16/198 | Loss: 1.615689992904663\n",
            "Epoch 31/50 | Batch 17/198 | Loss: 1.6851285696029663\n",
            "Epoch 31/50 | Batch 18/198 | Loss: 1.5355563163757324\n",
            "Epoch 31/50 | Batch 19/198 | Loss: 1.8706015348434448\n",
            "Epoch 31/50 | Batch 20/198 | Loss: 1.451089859008789\n",
            "Epoch 31/50 | Batch 21/198 | Loss: 1.4857592582702637\n",
            "Epoch 31/50 | Batch 22/198 | Loss: 1.533023476600647\n",
            "Epoch 31/50 | Batch 23/198 | Loss: 1.4819661378860474\n",
            "Epoch 31/50 | Batch 24/198 | Loss: 1.6277434825897217\n",
            "Epoch 31/50 | Batch 25/198 | Loss: 1.8033113479614258\n",
            "Epoch 31/50 | Batch 26/198 | Loss: 1.6005687713623047\n",
            "Epoch 31/50 | Batch 27/198 | Loss: 1.609351634979248\n",
            "Epoch 31/50 | Batch 28/198 | Loss: 1.190045714378357\n",
            "Epoch 31/50 | Batch 29/198 | Loss: 1.4421300888061523\n",
            "Epoch 31/50 | Batch 30/198 | Loss: 1.5461640357971191\n",
            "Epoch 31/50 | Batch 31/198 | Loss: 1.7527028322219849\n",
            "Epoch 31/50 | Batch 32/198 | Loss: 1.5446925163269043\n",
            "Epoch 31/50 | Batch 33/198 | Loss: 1.5655020475387573\n",
            "Epoch 31/50 | Batch 34/198 | Loss: 1.5585073232650757\n",
            "Epoch 31/50 | Batch 35/198 | Loss: 1.5750598907470703\n",
            "Epoch 31/50 | Batch 36/198 | Loss: 1.619722843170166\n",
            "Epoch 31/50 | Batch 37/198 | Loss: 1.4937107563018799\n",
            "Epoch 31/50 | Batch 38/198 | Loss: 1.572953462600708\n",
            "Epoch 31/50 | Batch 39/198 | Loss: 1.6117894649505615\n",
            "Epoch 31/50 | Batch 40/198 | Loss: 1.5272597074508667\n",
            "Epoch 31/50 | Batch 41/198 | Loss: 1.7312211990356445\n",
            "Epoch 31/50 | Batch 42/198 | Loss: 1.2468409538269043\n",
            "Epoch 31/50 | Batch 43/198 | Loss: 1.4859563112258911\n",
            "Epoch 31/50 | Batch 44/198 | Loss: 1.7314420938491821\n",
            "Epoch 31/50 | Batch 45/198 | Loss: 1.5717673301696777\n",
            "Epoch 31/50 | Batch 46/198 | Loss: 1.5993105173110962\n",
            "Epoch 31/50 | Batch 47/198 | Loss: 1.5585126876831055\n",
            "Epoch 31/50 | Batch 48/198 | Loss: 1.49984872341156\n",
            "Epoch 31/50 | Batch 49/198 | Loss: 1.726090669631958\n",
            "Epoch 31/50 | Batch 50/198 | Loss: 1.5787715911865234\n",
            "Epoch 31/50 | Batch 51/198 | Loss: 1.839440941810608\n",
            "Epoch 31/50 | Batch 52/198 | Loss: 1.7173272371292114\n",
            "Epoch 31/50 | Batch 53/198 | Loss: 1.5896683931350708\n",
            "Epoch 31/50 | Batch 54/198 | Loss: 1.621389627456665\n",
            "Epoch 31/50 | Batch 55/198 | Loss: 1.4874714612960815\n",
            "Epoch 31/50 | Batch 56/198 | Loss: 1.7644286155700684\n",
            "Epoch 31/50 | Batch 57/198 | Loss: 1.7073630094528198\n",
            "Epoch 31/50 | Batch 58/198 | Loss: 1.4172215461730957\n",
            "Epoch 31/50 | Batch 59/198 | Loss: 1.488297462463379\n",
            "Epoch 31/50 | Batch 60/198 | Loss: 1.6554229259490967\n",
            "Epoch 31/50 | Batch 61/198 | Loss: 1.4788342714309692\n",
            "Epoch 31/50 | Batch 62/198 | Loss: 1.7068893909454346\n",
            "Epoch 31/50 | Batch 63/198 | Loss: 1.6603960990905762\n",
            "Epoch 31/50 | Batch 64/198 | Loss: 1.5811245441436768\n",
            "Epoch 31/50 | Batch 65/198 | Loss: 1.498415231704712\n",
            "Epoch 31/50 | Batch 66/198 | Loss: 1.3754608631134033\n",
            "Epoch 31/50 | Batch 67/198 | Loss: 1.7977421283721924\n",
            "Epoch 31/50 | Batch 68/198 | Loss: 1.4092178344726562\n",
            "Epoch 31/50 | Batch 69/198 | Loss: 1.8172310590744019\n",
            "Epoch 31/50 | Batch 70/198 | Loss: 1.5191783905029297\n",
            "Epoch 31/50 | Batch 71/198 | Loss: 1.4712573289871216\n",
            "Epoch 31/50 | Batch 72/198 | Loss: 1.6901071071624756\n",
            "Epoch 31/50 | Batch 73/198 | Loss: 1.474166750907898\n",
            "Epoch 31/50 | Batch 74/198 | Loss: 1.4181398153305054\n",
            "Epoch 31/50 | Batch 75/198 | Loss: 1.5603052377700806\n",
            "Epoch 31/50 | Batch 76/198 | Loss: 1.4908490180969238\n",
            "Epoch 31/50 | Batch 77/198 | Loss: 1.4185196161270142\n",
            "Epoch 31/50 | Batch 78/198 | Loss: 1.4784139394760132\n",
            "Epoch 31/50 | Batch 79/198 | Loss: 1.6838490962982178\n",
            "Epoch 31/50 | Batch 80/198 | Loss: 1.4945666790008545\n",
            "Epoch 31/50 | Batch 81/198 | Loss: 1.6837044954299927\n",
            "Epoch 31/50 | Batch 82/198 | Loss: 1.6361802816390991\n",
            "Epoch 31/50 | Batch 83/198 | Loss: 1.583297610282898\n",
            "Epoch 31/50 | Batch 84/198 | Loss: 1.7704569101333618\n",
            "Epoch 31/50 | Batch 85/198 | Loss: 1.414188265800476\n",
            "Epoch 31/50 | Batch 86/198 | Loss: 1.566085696220398\n",
            "Epoch 31/50 | Batch 87/198 | Loss: 1.607657790184021\n",
            "Epoch 31/50 | Batch 88/198 | Loss: 1.581324815750122\n",
            "Epoch 31/50 | Batch 89/198 | Loss: 1.5750064849853516\n",
            "Epoch 31/50 | Batch 90/198 | Loss: 1.4905277490615845\n",
            "Epoch 31/50 | Batch 91/198 | Loss: 1.6709288358688354\n",
            "Epoch 31/50 | Batch 92/198 | Loss: 1.5609228610992432\n",
            "Epoch 31/50 | Batch 93/198 | Loss: 1.8353089094161987\n",
            "Epoch 31/50 | Batch 94/198 | Loss: 1.410775065422058\n",
            "Epoch 31/50 | Batch 95/198 | Loss: 1.730040431022644\n",
            "Epoch 31/50 | Batch 96/198 | Loss: 1.395932912826538\n",
            "Epoch 31/50 | Batch 97/198 | Loss: 1.7982217073440552\n",
            "Epoch 31/50 | Batch 98/198 | Loss: 1.4386200904846191\n",
            "Epoch 31/50 | Batch 99/198 | Loss: 1.5129321813583374\n",
            "Epoch 31/50 | Batch 100/198 | Loss: 1.7006157636642456\n",
            "Epoch 31/50 | Batch 101/198 | Loss: 1.3889591693878174\n",
            "Epoch 31/50 | Batch 102/198 | Loss: 1.4015766382217407\n",
            "Epoch 31/50 | Batch 103/198 | Loss: 1.4764811992645264\n",
            "Epoch 31/50 | Batch 104/198 | Loss: 1.490142822265625\n",
            "Epoch 31/50 | Batch 105/198 | Loss: 1.634865164756775\n",
            "Epoch 31/50 | Batch 106/198 | Loss: 1.4976513385772705\n",
            "Epoch 31/50 | Batch 107/198 | Loss: 1.6967077255249023\n",
            "Epoch 31/50 | Batch 108/198 | Loss: 1.7801544666290283\n",
            "Epoch 31/50 | Batch 109/198 | Loss: 1.695332407951355\n",
            "Epoch 31/50 | Batch 110/198 | Loss: 1.6064658164978027\n",
            "Epoch 31/50 | Batch 111/198 | Loss: 1.6713320016860962\n",
            "Epoch 31/50 | Batch 112/198 | Loss: 1.6111879348754883\n",
            "Epoch 31/50 | Batch 113/198 | Loss: 1.6944698095321655\n",
            "Epoch 31/50 | Batch 114/198 | Loss: 1.516556978225708\n",
            "Epoch 31/50 | Batch 115/198 | Loss: 1.5954152345657349\n",
            "Epoch 31/50 | Batch 116/198 | Loss: 1.6772902011871338\n",
            "Epoch 31/50 | Batch 117/198 | Loss: 1.5383572578430176\n",
            "Epoch 31/50 | Batch 118/198 | Loss: 1.512800693511963\n",
            "Epoch 31/50 | Batch 119/198 | Loss: 1.4152898788452148\n",
            "Epoch 31/50 | Batch 120/198 | Loss: 1.5278750658035278\n",
            "Epoch 31/50 | Batch 121/198 | Loss: 1.472434401512146\n",
            "Epoch 31/50 | Batch 122/198 | Loss: 1.5529069900512695\n",
            "Epoch 31/50 | Batch 123/198 | Loss: 1.6720106601715088\n",
            "Epoch 31/50 | Batch 124/198 | Loss: 1.7370153665542603\n",
            "Epoch 31/50 | Batch 125/198 | Loss: 1.6925162076950073\n",
            "Epoch 31/50 | Batch 126/198 | Loss: 1.6174424886703491\n",
            "Epoch 31/50 | Batch 127/198 | Loss: 1.5811628103256226\n",
            "Epoch 31/50 | Batch 128/198 | Loss: 1.5417439937591553\n",
            "Epoch 31/50 | Batch 129/198 | Loss: 1.6076126098632812\n",
            "Epoch 31/50 | Batch 130/198 | Loss: 1.5478194952011108\n",
            "Epoch 31/50 | Batch 131/198 | Loss: 1.4876022338867188\n",
            "Epoch 31/50 | Batch 132/198 | Loss: 1.6770761013031006\n",
            "Epoch 31/50 | Batch 133/198 | Loss: 1.6907262802124023\n",
            "Epoch 31/50 | Batch 134/198 | Loss: 1.561061978340149\n",
            "Epoch 31/50 | Batch 135/198 | Loss: 1.6345044374465942\n",
            "Epoch 31/50 | Batch 136/198 | Loss: 1.5484956502914429\n",
            "Epoch 31/50 | Batch 137/198 | Loss: 1.4432026147842407\n",
            "Epoch 31/50 | Batch 138/198 | Loss: 1.579610824584961\n",
            "Epoch 31/50 | Batch 139/198 | Loss: 1.7314977645874023\n",
            "Epoch 31/50 | Batch 140/198 | Loss: 1.6277331113815308\n",
            "Epoch 31/50 | Batch 141/198 | Loss: 1.7746853828430176\n",
            "Epoch 31/50 | Batch 142/198 | Loss: 1.5638973712921143\n",
            "Epoch 31/50 | Batch 143/198 | Loss: 1.6232085227966309\n",
            "Epoch 31/50 | Batch 144/198 | Loss: 1.692996621131897\n",
            "Epoch 31/50 | Batch 145/198 | Loss: 1.9642199277877808\n",
            "Epoch 31/50 | Batch 146/198 | Loss: 1.611352801322937\n",
            "Epoch 31/50 | Batch 147/198 | Loss: 1.7379250526428223\n",
            "Epoch 31/50 | Batch 148/198 | Loss: 1.6977158784866333\n",
            "Epoch 31/50 | Batch 149/198 | Loss: 1.4418624639511108\n",
            "Epoch 31/50 | Batch 150/198 | Loss: 1.6919559240341187\n",
            "Epoch 31/50 | Batch 151/198 | Loss: 1.519190788269043\n",
            "Epoch 31/50 | Batch 152/198 | Loss: 1.6630479097366333\n",
            "Epoch 31/50 | Batch 153/198 | Loss: 1.5006134510040283\n",
            "Epoch 31/50 | Batch 154/198 | Loss: 1.6550724506378174\n",
            "Epoch 31/50 | Batch 155/198 | Loss: 1.6060105562210083\n",
            "Epoch 31/50 | Batch 156/198 | Loss: 1.5403393507003784\n",
            "Epoch 31/50 | Batch 157/198 | Loss: 1.6433182954788208\n",
            "Epoch 31/50 | Batch 158/198 | Loss: 1.7140731811523438\n",
            "Epoch 31/50 | Batch 159/198 | Loss: 1.6457022428512573\n",
            "Epoch 31/50 | Batch 160/198 | Loss: 1.8100095987319946\n",
            "Epoch 31/50 | Batch 161/198 | Loss: 1.6855474710464478\n",
            "Epoch 31/50 | Batch 162/198 | Loss: 1.7550843954086304\n",
            "Epoch 31/50 | Batch 163/198 | Loss: 1.7718844413757324\n",
            "Epoch 31/50 | Batch 164/198 | Loss: 1.5612995624542236\n",
            "Epoch 31/50 | Batch 165/198 | Loss: 1.7086015939712524\n",
            "Epoch 31/50 | Batch 166/198 | Loss: 1.2953351736068726\n",
            "Epoch 31/50 | Batch 167/198 | Loss: 1.5292651653289795\n",
            "Epoch 31/50 | Batch 168/198 | Loss: 1.629944920539856\n",
            "Epoch 31/50 | Batch 169/198 | Loss: 1.3042141199111938\n",
            "Epoch 31/50 | Batch 170/198 | Loss: 1.685056209564209\n",
            "Epoch 31/50 | Batch 171/198 | Loss: 1.4154859781265259\n",
            "Epoch 31/50 | Batch 172/198 | Loss: 1.5807874202728271\n",
            "Epoch 31/50 | Batch 173/198 | Loss: 1.6360927820205688\n",
            "Epoch 31/50 | Batch 174/198 | Loss: 1.48262619972229\n",
            "Epoch 31/50 | Batch 175/198 | Loss: 1.664331078529358\n",
            "Epoch 31/50 | Batch 176/198 | Loss: 1.769288420677185\n",
            "Epoch 31/50 | Batch 177/198 | Loss: 1.8229230642318726\n",
            "Epoch 31/50 | Batch 178/198 | Loss: 1.407181978225708\n",
            "Epoch 31/50 | Batch 179/198 | Loss: 1.5741878747940063\n",
            "Epoch 31/50 | Batch 180/198 | Loss: 1.6527694463729858\n",
            "Epoch 31/50 | Batch 181/198 | Loss: 1.5875060558319092\n",
            "Epoch 31/50 | Batch 182/198 | Loss: 1.6383863687515259\n",
            "Epoch 31/50 | Batch 183/198 | Loss: 1.449350118637085\n",
            "Epoch 31/50 | Batch 184/198 | Loss: 1.6740888357162476\n",
            "Epoch 31/50 | Batch 185/198 | Loss: 1.866167664527893\n",
            "Epoch 31/50 | Batch 186/198 | Loss: 1.715462565422058\n",
            "Epoch 31/50 | Batch 187/198 | Loss: 1.492918610572815\n",
            "Epoch 31/50 | Batch 188/198 | Loss: 1.7135674953460693\n",
            "Epoch 31/50 | Batch 189/198 | Loss: 1.776639699935913\n",
            "Epoch 31/50 | Batch 190/198 | Loss: 1.7858749628067017\n",
            "Epoch 31/50 | Batch 191/198 | Loss: 1.4281281232833862\n",
            "Epoch 31/50 | Batch 192/198 | Loss: 1.7552287578582764\n",
            "Epoch 31/50 | Batch 193/198 | Loss: 1.6241313219070435\n",
            "Epoch 31/50 | Batch 194/198 | Loss: 1.6522465944290161\n",
            "Epoch 31/50 | Batch 195/198 | Loss: 1.6283011436462402\n",
            "Epoch 31/50 | Batch 196/198 | Loss: 1.5052335262298584\n",
            "Epoch 31/50 | Batch 197/198 | Loss: 1.382960557937622\n",
            "Epoch 31/50 | Batch 198/198 | Loss: 1.4800431728363037\n",
            "Epoch 31/50 | Average Loss: 1.5917201596077042\n",
            "Epoch 32/50 | Batch 1/198 | Loss: 1.4860143661499023\n",
            "Epoch 32/50 | Batch 2/198 | Loss: 1.3829423189163208\n",
            "Epoch 32/50 | Batch 3/198 | Loss: 1.5673341751098633\n",
            "Epoch 32/50 | Batch 4/198 | Loss: 1.589158058166504\n",
            "Epoch 32/50 | Batch 5/198 | Loss: 1.3027504682540894\n",
            "Epoch 32/50 | Batch 6/198 | Loss: 1.647294521331787\n",
            "Epoch 32/50 | Batch 7/198 | Loss: 1.4691025018692017\n",
            "Epoch 32/50 | Batch 8/198 | Loss: 1.488745093345642\n",
            "Epoch 32/50 | Batch 9/198 | Loss: 1.5810519456863403\n",
            "Epoch 32/50 | Batch 10/198 | Loss: 1.5882887840270996\n",
            "Epoch 32/50 | Batch 11/198 | Loss: 1.33863365650177\n",
            "Epoch 32/50 | Batch 12/198 | Loss: 1.3708468675613403\n",
            "Epoch 32/50 | Batch 13/198 | Loss: 1.5001091957092285\n",
            "Epoch 32/50 | Batch 14/198 | Loss: 1.4747282266616821\n",
            "Epoch 32/50 | Batch 15/198 | Loss: 1.446293830871582\n",
            "Epoch 32/50 | Batch 16/198 | Loss: 1.419135570526123\n",
            "Epoch 32/50 | Batch 17/198 | Loss: 1.572011113166809\n",
            "Epoch 32/50 | Batch 18/198 | Loss: 1.5256869792938232\n",
            "Epoch 32/50 | Batch 19/198 | Loss: 1.4652520418167114\n",
            "Epoch 32/50 | Batch 20/198 | Loss: 1.5372217893600464\n",
            "Epoch 32/50 | Batch 21/198 | Loss: 1.5253653526306152\n",
            "Epoch 32/50 | Batch 22/198 | Loss: 1.6644912958145142\n",
            "Epoch 32/50 | Batch 23/198 | Loss: 1.5248188972473145\n",
            "Epoch 32/50 | Batch 24/198 | Loss: 1.4330443143844604\n",
            "Epoch 32/50 | Batch 25/198 | Loss: 1.5437215566635132\n",
            "Epoch 32/50 | Batch 26/198 | Loss: 1.6171284914016724\n",
            "Epoch 32/50 | Batch 27/198 | Loss: 1.7067129611968994\n",
            "Epoch 32/50 | Batch 28/198 | Loss: 1.6153650283813477\n",
            "Epoch 32/50 | Batch 29/198 | Loss: 1.6212412118911743\n",
            "Epoch 32/50 | Batch 30/198 | Loss: 1.3922373056411743\n",
            "Epoch 32/50 | Batch 31/198 | Loss: 1.5623165369033813\n",
            "Epoch 32/50 | Batch 32/198 | Loss: 1.6480334997177124\n",
            "Epoch 32/50 | Batch 33/198 | Loss: 1.6774096488952637\n",
            "Epoch 32/50 | Batch 34/198 | Loss: 1.4689912796020508\n",
            "Epoch 32/50 | Batch 35/198 | Loss: 1.3345184326171875\n",
            "Epoch 32/50 | Batch 36/198 | Loss: 1.72666597366333\n",
            "Epoch 32/50 | Batch 37/198 | Loss: 1.5533604621887207\n",
            "Epoch 32/50 | Batch 38/198 | Loss: 1.2644585371017456\n",
            "Epoch 32/50 | Batch 39/198 | Loss: 1.5624432563781738\n",
            "Epoch 32/50 | Batch 40/198 | Loss: 1.4832412004470825\n",
            "Epoch 32/50 | Batch 41/198 | Loss: 1.3667956590652466\n",
            "Epoch 32/50 | Batch 42/198 | Loss: 1.4960706233978271\n",
            "Epoch 32/50 | Batch 43/198 | Loss: 1.4414476156234741\n",
            "Epoch 32/50 | Batch 44/198 | Loss: 1.506865382194519\n",
            "Epoch 32/50 | Batch 45/198 | Loss: 1.6507699489593506\n",
            "Epoch 32/50 | Batch 46/198 | Loss: 1.6175094842910767\n",
            "Epoch 32/50 | Batch 47/198 | Loss: 1.6557124853134155\n",
            "Epoch 32/50 | Batch 48/198 | Loss: 1.65586256980896\n",
            "Epoch 32/50 | Batch 49/198 | Loss: 1.374858021736145\n",
            "Epoch 32/50 | Batch 50/198 | Loss: 1.6871401071548462\n",
            "Epoch 32/50 | Batch 51/198 | Loss: 1.6492135524749756\n",
            "Epoch 32/50 | Batch 52/198 | Loss: 1.4356969594955444\n",
            "Epoch 32/50 | Batch 53/198 | Loss: 1.5790746212005615\n",
            "Epoch 32/50 | Batch 54/198 | Loss: 1.5494840145111084\n",
            "Epoch 32/50 | Batch 55/198 | Loss: 1.7087441682815552\n",
            "Epoch 32/50 | Batch 56/198 | Loss: 1.6393020153045654\n",
            "Epoch 32/50 | Batch 57/198 | Loss: 1.5015783309936523\n",
            "Epoch 32/50 | Batch 58/198 | Loss: 1.5681400299072266\n",
            "Epoch 32/50 | Batch 59/198 | Loss: 1.720146656036377\n",
            "Epoch 32/50 | Batch 60/198 | Loss: 1.7985025644302368\n",
            "Epoch 32/50 | Batch 61/198 | Loss: 1.5144193172454834\n",
            "Epoch 32/50 | Batch 62/198 | Loss: 1.679736852645874\n",
            "Epoch 32/50 | Batch 63/198 | Loss: 1.4421396255493164\n",
            "Epoch 32/50 | Batch 64/198 | Loss: 1.5363831520080566\n",
            "Epoch 32/50 | Batch 65/198 | Loss: 1.8291399478912354\n",
            "Epoch 32/50 | Batch 66/198 | Loss: 1.7052172422409058\n",
            "Epoch 32/50 | Batch 67/198 | Loss: 1.683648705482483\n",
            "Epoch 32/50 | Batch 68/198 | Loss: 1.5155435800552368\n",
            "Epoch 32/50 | Batch 69/198 | Loss: 1.5625808238983154\n",
            "Epoch 32/50 | Batch 70/198 | Loss: 1.476792812347412\n",
            "Epoch 32/50 | Batch 71/198 | Loss: 1.6234267950057983\n",
            "Epoch 32/50 | Batch 72/198 | Loss: 1.413135290145874\n",
            "Epoch 32/50 | Batch 73/198 | Loss: 1.443859338760376\n",
            "Epoch 32/50 | Batch 74/198 | Loss: 1.7598377466201782\n",
            "Epoch 32/50 | Batch 75/198 | Loss: 1.4191324710845947\n",
            "Epoch 32/50 | Batch 76/198 | Loss: 1.5775197744369507\n",
            "Epoch 32/50 | Batch 77/198 | Loss: 1.6388741731643677\n",
            "Epoch 32/50 | Batch 78/198 | Loss: 1.6900473833084106\n",
            "Epoch 32/50 | Batch 79/198 | Loss: 1.4959059953689575\n",
            "Epoch 32/50 | Batch 80/198 | Loss: 1.6244478225708008\n",
            "Epoch 32/50 | Batch 81/198 | Loss: 1.57038414478302\n",
            "Epoch 32/50 | Batch 82/198 | Loss: 1.6154359579086304\n",
            "Epoch 32/50 | Batch 83/198 | Loss: 1.5784108638763428\n",
            "Epoch 32/50 | Batch 84/198 | Loss: 1.2870867252349854\n",
            "Epoch 32/50 | Batch 85/198 | Loss: 1.7126612663269043\n",
            "Epoch 32/50 | Batch 86/198 | Loss: 1.6423460245132446\n",
            "Epoch 32/50 | Batch 87/198 | Loss: 1.6295441389083862\n",
            "Epoch 32/50 | Batch 88/198 | Loss: 1.3553715944290161\n",
            "Epoch 32/50 | Batch 89/198 | Loss: 1.6233923435211182\n",
            "Epoch 32/50 | Batch 90/198 | Loss: 1.6135435104370117\n",
            "Epoch 32/50 | Batch 91/198 | Loss: 1.7384194135665894\n",
            "Epoch 32/50 | Batch 92/198 | Loss: 1.4766857624053955\n",
            "Epoch 32/50 | Batch 93/198 | Loss: 1.6789687871932983\n",
            "Epoch 32/50 | Batch 94/198 | Loss: 1.8054020404815674\n",
            "Epoch 32/50 | Batch 95/198 | Loss: 1.6283307075500488\n",
            "Epoch 32/50 | Batch 96/198 | Loss: 1.4922120571136475\n",
            "Epoch 32/50 | Batch 97/198 | Loss: 1.4707692861557007\n",
            "Epoch 32/50 | Batch 98/198 | Loss: 1.682921290397644\n",
            "Epoch 32/50 | Batch 99/198 | Loss: 1.5139902830123901\n",
            "Epoch 32/50 | Batch 100/198 | Loss: 1.6574997901916504\n",
            "Epoch 32/50 | Batch 101/198 | Loss: 1.5067682266235352\n",
            "Epoch 32/50 | Batch 102/198 | Loss: 1.2955293655395508\n",
            "Epoch 32/50 | Batch 103/198 | Loss: 1.5191800594329834\n",
            "Epoch 32/50 | Batch 104/198 | Loss: 1.604577660560608\n",
            "Epoch 32/50 | Batch 105/198 | Loss: 1.621010422706604\n",
            "Epoch 32/50 | Batch 106/198 | Loss: 1.6280077695846558\n",
            "Epoch 32/50 | Batch 107/198 | Loss: 1.5623483657836914\n",
            "Epoch 32/50 | Batch 108/198 | Loss: 1.528986930847168\n",
            "Epoch 32/50 | Batch 109/198 | Loss: 1.2722089290618896\n",
            "Epoch 32/50 | Batch 110/198 | Loss: 1.8119338750839233\n",
            "Epoch 32/50 | Batch 111/198 | Loss: 1.5845986604690552\n",
            "Epoch 32/50 | Batch 112/198 | Loss: 1.579696774482727\n",
            "Epoch 32/50 | Batch 113/198 | Loss: 1.5283669233322144\n",
            "Epoch 32/50 | Batch 114/198 | Loss: 1.6165026426315308\n",
            "Epoch 32/50 | Batch 115/198 | Loss: 1.714477300643921\n",
            "Epoch 32/50 | Batch 116/198 | Loss: 1.5668666362762451\n",
            "Epoch 32/50 | Batch 117/198 | Loss: 1.650439739227295\n",
            "Epoch 32/50 | Batch 118/198 | Loss: 1.3823891878128052\n",
            "Epoch 32/50 | Batch 119/198 | Loss: 1.4403694868087769\n",
            "Epoch 32/50 | Batch 120/198 | Loss: 1.6403502225875854\n",
            "Epoch 32/50 | Batch 121/198 | Loss: 1.6346638202667236\n",
            "Epoch 32/50 | Batch 122/198 | Loss: 1.4490201473236084\n",
            "Epoch 32/50 | Batch 123/198 | Loss: 1.4521342515945435\n",
            "Epoch 32/50 | Batch 124/198 | Loss: 1.4725313186645508\n",
            "Epoch 32/50 | Batch 125/198 | Loss: 1.6409155130386353\n",
            "Epoch 32/50 | Batch 126/198 | Loss: 1.5150176286697388\n",
            "Epoch 32/50 | Batch 127/198 | Loss: 1.3683725595474243\n",
            "Epoch 32/50 | Batch 128/198 | Loss: 1.5627412796020508\n",
            "Epoch 32/50 | Batch 129/198 | Loss: 1.4291937351226807\n",
            "Epoch 32/50 | Batch 130/198 | Loss: 1.4927178621292114\n",
            "Epoch 32/50 | Batch 131/198 | Loss: 1.3131654262542725\n",
            "Epoch 32/50 | Batch 132/198 | Loss: 1.4518418312072754\n",
            "Epoch 32/50 | Batch 133/198 | Loss: 1.7608972787857056\n",
            "Epoch 32/50 | Batch 134/198 | Loss: 1.48258638381958\n",
            "Epoch 32/50 | Batch 135/198 | Loss: 1.8574116230010986\n",
            "Epoch 32/50 | Batch 136/198 | Loss: 1.613154411315918\n",
            "Epoch 32/50 | Batch 137/198 | Loss: 1.6373162269592285\n",
            "Epoch 32/50 | Batch 138/198 | Loss: 1.6530884504318237\n",
            "Epoch 32/50 | Batch 139/198 | Loss: 1.5066194534301758\n",
            "Epoch 32/50 | Batch 140/198 | Loss: 1.6015794277191162\n",
            "Epoch 32/50 | Batch 141/198 | Loss: 1.446500301361084\n",
            "Epoch 32/50 | Batch 142/198 | Loss: 1.6539034843444824\n",
            "Epoch 32/50 | Batch 143/198 | Loss: 1.6487221717834473\n",
            "Epoch 32/50 | Batch 144/198 | Loss: 1.4644975662231445\n",
            "Epoch 32/50 | Batch 145/198 | Loss: 1.6691514253616333\n",
            "Epoch 32/50 | Batch 146/198 | Loss: 1.609513759613037\n",
            "Epoch 32/50 | Batch 147/198 | Loss: 1.4950575828552246\n",
            "Epoch 32/50 | Batch 148/198 | Loss: 1.668557047843933\n",
            "Epoch 32/50 | Batch 149/198 | Loss: 1.7474515438079834\n",
            "Epoch 32/50 | Batch 150/198 | Loss: 1.5777374505996704\n",
            "Epoch 32/50 | Batch 151/198 | Loss: 1.4545392990112305\n",
            "Epoch 32/50 | Batch 152/198 | Loss: 1.5006979703903198\n",
            "Epoch 32/50 | Batch 153/198 | Loss: 1.341748595237732\n",
            "Epoch 32/50 | Batch 154/198 | Loss: 1.7120308876037598\n",
            "Epoch 32/50 | Batch 155/198 | Loss: 1.5063414573669434\n",
            "Epoch 32/50 | Batch 156/198 | Loss: 1.7417497634887695\n",
            "Epoch 32/50 | Batch 157/198 | Loss: 1.7398518323898315\n",
            "Epoch 32/50 | Batch 158/198 | Loss: 1.6893024444580078\n",
            "Epoch 32/50 | Batch 159/198 | Loss: 1.4159901142120361\n",
            "Epoch 32/50 | Batch 160/198 | Loss: 1.4752765893936157\n",
            "Epoch 32/50 | Batch 161/198 | Loss: 1.484452247619629\n",
            "Epoch 32/50 | Batch 162/198 | Loss: 1.5909945964813232\n",
            "Epoch 32/50 | Batch 163/198 | Loss: 1.4465879201889038\n",
            "Epoch 32/50 | Batch 164/198 | Loss: 1.5119210481643677\n",
            "Epoch 32/50 | Batch 165/198 | Loss: 1.6620014905929565\n",
            "Epoch 32/50 | Batch 166/198 | Loss: 1.6189091205596924\n",
            "Epoch 32/50 | Batch 167/198 | Loss: 1.6168625354766846\n",
            "Epoch 32/50 | Batch 168/198 | Loss: 1.552162766456604\n",
            "Epoch 32/50 | Batch 169/198 | Loss: 1.7639836072921753\n",
            "Epoch 32/50 | Batch 170/198 | Loss: 1.7231227159500122\n",
            "Epoch 32/50 | Batch 171/198 | Loss: 1.7644720077514648\n",
            "Epoch 32/50 | Batch 172/198 | Loss: 1.7583602666854858\n",
            "Epoch 32/50 | Batch 173/198 | Loss: 1.3524038791656494\n",
            "Epoch 32/50 | Batch 174/198 | Loss: 1.5911977291107178\n",
            "Epoch 32/50 | Batch 175/198 | Loss: 1.6316579580307007\n",
            "Epoch 32/50 | Batch 176/198 | Loss: 1.7096960544586182\n",
            "Epoch 32/50 | Batch 177/198 | Loss: 1.4105076789855957\n",
            "Epoch 32/50 | Batch 178/198 | Loss: 1.4169840812683105\n",
            "Epoch 32/50 | Batch 179/198 | Loss: 1.4940552711486816\n",
            "Epoch 32/50 | Batch 180/198 | Loss: 1.6074624061584473\n",
            "Epoch 32/50 | Batch 181/198 | Loss: 1.8416273593902588\n",
            "Epoch 32/50 | Batch 182/198 | Loss: 1.588539958000183\n",
            "Epoch 32/50 | Batch 183/198 | Loss: 1.474801778793335\n",
            "Epoch 32/50 | Batch 184/198 | Loss: 1.7651910781860352\n",
            "Epoch 32/50 | Batch 185/198 | Loss: 1.4514083862304688\n",
            "Epoch 32/50 | Batch 186/198 | Loss: 1.6662298440933228\n",
            "Epoch 32/50 | Batch 187/198 | Loss: 1.6724332571029663\n",
            "Epoch 32/50 | Batch 188/198 | Loss: 1.572519063949585\n",
            "Epoch 32/50 | Batch 189/198 | Loss: 1.5531566143035889\n",
            "Epoch 32/50 | Batch 190/198 | Loss: 1.8333072662353516\n",
            "Epoch 32/50 | Batch 191/198 | Loss: 1.4066945314407349\n",
            "Epoch 32/50 | Batch 192/198 | Loss: 1.5788880586624146\n",
            "Epoch 32/50 | Batch 193/198 | Loss: 1.6668026447296143\n",
            "Epoch 32/50 | Batch 194/198 | Loss: 1.7475955486297607\n",
            "Epoch 32/50 | Batch 195/198 | Loss: 1.7894748449325562\n",
            "Epoch 32/50 | Batch 196/198 | Loss: 1.6738852262496948\n",
            "Epoch 32/50 | Batch 197/198 | Loss: 1.4629651308059692\n",
            "Epoch 32/50 | Batch 198/198 | Loss: 1.4874471426010132\n",
            "Epoch 32/50 | Average Loss: 1.5663394374076767\n",
            "Epoch 33/50 | Batch 1/198 | Loss: 1.5387903451919556\n",
            "Epoch 33/50 | Batch 2/198 | Loss: 1.633940577507019\n",
            "Epoch 33/50 | Batch 3/198 | Loss: 1.4162739515304565\n",
            "Epoch 33/50 | Batch 4/198 | Loss: 1.5701417922973633\n",
            "Epoch 33/50 | Batch 5/198 | Loss: 1.481728196144104\n",
            "Epoch 33/50 | Batch 6/198 | Loss: 1.5342772006988525\n",
            "Epoch 33/50 | Batch 7/198 | Loss: 1.3392834663391113\n",
            "Epoch 33/50 | Batch 8/198 | Loss: 1.638422966003418\n",
            "Epoch 33/50 | Batch 9/198 | Loss: 1.723816156387329\n",
            "Epoch 33/50 | Batch 10/198 | Loss: 1.539972186088562\n",
            "Epoch 33/50 | Batch 11/198 | Loss: 1.5942821502685547\n",
            "Epoch 33/50 | Batch 12/198 | Loss: 1.5503664016723633\n",
            "Epoch 33/50 | Batch 13/198 | Loss: 1.7248810529708862\n",
            "Epoch 33/50 | Batch 14/198 | Loss: 1.6249059438705444\n",
            "Epoch 33/50 | Batch 15/198 | Loss: 1.7551665306091309\n",
            "Epoch 33/50 | Batch 16/198 | Loss: 1.8859102725982666\n",
            "Epoch 33/50 | Batch 17/198 | Loss: 1.5823132991790771\n",
            "Epoch 33/50 | Batch 18/198 | Loss: 1.4997261762619019\n",
            "Epoch 33/50 | Batch 19/198 | Loss: 1.3893444538116455\n",
            "Epoch 33/50 | Batch 20/198 | Loss: 1.477482795715332\n",
            "Epoch 33/50 | Batch 21/198 | Loss: 1.5698175430297852\n",
            "Epoch 33/50 | Batch 22/198 | Loss: 1.5975373983383179\n",
            "Epoch 33/50 | Batch 23/198 | Loss: 1.563757061958313\n",
            "Epoch 33/50 | Batch 24/198 | Loss: 1.5295122861862183\n",
            "Epoch 33/50 | Batch 25/198 | Loss: 1.428214430809021\n",
            "Epoch 33/50 | Batch 26/198 | Loss: 1.5702468156814575\n",
            "Epoch 33/50 | Batch 27/198 | Loss: 1.5658173561096191\n",
            "Epoch 33/50 | Batch 28/198 | Loss: 1.6159125566482544\n",
            "Epoch 33/50 | Batch 29/198 | Loss: 1.3062734603881836\n",
            "Epoch 33/50 | Batch 30/198 | Loss: 1.4690946340560913\n",
            "Epoch 33/50 | Batch 31/198 | Loss: 1.5243021249771118\n",
            "Epoch 33/50 | Batch 32/198 | Loss: 1.3269016742706299\n",
            "Epoch 33/50 | Batch 33/198 | Loss: 1.3969533443450928\n",
            "Epoch 33/50 | Batch 34/198 | Loss: 1.6247397661209106\n",
            "Epoch 33/50 | Batch 35/198 | Loss: 1.5906463861465454\n",
            "Epoch 33/50 | Batch 36/198 | Loss: 1.6055216789245605\n",
            "Epoch 33/50 | Batch 37/198 | Loss: 1.4296797513961792\n",
            "Epoch 33/50 | Batch 38/198 | Loss: 1.6753631830215454\n",
            "Epoch 33/50 | Batch 39/198 | Loss: 1.27409029006958\n",
            "Epoch 33/50 | Batch 40/198 | Loss: 1.5241047143936157\n",
            "Epoch 33/50 | Batch 41/198 | Loss: 1.5931336879730225\n",
            "Epoch 33/50 | Batch 42/198 | Loss: 1.4443554878234863\n",
            "Epoch 33/50 | Batch 43/198 | Loss: 1.2899101972579956\n",
            "Epoch 33/50 | Batch 44/198 | Loss: 1.5448962450027466\n",
            "Epoch 33/50 | Batch 45/198 | Loss: 1.6262127161026\n",
            "Epoch 33/50 | Batch 46/198 | Loss: 1.3765298128128052\n",
            "Epoch 33/50 | Batch 47/198 | Loss: 1.5273014307022095\n",
            "Epoch 33/50 | Batch 48/198 | Loss: 1.6551448106765747\n",
            "Epoch 33/50 | Batch 49/198 | Loss: 1.4381015300750732\n",
            "Epoch 33/50 | Batch 50/198 | Loss: 1.553300142288208\n",
            "Epoch 33/50 | Batch 51/198 | Loss: 1.712788462638855\n",
            "Epoch 33/50 | Batch 52/198 | Loss: 1.6218454837799072\n",
            "Epoch 33/50 | Batch 53/198 | Loss: 1.8445053100585938\n",
            "Epoch 33/50 | Batch 54/198 | Loss: 1.5680358409881592\n",
            "Epoch 33/50 | Batch 55/198 | Loss: 1.4921462535858154\n",
            "Epoch 33/50 | Batch 56/198 | Loss: 1.520461916923523\n",
            "Epoch 33/50 | Batch 57/198 | Loss: 1.5818251371383667\n",
            "Epoch 33/50 | Batch 58/198 | Loss: 1.5764591693878174\n",
            "Epoch 33/50 | Batch 59/198 | Loss: 1.5163391828536987\n",
            "Epoch 33/50 | Batch 60/198 | Loss: 1.4107614755630493\n",
            "Epoch 33/50 | Batch 61/198 | Loss: 1.7017245292663574\n",
            "Epoch 33/50 | Batch 62/198 | Loss: 1.6473592519760132\n",
            "Epoch 33/50 | Batch 63/198 | Loss: 1.547654390335083\n",
            "Epoch 33/50 | Batch 64/198 | Loss: 1.4846904277801514\n",
            "Epoch 33/50 | Batch 65/198 | Loss: 1.7290267944335938\n",
            "Epoch 33/50 | Batch 66/198 | Loss: 1.416002869606018\n",
            "Epoch 33/50 | Batch 67/198 | Loss: 1.473309874534607\n",
            "Epoch 33/50 | Batch 68/198 | Loss: 1.6318403482437134\n",
            "Epoch 33/50 | Batch 69/198 | Loss: 1.4347347021102905\n",
            "Epoch 33/50 | Batch 70/198 | Loss: 1.3707131147384644\n",
            "Epoch 33/50 | Batch 71/198 | Loss: 1.5474412441253662\n",
            "Epoch 33/50 | Batch 72/198 | Loss: 1.5788042545318604\n",
            "Epoch 33/50 | Batch 73/198 | Loss: 1.5738970041275024\n",
            "Epoch 33/50 | Batch 74/198 | Loss: 1.464469075202942\n",
            "Epoch 33/50 | Batch 75/198 | Loss: 1.6510130167007446\n",
            "Epoch 33/50 | Batch 76/198 | Loss: 1.689201831817627\n",
            "Epoch 33/50 | Batch 77/198 | Loss: 1.5921170711517334\n",
            "Epoch 33/50 | Batch 78/198 | Loss: 1.5492950677871704\n",
            "Epoch 33/50 | Batch 79/198 | Loss: 1.4192441701889038\n",
            "Epoch 33/50 | Batch 80/198 | Loss: 1.4971323013305664\n",
            "Epoch 33/50 | Batch 81/198 | Loss: 1.5265965461730957\n",
            "Epoch 33/50 | Batch 82/198 | Loss: 1.5210809707641602\n",
            "Epoch 33/50 | Batch 83/198 | Loss: 1.3662922382354736\n",
            "Epoch 33/50 | Batch 84/198 | Loss: 1.690071940422058\n",
            "Epoch 33/50 | Batch 85/198 | Loss: 1.4892628192901611\n",
            "Epoch 33/50 | Batch 86/198 | Loss: 1.5319751501083374\n",
            "Epoch 33/50 | Batch 87/198 | Loss: 1.4095569849014282\n",
            "Epoch 33/50 | Batch 88/198 | Loss: 1.506796956062317\n",
            "Epoch 33/50 | Batch 89/198 | Loss: 1.498212218284607\n",
            "Epoch 33/50 | Batch 90/198 | Loss: 1.5227737426757812\n",
            "Epoch 33/50 | Batch 91/198 | Loss: 1.700631022453308\n",
            "Epoch 33/50 | Batch 92/198 | Loss: 1.5488072633743286\n",
            "Epoch 33/50 | Batch 93/198 | Loss: 1.4846134185791016\n",
            "Epoch 33/50 | Batch 94/198 | Loss: 1.6091177463531494\n",
            "Epoch 33/50 | Batch 95/198 | Loss: 1.516206979751587\n",
            "Epoch 33/50 | Batch 96/198 | Loss: 1.3062082529067993\n",
            "Epoch 33/50 | Batch 97/198 | Loss: 1.3438829183578491\n",
            "Epoch 33/50 | Batch 98/198 | Loss: 1.4080760478973389\n",
            "Epoch 33/50 | Batch 99/198 | Loss: 1.6583424806594849\n",
            "Epoch 33/50 | Batch 100/198 | Loss: 1.7155308723449707\n",
            "Epoch 33/50 | Batch 101/198 | Loss: 1.474851369857788\n",
            "Epoch 33/50 | Batch 102/198 | Loss: 1.6274877786636353\n",
            "Epoch 33/50 | Batch 103/198 | Loss: 1.7417545318603516\n",
            "Epoch 33/50 | Batch 104/198 | Loss: 1.326583743095398\n",
            "Epoch 33/50 | Batch 105/198 | Loss: 1.5023587942123413\n",
            "Epoch 33/50 | Batch 106/198 | Loss: 1.7336088418960571\n",
            "Epoch 33/50 | Batch 107/198 | Loss: 1.6358402967453003\n",
            "Epoch 33/50 | Batch 108/198 | Loss: 1.801125168800354\n",
            "Epoch 33/50 | Batch 109/198 | Loss: 1.85966956615448\n",
            "Epoch 33/50 | Batch 110/198 | Loss: 1.4848567247390747\n",
            "Epoch 33/50 | Batch 111/198 | Loss: 1.3525583744049072\n",
            "Epoch 33/50 | Batch 112/198 | Loss: 1.3925641775131226\n",
            "Epoch 33/50 | Batch 113/198 | Loss: 1.6264777183532715\n",
            "Epoch 33/50 | Batch 114/198 | Loss: 1.5877827405929565\n",
            "Epoch 33/50 | Batch 115/198 | Loss: 1.5558351278305054\n",
            "Epoch 33/50 | Batch 116/198 | Loss: 1.4982985258102417\n",
            "Epoch 33/50 | Batch 117/198 | Loss: 1.9023369550704956\n",
            "Epoch 33/50 | Batch 118/198 | Loss: 1.4241764545440674\n",
            "Epoch 33/50 | Batch 119/198 | Loss: 1.470516324043274\n",
            "Epoch 33/50 | Batch 120/198 | Loss: 1.6514078378677368\n",
            "Epoch 33/50 | Batch 121/198 | Loss: 1.6811274290084839\n",
            "Epoch 33/50 | Batch 122/198 | Loss: 1.5201247930526733\n",
            "Epoch 33/50 | Batch 123/198 | Loss: 1.4354583024978638\n",
            "Epoch 33/50 | Batch 124/198 | Loss: 1.3421998023986816\n",
            "Epoch 33/50 | Batch 125/198 | Loss: 1.5426535606384277\n",
            "Epoch 33/50 | Batch 126/198 | Loss: 1.6953219175338745\n",
            "Epoch 33/50 | Batch 127/198 | Loss: 1.6335148811340332\n",
            "Epoch 33/50 | Batch 128/198 | Loss: 1.3365857601165771\n",
            "Epoch 33/50 | Batch 129/198 | Loss: 1.5226980447769165\n",
            "Epoch 33/50 | Batch 130/198 | Loss: 1.279858946800232\n",
            "Epoch 33/50 | Batch 131/198 | Loss: 1.615235447883606\n",
            "Epoch 33/50 | Batch 132/198 | Loss: 1.389399766921997\n",
            "Epoch 33/50 | Batch 133/198 | Loss: 1.4880564212799072\n",
            "Epoch 33/50 | Batch 134/198 | Loss: 1.5934489965438843\n",
            "Epoch 33/50 | Batch 135/198 | Loss: 1.5062159299850464\n",
            "Epoch 33/50 | Batch 136/198 | Loss: 1.653385877609253\n",
            "Epoch 33/50 | Batch 137/198 | Loss: 1.5580780506134033\n",
            "Epoch 33/50 | Batch 138/198 | Loss: 1.2595067024230957\n",
            "Epoch 33/50 | Batch 139/198 | Loss: 1.576789379119873\n",
            "Epoch 33/50 | Batch 140/198 | Loss: 1.3941370248794556\n",
            "Epoch 33/50 | Batch 141/198 | Loss: 1.4721301794052124\n",
            "Epoch 33/50 | Batch 142/198 | Loss: 1.6785670518875122\n",
            "Epoch 33/50 | Batch 143/198 | Loss: 1.6477640867233276\n",
            "Epoch 33/50 | Batch 144/198 | Loss: 1.3944920301437378\n",
            "Epoch 33/50 | Batch 145/198 | Loss: 1.5305962562561035\n",
            "Epoch 33/50 | Batch 146/198 | Loss: 1.4661896228790283\n",
            "Epoch 33/50 | Batch 147/198 | Loss: 1.4171150922775269\n",
            "Epoch 33/50 | Batch 148/198 | Loss: 1.7383265495300293\n",
            "Epoch 33/50 | Batch 149/198 | Loss: 1.4598044157028198\n",
            "Epoch 33/50 | Batch 150/198 | Loss: 1.498362421989441\n",
            "Epoch 33/50 | Batch 151/198 | Loss: 1.4213587045669556\n",
            "Epoch 33/50 | Batch 152/198 | Loss: 1.6515661478042603\n",
            "Epoch 33/50 | Batch 153/198 | Loss: 1.476552963256836\n",
            "Epoch 33/50 | Batch 154/198 | Loss: 1.7665724754333496\n",
            "Epoch 33/50 | Batch 155/198 | Loss: 1.6226441860198975\n",
            "Epoch 33/50 | Batch 156/198 | Loss: 1.833364725112915\n",
            "Epoch 33/50 | Batch 157/198 | Loss: 1.5896512269973755\n",
            "Epoch 33/50 | Batch 158/198 | Loss: 1.502948522567749\n",
            "Epoch 33/50 | Batch 159/198 | Loss: 1.3931515216827393\n",
            "Epoch 33/50 | Batch 160/198 | Loss: 1.511674165725708\n",
            "Epoch 33/50 | Batch 161/198 | Loss: 1.3333994150161743\n",
            "Epoch 33/50 | Batch 162/198 | Loss: 1.5585952997207642\n",
            "Epoch 33/50 | Batch 163/198 | Loss: 1.6802257299423218\n",
            "Epoch 33/50 | Batch 164/198 | Loss: 1.4996029138565063\n",
            "Epoch 33/50 | Batch 165/198 | Loss: 1.5910167694091797\n",
            "Epoch 33/50 | Batch 166/198 | Loss: 1.3321765661239624\n",
            "Epoch 33/50 | Batch 167/198 | Loss: 1.413508653640747\n",
            "Epoch 33/50 | Batch 168/198 | Loss: 1.4564076662063599\n",
            "Epoch 33/50 | Batch 169/198 | Loss: 1.5817339420318604\n",
            "Epoch 33/50 | Batch 170/198 | Loss: 1.2874479293823242\n",
            "Epoch 33/50 | Batch 171/198 | Loss: 1.644014835357666\n",
            "Epoch 33/50 | Batch 172/198 | Loss: 1.6249020099639893\n",
            "Epoch 33/50 | Batch 173/198 | Loss: 1.6159591674804688\n",
            "Epoch 33/50 | Batch 174/198 | Loss: 1.5773781538009644\n",
            "Epoch 33/50 | Batch 175/198 | Loss: 1.3044774532318115\n",
            "Epoch 33/50 | Batch 176/198 | Loss: 1.4417858123779297\n",
            "Epoch 33/50 | Batch 177/198 | Loss: 1.6344900131225586\n",
            "Epoch 33/50 | Batch 178/198 | Loss: 1.7066686153411865\n",
            "Epoch 33/50 | Batch 179/198 | Loss: 1.6959092617034912\n",
            "Epoch 33/50 | Batch 180/198 | Loss: 1.4234052896499634\n",
            "Epoch 33/50 | Batch 181/198 | Loss: 1.4230566024780273\n",
            "Epoch 33/50 | Batch 182/198 | Loss: 1.7704172134399414\n",
            "Epoch 33/50 | Batch 183/198 | Loss: 1.4839093685150146\n",
            "Epoch 33/50 | Batch 184/198 | Loss: 1.278371810913086\n",
            "Epoch 33/50 | Batch 185/198 | Loss: 1.4016999006271362\n",
            "Epoch 33/50 | Batch 186/198 | Loss: 1.6764624118804932\n",
            "Epoch 33/50 | Batch 187/198 | Loss: 1.750209093093872\n",
            "Epoch 33/50 | Batch 188/198 | Loss: 1.5285786390304565\n",
            "Epoch 33/50 | Batch 189/198 | Loss: 1.7727105617523193\n",
            "Epoch 33/50 | Batch 190/198 | Loss: 1.5559293031692505\n",
            "Epoch 33/50 | Batch 191/198 | Loss: 1.3336308002471924\n",
            "Epoch 33/50 | Batch 192/198 | Loss: 1.676478624343872\n",
            "Epoch 33/50 | Batch 193/198 | Loss: 1.6440988779067993\n",
            "Epoch 33/50 | Batch 194/198 | Loss: 1.6768519878387451\n",
            "Epoch 33/50 | Batch 195/198 | Loss: 1.6613709926605225\n",
            "Epoch 33/50 | Batch 196/198 | Loss: 1.754659652709961\n",
            "Epoch 33/50 | Batch 197/198 | Loss: 1.694888710975647\n",
            "Epoch 33/50 | Batch 198/198 | Loss: 1.4258341789245605\n",
            "Epoch 33/50 | Average Loss: 1.5434154506885644\n",
            "Epoch 34/50 | Batch 1/198 | Loss: 1.50676691532135\n",
            "Epoch 34/50 | Batch 2/198 | Loss: 1.3121871948242188\n",
            "Epoch 34/50 | Batch 3/198 | Loss: 1.5309388637542725\n",
            "Epoch 34/50 | Batch 4/198 | Loss: 1.467837929725647\n",
            "Epoch 34/50 | Batch 5/198 | Loss: 1.5696121454238892\n",
            "Epoch 34/50 | Batch 6/198 | Loss: 1.605787992477417\n",
            "Epoch 34/50 | Batch 7/198 | Loss: 1.59071946144104\n",
            "Epoch 34/50 | Batch 8/198 | Loss: 1.4702656269073486\n",
            "Epoch 34/50 | Batch 9/198 | Loss: 1.6255803108215332\n",
            "Epoch 34/50 | Batch 10/198 | Loss: 1.5972447395324707\n",
            "Epoch 34/50 | Batch 11/198 | Loss: 1.6622142791748047\n",
            "Epoch 34/50 | Batch 12/198 | Loss: 1.5348454713821411\n",
            "Epoch 34/50 | Batch 13/198 | Loss: 1.5144221782684326\n",
            "Epoch 34/50 | Batch 14/198 | Loss: 1.259592890739441\n",
            "Epoch 34/50 | Batch 15/198 | Loss: 1.6760185956954956\n",
            "Epoch 34/50 | Batch 16/198 | Loss: 1.5659668445587158\n",
            "Epoch 34/50 | Batch 17/198 | Loss: 1.2646254301071167\n",
            "Epoch 34/50 | Batch 18/198 | Loss: 1.4988389015197754\n",
            "Epoch 34/50 | Batch 19/198 | Loss: 1.51461923122406\n",
            "Epoch 34/50 | Batch 20/198 | Loss: 1.707518219947815\n",
            "Epoch 34/50 | Batch 21/198 | Loss: 1.4633314609527588\n",
            "Epoch 34/50 | Batch 22/198 | Loss: 1.5022245645523071\n",
            "Epoch 34/50 | Batch 23/198 | Loss: 1.3874125480651855\n",
            "Epoch 34/50 | Batch 24/198 | Loss: 1.5233101844787598\n",
            "Epoch 34/50 | Batch 25/198 | Loss: 1.5809667110443115\n",
            "Epoch 34/50 | Batch 26/198 | Loss: 1.549285650253296\n",
            "Epoch 34/50 | Batch 27/198 | Loss: 1.344276785850525\n",
            "Epoch 34/50 | Batch 28/198 | Loss: 1.2667641639709473\n",
            "Epoch 34/50 | Batch 29/198 | Loss: 1.508515477180481\n",
            "Epoch 34/50 | Batch 30/198 | Loss: 1.421601414680481\n",
            "Epoch 34/50 | Batch 31/198 | Loss: 1.3051011562347412\n",
            "Epoch 34/50 | Batch 32/198 | Loss: 1.5063713788986206\n",
            "Epoch 34/50 | Batch 33/198 | Loss: 1.248043417930603\n",
            "Epoch 34/50 | Batch 34/198 | Loss: 1.493355393409729\n",
            "Epoch 34/50 | Batch 35/198 | Loss: 1.61182701587677\n",
            "Epoch 34/50 | Batch 36/198 | Loss: 1.6416347026824951\n",
            "Epoch 34/50 | Batch 37/198 | Loss: 1.8357338905334473\n",
            "Epoch 34/50 | Batch 38/198 | Loss: 1.5609023571014404\n",
            "Epoch 34/50 | Batch 39/198 | Loss: 1.5429338216781616\n",
            "Epoch 34/50 | Batch 40/198 | Loss: 1.3977437019348145\n",
            "Epoch 34/50 | Batch 41/198 | Loss: 1.499855637550354\n",
            "Epoch 34/50 | Batch 42/198 | Loss: 1.4905750751495361\n",
            "Epoch 34/50 | Batch 43/198 | Loss: 1.5122272968292236\n",
            "Epoch 34/50 | Batch 44/198 | Loss: 1.4177473783493042\n",
            "Epoch 34/50 | Batch 45/198 | Loss: 1.4823884963989258\n",
            "Epoch 34/50 | Batch 46/198 | Loss: 1.5204987525939941\n",
            "Epoch 34/50 | Batch 47/198 | Loss: 1.3523072004318237\n",
            "Epoch 34/50 | Batch 48/198 | Loss: 1.5170013904571533\n",
            "Epoch 34/50 | Batch 49/198 | Loss: 1.2962228059768677\n",
            "Epoch 34/50 | Batch 50/198 | Loss: 1.6891266107559204\n",
            "Epoch 34/50 | Batch 51/198 | Loss: 1.4529211521148682\n",
            "Epoch 34/50 | Batch 52/198 | Loss: 1.5435154438018799\n",
            "Epoch 34/50 | Batch 53/198 | Loss: 1.6487621068954468\n",
            "Epoch 34/50 | Batch 54/198 | Loss: 1.4376262426376343\n",
            "Epoch 34/50 | Batch 55/198 | Loss: 1.6180580854415894\n",
            "Epoch 34/50 | Batch 56/198 | Loss: 1.4974126815795898\n",
            "Epoch 34/50 | Batch 57/198 | Loss: 1.617503046989441\n",
            "Epoch 34/50 | Batch 58/198 | Loss: 1.4965839385986328\n",
            "Epoch 34/50 | Batch 59/198 | Loss: 1.5546879768371582\n",
            "Epoch 34/50 | Batch 60/198 | Loss: 1.7250438928604126\n",
            "Epoch 34/50 | Batch 61/198 | Loss: 1.5536229610443115\n",
            "Epoch 34/50 | Batch 62/198 | Loss: 1.501535177230835\n",
            "Epoch 34/50 | Batch 63/198 | Loss: 1.1667593717575073\n",
            "Epoch 34/50 | Batch 64/198 | Loss: 1.419259786605835\n",
            "Epoch 34/50 | Batch 65/198 | Loss: 1.408098816871643\n",
            "Epoch 34/50 | Batch 66/198 | Loss: 1.3023651838302612\n",
            "Epoch 34/50 | Batch 67/198 | Loss: 1.5459431409835815\n",
            "Epoch 34/50 | Batch 68/198 | Loss: 1.5783661603927612\n",
            "Epoch 34/50 | Batch 69/198 | Loss: 1.4430500268936157\n",
            "Epoch 34/50 | Batch 70/198 | Loss: 1.542946457862854\n",
            "Epoch 34/50 | Batch 71/198 | Loss: 1.4998301267623901\n",
            "Epoch 34/50 | Batch 72/198 | Loss: 1.6060261726379395\n",
            "Epoch 34/50 | Batch 73/198 | Loss: 1.6358438730239868\n",
            "Epoch 34/50 | Batch 74/198 | Loss: 1.4613943099975586\n",
            "Epoch 34/50 | Batch 75/198 | Loss: 1.490491271018982\n",
            "Epoch 34/50 | Batch 76/198 | Loss: 1.744533658027649\n",
            "Epoch 34/50 | Batch 77/198 | Loss: 1.5669101476669312\n",
            "Epoch 34/50 | Batch 78/198 | Loss: 1.3427627086639404\n",
            "Epoch 34/50 | Batch 79/198 | Loss: 1.5264537334442139\n",
            "Epoch 34/50 | Batch 80/198 | Loss: 1.1816684007644653\n",
            "Epoch 34/50 | Batch 81/198 | Loss: 1.55545175075531\n",
            "Epoch 34/50 | Batch 82/198 | Loss: 1.6948844194412231\n",
            "Epoch 34/50 | Batch 83/198 | Loss: 1.6011840105056763\n",
            "Epoch 34/50 | Batch 84/198 | Loss: 1.5476268529891968\n",
            "Epoch 34/50 | Batch 85/198 | Loss: 1.278403401374817\n",
            "Epoch 34/50 | Batch 86/198 | Loss: 1.4249098300933838\n",
            "Epoch 34/50 | Batch 87/198 | Loss: 1.5476291179656982\n",
            "Epoch 34/50 | Batch 88/198 | Loss: 1.5963435173034668\n",
            "Epoch 34/50 | Batch 89/198 | Loss: 1.438517689704895\n",
            "Epoch 34/50 | Batch 90/198 | Loss: 1.425200343132019\n",
            "Epoch 34/50 | Batch 91/198 | Loss: 1.515228271484375\n",
            "Epoch 34/50 | Batch 92/198 | Loss: 1.5665998458862305\n",
            "Epoch 34/50 | Batch 93/198 | Loss: 1.4939696788787842\n",
            "Epoch 34/50 | Batch 94/198 | Loss: 1.6744707822799683\n",
            "Epoch 34/50 | Batch 95/198 | Loss: 1.386829137802124\n",
            "Epoch 34/50 | Batch 96/198 | Loss: 1.4664514064788818\n",
            "Epoch 34/50 | Batch 97/198 | Loss: 1.6135743856430054\n",
            "Epoch 34/50 | Batch 98/198 | Loss: 1.5168875455856323\n",
            "Epoch 34/50 | Batch 99/198 | Loss: 1.5833849906921387\n",
            "Epoch 34/50 | Batch 100/198 | Loss: 1.7283223867416382\n",
            "Epoch 34/50 | Batch 101/198 | Loss: 1.6434826850891113\n",
            "Epoch 34/50 | Batch 102/198 | Loss: 1.4267216920852661\n",
            "Epoch 34/50 | Batch 103/198 | Loss: 1.4554258584976196\n",
            "Epoch 34/50 | Batch 104/198 | Loss: 1.6804109811782837\n",
            "Epoch 34/50 | Batch 105/198 | Loss: 1.3642656803131104\n",
            "Epoch 34/50 | Batch 106/198 | Loss: 1.4102702140808105\n",
            "Epoch 34/50 | Batch 107/198 | Loss: 1.6709128618240356\n",
            "Epoch 34/50 | Batch 108/198 | Loss: 1.617877721786499\n",
            "Epoch 34/50 | Batch 109/198 | Loss: 1.3939313888549805\n",
            "Epoch 34/50 | Batch 110/198 | Loss: 1.4685719013214111\n",
            "Epoch 34/50 | Batch 111/198 | Loss: 1.4227808713912964\n",
            "Epoch 34/50 | Batch 112/198 | Loss: 1.626902461051941\n",
            "Epoch 34/50 | Batch 113/198 | Loss: 1.5309172868728638\n",
            "Epoch 34/50 | Batch 114/198 | Loss: 1.6086071729660034\n",
            "Epoch 34/50 | Batch 115/198 | Loss: 1.7392878532409668\n",
            "Epoch 34/50 | Batch 116/198 | Loss: 1.582063913345337\n",
            "Epoch 34/50 | Batch 117/198 | Loss: 1.4560928344726562\n",
            "Epoch 34/50 | Batch 118/198 | Loss: 1.4201865196228027\n",
            "Epoch 34/50 | Batch 119/198 | Loss: 1.6103771924972534\n",
            "Epoch 34/50 | Batch 120/198 | Loss: 1.6770215034484863\n",
            "Epoch 34/50 | Batch 121/198 | Loss: 1.4491084814071655\n",
            "Epoch 34/50 | Batch 122/198 | Loss: 1.5014379024505615\n",
            "Epoch 34/50 | Batch 123/198 | Loss: 1.6819452047348022\n",
            "Epoch 34/50 | Batch 124/198 | Loss: 1.444879412651062\n",
            "Epoch 34/50 | Batch 125/198 | Loss: 1.6163043975830078\n",
            "Epoch 34/50 | Batch 126/198 | Loss: 1.3790982961654663\n",
            "Epoch 34/50 | Batch 127/198 | Loss: 1.4610707759857178\n",
            "Epoch 34/50 | Batch 128/198 | Loss: 1.7173480987548828\n",
            "Epoch 34/50 | Batch 129/198 | Loss: 1.3981950283050537\n",
            "Epoch 34/50 | Batch 130/198 | Loss: 1.5494848489761353\n",
            "Epoch 34/50 | Batch 131/198 | Loss: 1.3284404277801514\n",
            "Epoch 34/50 | Batch 132/198 | Loss: 1.4186841249465942\n",
            "Epoch 34/50 | Batch 133/198 | Loss: 1.677953839302063\n",
            "Epoch 34/50 | Batch 134/198 | Loss: 1.44282066822052\n",
            "Epoch 34/50 | Batch 135/198 | Loss: 1.730095386505127\n",
            "Epoch 34/50 | Batch 136/198 | Loss: 1.6481959819793701\n",
            "Epoch 34/50 | Batch 137/198 | Loss: 1.6034700870513916\n",
            "Epoch 34/50 | Batch 138/198 | Loss: 1.5736526250839233\n",
            "Epoch 34/50 | Batch 139/198 | Loss: 1.60458505153656\n",
            "Epoch 34/50 | Batch 140/198 | Loss: 1.7013417482376099\n",
            "Epoch 34/50 | Batch 141/198 | Loss: 1.61643385887146\n",
            "Epoch 34/50 | Batch 142/198 | Loss: 1.4872288703918457\n",
            "Epoch 34/50 | Batch 143/198 | Loss: 1.7018094062805176\n",
            "Epoch 34/50 | Batch 144/198 | Loss: 1.3896679878234863\n",
            "Epoch 34/50 | Batch 145/198 | Loss: 1.6421273946762085\n",
            "Epoch 34/50 | Batch 146/198 | Loss: 1.6943737268447876\n",
            "Epoch 34/50 | Batch 147/198 | Loss: 1.6342555284500122\n",
            "Epoch 34/50 | Batch 148/198 | Loss: 1.3933366537094116\n",
            "Epoch 34/50 | Batch 149/198 | Loss: 1.6415408849716187\n",
            "Epoch 34/50 | Batch 150/198 | Loss: 1.3756028413772583\n",
            "Epoch 34/50 | Batch 151/198 | Loss: 1.4222965240478516\n",
            "Epoch 34/50 | Batch 152/198 | Loss: 1.342902660369873\n",
            "Epoch 34/50 | Batch 153/198 | Loss: 1.4670944213867188\n",
            "Epoch 34/50 | Batch 154/198 | Loss: 1.374833583831787\n",
            "Epoch 34/50 | Batch 155/198 | Loss: 1.5846236944198608\n",
            "Epoch 34/50 | Batch 156/198 | Loss: 1.6044588088989258\n",
            "Epoch 34/50 | Batch 157/198 | Loss: 1.4768630266189575\n",
            "Epoch 34/50 | Batch 158/198 | Loss: 1.5860449075698853\n",
            "Epoch 34/50 | Batch 159/198 | Loss: 1.5081888437271118\n",
            "Epoch 34/50 | Batch 160/198 | Loss: 1.5587328672409058\n",
            "Epoch 34/50 | Batch 161/198 | Loss: 1.4422651529312134\n",
            "Epoch 34/50 | Batch 162/198 | Loss: 1.680846929550171\n",
            "Epoch 34/50 | Batch 163/198 | Loss: 1.5128322839736938\n",
            "Epoch 34/50 | Batch 164/198 | Loss: 1.3484352827072144\n",
            "Epoch 34/50 | Batch 165/198 | Loss: 1.487881064414978\n",
            "Epoch 34/50 | Batch 166/198 | Loss: 1.6160264015197754\n",
            "Epoch 34/50 | Batch 167/198 | Loss: 1.24253249168396\n",
            "Epoch 34/50 | Batch 168/198 | Loss: 1.5474251508712769\n",
            "Epoch 34/50 | Batch 169/198 | Loss: 1.5231841802597046\n",
            "Epoch 34/50 | Batch 170/198 | Loss: 1.4693570137023926\n",
            "Epoch 34/50 | Batch 171/198 | Loss: 1.8029661178588867\n",
            "Epoch 34/50 | Batch 172/198 | Loss: 1.56144380569458\n",
            "Epoch 34/50 | Batch 173/198 | Loss: 1.5004558563232422\n",
            "Epoch 34/50 | Batch 174/198 | Loss: 1.46541428565979\n",
            "Epoch 34/50 | Batch 175/198 | Loss: 1.7260645627975464\n",
            "Epoch 34/50 | Batch 176/198 | Loss: 1.3821254968643188\n",
            "Epoch 34/50 | Batch 177/198 | Loss: 1.2868707180023193\n",
            "Epoch 34/50 | Batch 178/198 | Loss: 1.5280659198760986\n",
            "Epoch 34/50 | Batch 179/198 | Loss: 1.599592924118042\n",
            "Epoch 34/50 | Batch 180/198 | Loss: 1.8233331441879272\n",
            "Epoch 34/50 | Batch 181/198 | Loss: 1.60299551486969\n",
            "Epoch 34/50 | Batch 182/198 | Loss: 1.5845776796340942\n",
            "Epoch 34/50 | Batch 183/198 | Loss: 1.6304000616073608\n",
            "Epoch 34/50 | Batch 184/198 | Loss: 1.7960964441299438\n",
            "Epoch 34/50 | Batch 185/198 | Loss: 1.4590764045715332\n",
            "Epoch 34/50 | Batch 186/198 | Loss: 1.526429295539856\n",
            "Epoch 34/50 | Batch 187/198 | Loss: 1.609291672706604\n",
            "Epoch 34/50 | Batch 188/198 | Loss: 1.4262746572494507\n",
            "Epoch 34/50 | Batch 189/198 | Loss: 1.3377001285552979\n",
            "Epoch 34/50 | Batch 190/198 | Loss: 1.4179795980453491\n",
            "Epoch 34/50 | Batch 191/198 | Loss: 1.6002670526504517\n",
            "Epoch 34/50 | Batch 192/198 | Loss: 1.7085803747177124\n",
            "Epoch 34/50 | Batch 193/198 | Loss: 1.6628268957138062\n",
            "Epoch 34/50 | Batch 194/198 | Loss: 1.6299980878829956\n",
            "Epoch 34/50 | Batch 195/198 | Loss: 1.3930774927139282\n",
            "Epoch 34/50 | Batch 196/198 | Loss: 1.6858768463134766\n",
            "Epoch 34/50 | Batch 197/198 | Loss: 1.6234322786331177\n",
            "Epoch 34/50 | Batch 198/198 | Loss: 1.2885427474975586\n",
            "Epoch 34/50 | Average Loss: 1.5226157301604146\n",
            "Epoch 35/50 | Batch 1/198 | Loss: 1.2926775217056274\n",
            "Epoch 35/50 | Batch 2/198 | Loss: 1.601724624633789\n",
            "Epoch 35/50 | Batch 3/198 | Loss: 1.6198350191116333\n",
            "Epoch 35/50 | Batch 4/198 | Loss: 1.608026146888733\n",
            "Epoch 35/50 | Batch 5/198 | Loss: 1.5909351110458374\n",
            "Epoch 35/50 | Batch 6/198 | Loss: 1.4103015661239624\n",
            "Epoch 35/50 | Batch 7/198 | Loss: 1.637465238571167\n",
            "Epoch 35/50 | Batch 8/198 | Loss: 1.4164379835128784\n",
            "Epoch 35/50 | Batch 9/198 | Loss: 1.391708254814148\n",
            "Epoch 35/50 | Batch 10/198 | Loss: 1.695510745048523\n",
            "Epoch 35/50 | Batch 11/198 | Loss: 1.5448503494262695\n",
            "Epoch 35/50 | Batch 12/198 | Loss: 1.587800145149231\n",
            "Epoch 35/50 | Batch 13/198 | Loss: 1.5435458421707153\n",
            "Epoch 35/50 | Batch 14/198 | Loss: 1.4599318504333496\n",
            "Epoch 35/50 | Batch 15/198 | Loss: 1.3451018333435059\n",
            "Epoch 35/50 | Batch 16/198 | Loss: 1.5882335901260376\n",
            "Epoch 35/50 | Batch 17/198 | Loss: 1.4678696393966675\n",
            "Epoch 35/50 | Batch 18/198 | Loss: 1.3400064706802368\n",
            "Epoch 35/50 | Batch 19/198 | Loss: 1.7056561708450317\n",
            "Epoch 35/50 | Batch 20/198 | Loss: 1.4989949464797974\n",
            "Epoch 35/50 | Batch 21/198 | Loss: 1.5515376329421997\n",
            "Epoch 35/50 | Batch 22/198 | Loss: 1.4518554210662842\n",
            "Epoch 35/50 | Batch 23/198 | Loss: 1.425229787826538\n",
            "Epoch 35/50 | Batch 24/198 | Loss: 1.4913936853408813\n",
            "Epoch 35/50 | Batch 25/198 | Loss: 1.4777647256851196\n",
            "Epoch 35/50 | Batch 26/198 | Loss: 1.3982831239700317\n",
            "Epoch 35/50 | Batch 27/198 | Loss: 1.5014495849609375\n",
            "Epoch 35/50 | Batch 28/198 | Loss: 1.5066598653793335\n",
            "Epoch 35/50 | Batch 29/198 | Loss: 1.4970053434371948\n",
            "Epoch 35/50 | Batch 30/198 | Loss: 1.5425149202346802\n",
            "Epoch 35/50 | Batch 31/198 | Loss: 1.6229872703552246\n",
            "Epoch 35/50 | Batch 32/198 | Loss: 1.4948861598968506\n",
            "Epoch 35/50 | Batch 33/198 | Loss: 1.381390929222107\n",
            "Epoch 35/50 | Batch 34/198 | Loss: 1.4962092638015747\n",
            "Epoch 35/50 | Batch 35/198 | Loss: 1.3720667362213135\n",
            "Epoch 35/50 | Batch 36/198 | Loss: 1.404344081878662\n",
            "Epoch 35/50 | Batch 37/198 | Loss: 1.439383625984192\n",
            "Epoch 35/50 | Batch 38/198 | Loss: 1.4411790370941162\n",
            "Epoch 35/50 | Batch 39/198 | Loss: 1.553957223892212\n",
            "Epoch 35/50 | Batch 40/198 | Loss: 1.3378560543060303\n",
            "Epoch 35/50 | Batch 41/198 | Loss: 1.3402340412139893\n",
            "Epoch 35/50 | Batch 42/198 | Loss: 1.5572842359542847\n",
            "Epoch 35/50 | Batch 43/198 | Loss: 1.378200650215149\n",
            "Epoch 35/50 | Batch 44/198 | Loss: 1.645018219947815\n",
            "Epoch 35/50 | Batch 45/198 | Loss: 1.5658460855484009\n",
            "Epoch 35/50 | Batch 46/198 | Loss: 1.5229132175445557\n",
            "Epoch 35/50 | Batch 47/198 | Loss: 1.4907257556915283\n",
            "Epoch 35/50 | Batch 48/198 | Loss: 1.4518483877182007\n",
            "Epoch 35/50 | Batch 49/198 | Loss: 1.4018725156784058\n",
            "Epoch 35/50 | Batch 50/198 | Loss: 1.5271562337875366\n",
            "Epoch 35/50 | Batch 51/198 | Loss: 1.6066842079162598\n",
            "Epoch 35/50 | Batch 52/198 | Loss: 1.664505124092102\n",
            "Epoch 35/50 | Batch 53/198 | Loss: 1.4972389936447144\n",
            "Epoch 35/50 | Batch 54/198 | Loss: 1.5828875303268433\n",
            "Epoch 35/50 | Batch 55/198 | Loss: 1.319296956062317\n",
            "Epoch 35/50 | Batch 56/198 | Loss: 1.541780948638916\n",
            "Epoch 35/50 | Batch 57/198 | Loss: 1.6419051885604858\n",
            "Epoch 35/50 | Batch 58/198 | Loss: 1.324733853340149\n",
            "Epoch 35/50 | Batch 59/198 | Loss: 1.5504980087280273\n",
            "Epoch 35/50 | Batch 60/198 | Loss: 1.4418308734893799\n",
            "Epoch 35/50 | Batch 61/198 | Loss: 1.3318103551864624\n",
            "Epoch 35/50 | Batch 62/198 | Loss: 1.5779907703399658\n",
            "Epoch 35/50 | Batch 63/198 | Loss: 1.5427556037902832\n",
            "Epoch 35/50 | Batch 64/198 | Loss: 1.7771183252334595\n",
            "Epoch 35/50 | Batch 65/198 | Loss: 1.5906232595443726\n",
            "Epoch 35/50 | Batch 66/198 | Loss: 1.708687663078308\n",
            "Epoch 35/50 | Batch 67/198 | Loss: 1.5195988416671753\n",
            "Epoch 35/50 | Batch 68/198 | Loss: 1.4697248935699463\n",
            "Epoch 35/50 | Batch 69/198 | Loss: 1.6750891208648682\n",
            "Epoch 35/50 | Batch 70/198 | Loss: 1.2839847803115845\n",
            "Epoch 35/50 | Batch 71/198 | Loss: 1.3380850553512573\n",
            "Epoch 35/50 | Batch 72/198 | Loss: 1.4954577684402466\n",
            "Epoch 35/50 | Batch 73/198 | Loss: 1.5751605033874512\n",
            "Epoch 35/50 | Batch 74/198 | Loss: 1.9044557809829712\n",
            "Epoch 35/50 | Batch 75/198 | Loss: 1.4941065311431885\n",
            "Epoch 35/50 | Batch 76/198 | Loss: 1.5206776857376099\n",
            "Epoch 35/50 | Batch 77/198 | Loss: 1.7145709991455078\n",
            "Epoch 35/50 | Batch 78/198 | Loss: 1.3016493320465088\n",
            "Epoch 35/50 | Batch 79/198 | Loss: 1.6385619640350342\n",
            "Epoch 35/50 | Batch 80/198 | Loss: 1.564241886138916\n",
            "Epoch 35/50 | Batch 81/198 | Loss: 1.4430592060089111\n",
            "Epoch 35/50 | Batch 82/198 | Loss: 1.369584083557129\n",
            "Epoch 35/50 | Batch 83/198 | Loss: 1.3442587852478027\n",
            "Epoch 35/50 | Batch 84/198 | Loss: 1.363563060760498\n",
            "Epoch 35/50 | Batch 85/198 | Loss: 1.4266483783721924\n",
            "Epoch 35/50 | Batch 86/198 | Loss: 1.4209330081939697\n",
            "Epoch 35/50 | Batch 87/198 | Loss: 1.508323073387146\n",
            "Epoch 35/50 | Batch 88/198 | Loss: 1.553329586982727\n",
            "Epoch 35/50 | Batch 89/198 | Loss: 1.6008514165878296\n",
            "Epoch 35/50 | Batch 90/198 | Loss: 1.4850374460220337\n",
            "Epoch 35/50 | Batch 91/198 | Loss: 1.5848727226257324\n",
            "Epoch 35/50 | Batch 92/198 | Loss: 1.4202649593353271\n",
            "Epoch 35/50 | Batch 93/198 | Loss: 1.6361497640609741\n",
            "Epoch 35/50 | Batch 94/198 | Loss: 1.6559828519821167\n",
            "Epoch 35/50 | Batch 95/198 | Loss: 1.5394930839538574\n",
            "Epoch 35/50 | Batch 96/198 | Loss: 1.3511297702789307\n",
            "Epoch 35/50 | Batch 97/198 | Loss: 1.5989350080490112\n",
            "Epoch 35/50 | Batch 98/198 | Loss: 1.3472808599472046\n",
            "Epoch 35/50 | Batch 99/198 | Loss: 1.4355823993682861\n",
            "Epoch 35/50 | Batch 100/198 | Loss: 1.3215137720108032\n",
            "Epoch 35/50 | Batch 101/198 | Loss: 1.390523076057434\n",
            "Epoch 35/50 | Batch 102/198 | Loss: 1.560814380645752\n",
            "Epoch 35/50 | Batch 103/198 | Loss: 1.252597451210022\n",
            "Epoch 35/50 | Batch 104/198 | Loss: 1.5828170776367188\n",
            "Epoch 35/50 | Batch 105/198 | Loss: 1.5564525127410889\n",
            "Epoch 35/50 | Batch 106/198 | Loss: 1.5960018634796143\n",
            "Epoch 35/50 | Batch 107/198 | Loss: 1.53638756275177\n",
            "Epoch 35/50 | Batch 108/198 | Loss: 1.4904983043670654\n",
            "Epoch 35/50 | Batch 109/198 | Loss: 1.509639859199524\n",
            "Epoch 35/50 | Batch 110/198 | Loss: 1.5374802350997925\n",
            "Epoch 35/50 | Batch 111/198 | Loss: 1.6705149412155151\n",
            "Epoch 35/50 | Batch 112/198 | Loss: 1.576562523841858\n",
            "Epoch 35/50 | Batch 113/198 | Loss: 1.4039655923843384\n",
            "Epoch 35/50 | Batch 114/198 | Loss: 1.5476408004760742\n",
            "Epoch 35/50 | Batch 115/198 | Loss: 1.3778458833694458\n",
            "Epoch 35/50 | Batch 116/198 | Loss: 1.6948723793029785\n",
            "Epoch 35/50 | Batch 117/198 | Loss: 1.336454153060913\n",
            "Epoch 35/50 | Batch 118/198 | Loss: 1.55765962600708\n",
            "Epoch 35/50 | Batch 119/198 | Loss: 1.5551016330718994\n",
            "Epoch 35/50 | Batch 120/198 | Loss: 1.4603605270385742\n",
            "Epoch 35/50 | Batch 121/198 | Loss: 1.4509390592575073\n",
            "Epoch 35/50 | Batch 122/198 | Loss: 1.4376204013824463\n",
            "Epoch 35/50 | Batch 123/198 | Loss: 1.528226375579834\n",
            "Epoch 35/50 | Batch 124/198 | Loss: 1.5364031791687012\n",
            "Epoch 35/50 | Batch 125/198 | Loss: 1.6562925577163696\n",
            "Epoch 35/50 | Batch 126/198 | Loss: 1.4880621433258057\n",
            "Epoch 35/50 | Batch 127/198 | Loss: 1.539766788482666\n",
            "Epoch 35/50 | Batch 128/198 | Loss: 1.3948718309402466\n",
            "Epoch 35/50 | Batch 129/198 | Loss: 1.3109813928604126\n",
            "Epoch 35/50 | Batch 130/198 | Loss: 1.525646448135376\n",
            "Epoch 35/50 | Batch 131/198 | Loss: 1.5084365606307983\n",
            "Epoch 35/50 | Batch 132/198 | Loss: 1.380597472190857\n",
            "Epoch 35/50 | Batch 133/198 | Loss: 1.6261941194534302\n",
            "Epoch 35/50 | Batch 134/198 | Loss: 1.450383186340332\n",
            "Epoch 35/50 | Batch 135/198 | Loss: 1.4965571165084839\n",
            "Epoch 35/50 | Batch 136/198 | Loss: 1.245413064956665\n",
            "Epoch 35/50 | Batch 137/198 | Loss: 1.416400671005249\n",
            "Epoch 35/50 | Batch 138/198 | Loss: 1.42893648147583\n",
            "Epoch 35/50 | Batch 139/198 | Loss: 1.724192500114441\n",
            "Epoch 35/50 | Batch 140/198 | Loss: 1.4123291969299316\n",
            "Epoch 35/50 | Batch 141/198 | Loss: 1.6739137172698975\n",
            "Epoch 35/50 | Batch 142/198 | Loss: 1.5248384475708008\n",
            "Epoch 35/50 | Batch 143/198 | Loss: 1.8000754117965698\n",
            "Epoch 35/50 | Batch 144/198 | Loss: 1.3013032674789429\n",
            "Epoch 35/50 | Batch 145/198 | Loss: 1.4483487606048584\n",
            "Epoch 35/50 | Batch 146/198 | Loss: 1.434667706489563\n",
            "Epoch 35/50 | Batch 147/198 | Loss: 1.4623180627822876\n",
            "Epoch 35/50 | Batch 148/198 | Loss: 1.568345546722412\n",
            "Epoch 35/50 | Batch 149/198 | Loss: 1.4494304656982422\n",
            "Epoch 35/50 | Batch 150/198 | Loss: 1.5574144124984741\n",
            "Epoch 35/50 | Batch 151/198 | Loss: 1.5043762922286987\n",
            "Epoch 35/50 | Batch 152/198 | Loss: 1.4952987432479858\n",
            "Epoch 35/50 | Batch 153/198 | Loss: 1.7966808080673218\n",
            "Epoch 35/50 | Batch 154/198 | Loss: 1.4368178844451904\n",
            "Epoch 35/50 | Batch 155/198 | Loss: 1.3840413093566895\n",
            "Epoch 35/50 | Batch 156/198 | Loss: 1.6141698360443115\n",
            "Epoch 35/50 | Batch 157/198 | Loss: 1.387499451637268\n",
            "Epoch 35/50 | Batch 158/198 | Loss: 1.610046625137329\n",
            "Epoch 35/50 | Batch 159/198 | Loss: 1.5960549116134644\n",
            "Epoch 35/50 | Batch 160/198 | Loss: 1.5326263904571533\n",
            "Epoch 35/50 | Batch 161/198 | Loss: 1.5717054605484009\n",
            "Epoch 35/50 | Batch 162/198 | Loss: 1.4339861869812012\n",
            "Epoch 35/50 | Batch 163/198 | Loss: 1.5432156324386597\n",
            "Epoch 35/50 | Batch 164/198 | Loss: 1.2982257604599\n",
            "Epoch 35/50 | Batch 165/198 | Loss: 1.6107280254364014\n",
            "Epoch 35/50 | Batch 166/198 | Loss: 1.4280905723571777\n",
            "Epoch 35/50 | Batch 167/198 | Loss: 1.5990384817123413\n",
            "Epoch 35/50 | Batch 168/198 | Loss: 1.7320281267166138\n",
            "Epoch 35/50 | Batch 169/198 | Loss: 1.5475008487701416\n",
            "Epoch 35/50 | Batch 170/198 | Loss: 1.5515722036361694\n",
            "Epoch 35/50 | Batch 171/198 | Loss: 1.2200219631195068\n",
            "Epoch 35/50 | Batch 172/198 | Loss: 1.5515658855438232\n",
            "Epoch 35/50 | Batch 173/198 | Loss: 1.3552789688110352\n",
            "Epoch 35/50 | Batch 174/198 | Loss: 1.5395814180374146\n",
            "Epoch 35/50 | Batch 175/198 | Loss: 1.6258082389831543\n",
            "Epoch 35/50 | Batch 176/198 | Loss: 1.4315998554229736\n",
            "Epoch 35/50 | Batch 177/198 | Loss: 1.7126572132110596\n",
            "Epoch 35/50 | Batch 178/198 | Loss: 1.4355320930480957\n",
            "Epoch 35/50 | Batch 179/198 | Loss: 1.5124176740646362\n",
            "Epoch 35/50 | Batch 180/198 | Loss: 1.4301238059997559\n",
            "Epoch 35/50 | Batch 181/198 | Loss: 1.6886457204818726\n",
            "Epoch 35/50 | Batch 182/198 | Loss: 1.4658801555633545\n",
            "Epoch 35/50 | Batch 183/198 | Loss: 1.4170957803726196\n",
            "Epoch 35/50 | Batch 184/198 | Loss: 1.4866472482681274\n",
            "Epoch 35/50 | Batch 185/198 | Loss: 1.5504231452941895\n",
            "Epoch 35/50 | Batch 186/198 | Loss: 1.5843530893325806\n",
            "Epoch 35/50 | Batch 187/198 | Loss: 1.4408904314041138\n",
            "Epoch 35/50 | Batch 188/198 | Loss: 1.305113434791565\n",
            "Epoch 35/50 | Batch 189/198 | Loss: 1.6669191122055054\n",
            "Epoch 35/50 | Batch 190/198 | Loss: 1.3443100452423096\n",
            "Epoch 35/50 | Batch 191/198 | Loss: 1.7501027584075928\n",
            "Epoch 35/50 | Batch 192/198 | Loss: 1.813664436340332\n",
            "Epoch 35/50 | Batch 193/198 | Loss: 1.3025164604187012\n",
            "Epoch 35/50 | Batch 194/198 | Loss: 1.4058730602264404\n",
            "Epoch 35/50 | Batch 195/198 | Loss: 1.4862838983535767\n",
            "Epoch 35/50 | Batch 196/198 | Loss: 1.4427253007888794\n",
            "Epoch 35/50 | Batch 197/198 | Loss: 1.6603518724441528\n",
            "Epoch 35/50 | Batch 198/198 | Loss: 1.289896011352539\n",
            "Epoch 35/50 | Average Loss: 1.5039457185099823\n",
            "Epoch 36/50 | Batch 1/198 | Loss: 1.449430227279663\n",
            "Epoch 36/50 | Batch 2/198 | Loss: 1.3707276582717896\n",
            "Epoch 36/50 | Batch 3/198 | Loss: 1.806138515472412\n",
            "Epoch 36/50 | Batch 4/198 | Loss: 1.6112550497055054\n",
            "Epoch 36/50 | Batch 5/198 | Loss: 1.5179458856582642\n",
            "Epoch 36/50 | Batch 6/198 | Loss: 1.6413159370422363\n",
            "Epoch 36/50 | Batch 7/198 | Loss: 1.6726733446121216\n",
            "Epoch 36/50 | Batch 8/198 | Loss: 1.4535658359527588\n",
            "Epoch 36/50 | Batch 9/198 | Loss: 1.1830750703811646\n",
            "Epoch 36/50 | Batch 10/198 | Loss: 1.3447542190551758\n",
            "Epoch 36/50 | Batch 11/198 | Loss: 1.509300708770752\n",
            "Epoch 36/50 | Batch 12/198 | Loss: 1.720860242843628\n",
            "Epoch 36/50 | Batch 13/198 | Loss: 1.5113890171051025\n",
            "Epoch 36/50 | Batch 14/198 | Loss: 1.415645718574524\n",
            "Epoch 36/50 | Batch 15/198 | Loss: 1.3492052555084229\n",
            "Epoch 36/50 | Batch 16/198 | Loss: 1.3521798849105835\n",
            "Epoch 36/50 | Batch 17/198 | Loss: 1.375685453414917\n",
            "Epoch 36/50 | Batch 18/198 | Loss: 1.3290767669677734\n",
            "Epoch 36/50 | Batch 19/198 | Loss: 1.5980433225631714\n",
            "Epoch 36/50 | Batch 20/198 | Loss: 1.5926278829574585\n",
            "Epoch 36/50 | Batch 21/198 | Loss: 1.4888172149658203\n",
            "Epoch 36/50 | Batch 22/198 | Loss: 1.4143136739730835\n",
            "Epoch 36/50 | Batch 23/198 | Loss: 1.282429814338684\n",
            "Epoch 36/50 | Batch 24/198 | Loss: 1.5398977994918823\n",
            "Epoch 36/50 | Batch 25/198 | Loss: 1.34119713306427\n",
            "Epoch 36/50 | Batch 26/198 | Loss: 1.455281138420105\n",
            "Epoch 36/50 | Batch 27/198 | Loss: 1.5018846988677979\n",
            "Epoch 36/50 | Batch 28/198 | Loss: 1.4596319198608398\n",
            "Epoch 36/50 | Batch 29/198 | Loss: 1.2799601554870605\n",
            "Epoch 36/50 | Batch 30/198 | Loss: 1.51616370677948\n",
            "Epoch 36/50 | Batch 31/198 | Loss: 1.3882893323898315\n",
            "Epoch 36/50 | Batch 32/198 | Loss: 1.370036244392395\n",
            "Epoch 36/50 | Batch 33/198 | Loss: 1.5781924724578857\n",
            "Epoch 36/50 | Batch 34/198 | Loss: 1.4514580965042114\n",
            "Epoch 36/50 | Batch 35/198 | Loss: 1.4567148685455322\n",
            "Epoch 36/50 | Batch 36/198 | Loss: 1.4275569915771484\n",
            "Epoch 36/50 | Batch 37/198 | Loss: 1.4929195642471313\n",
            "Epoch 36/50 | Batch 38/198 | Loss: 1.5225435495376587\n",
            "Epoch 36/50 | Batch 39/198 | Loss: 1.6975820064544678\n",
            "Epoch 36/50 | Batch 40/198 | Loss: 1.5664045810699463\n",
            "Epoch 36/50 | Batch 41/198 | Loss: 1.4394193887710571\n",
            "Epoch 36/50 | Batch 42/198 | Loss: 1.5473551750183105\n",
            "Epoch 36/50 | Batch 43/198 | Loss: 1.1939009428024292\n",
            "Epoch 36/50 | Batch 44/198 | Loss: 1.3915777206420898\n",
            "Epoch 36/50 | Batch 45/198 | Loss: 1.6133757829666138\n",
            "Epoch 36/50 | Batch 46/198 | Loss: 1.439449429512024\n",
            "Epoch 36/50 | Batch 47/198 | Loss: 1.458665132522583\n",
            "Epoch 36/50 | Batch 48/198 | Loss: 1.4163486957550049\n",
            "Epoch 36/50 | Batch 49/198 | Loss: 1.5713945627212524\n",
            "Epoch 36/50 | Batch 50/198 | Loss: 1.3702188730239868\n",
            "Epoch 36/50 | Batch 51/198 | Loss: 1.5739184617996216\n",
            "Epoch 36/50 | Batch 52/198 | Loss: 1.4827200174331665\n",
            "Epoch 36/50 | Batch 53/198 | Loss: 1.4655823707580566\n",
            "Epoch 36/50 | Batch 54/198 | Loss: 1.8881901502609253\n",
            "Epoch 36/50 | Batch 55/198 | Loss: 1.385628342628479\n",
            "Epoch 36/50 | Batch 56/198 | Loss: 1.4170470237731934\n",
            "Epoch 36/50 | Batch 57/198 | Loss: 1.5677540302276611\n",
            "Epoch 36/50 | Batch 58/198 | Loss: 1.623479962348938\n",
            "Epoch 36/50 | Batch 59/198 | Loss: 1.414372205734253\n",
            "Epoch 36/50 | Batch 60/198 | Loss: 1.5964511632919312\n",
            "Epoch 36/50 | Batch 61/198 | Loss: 1.4421597719192505\n",
            "Epoch 36/50 | Batch 62/198 | Loss: 1.7104465961456299\n",
            "Epoch 36/50 | Batch 63/198 | Loss: 1.6388931274414062\n",
            "Epoch 36/50 | Batch 64/198 | Loss: 1.3767098188400269\n",
            "Epoch 36/50 | Batch 65/198 | Loss: 1.4838308095932007\n",
            "Epoch 36/50 | Batch 66/198 | Loss: 1.5066057443618774\n",
            "Epoch 36/50 | Batch 67/198 | Loss: 1.5945137739181519\n",
            "Epoch 36/50 | Batch 68/198 | Loss: 1.3700987100601196\n",
            "Epoch 36/50 | Batch 69/198 | Loss: 1.4619680643081665\n",
            "Epoch 36/50 | Batch 70/198 | Loss: 1.3522785902023315\n",
            "Epoch 36/50 | Batch 71/198 | Loss: 1.5005619525909424\n",
            "Epoch 36/50 | Batch 72/198 | Loss: 1.5763942003250122\n",
            "Epoch 36/50 | Batch 73/198 | Loss: 1.566715955734253\n",
            "Epoch 36/50 | Batch 74/198 | Loss: 1.5398273468017578\n",
            "Epoch 36/50 | Batch 75/198 | Loss: 1.5563852787017822\n",
            "Epoch 36/50 | Batch 76/198 | Loss: 1.5811125040054321\n",
            "Epoch 36/50 | Batch 77/198 | Loss: 1.4017024040222168\n",
            "Epoch 36/50 | Batch 78/198 | Loss: 1.5599110126495361\n",
            "Epoch 36/50 | Batch 79/198 | Loss: 1.6610509157180786\n",
            "Epoch 36/50 | Batch 80/198 | Loss: 1.4228979349136353\n",
            "Epoch 36/50 | Batch 81/198 | Loss: 1.4915186166763306\n",
            "Epoch 36/50 | Batch 82/198 | Loss: 1.4659435749053955\n",
            "Epoch 36/50 | Batch 83/198 | Loss: 1.5063890218734741\n",
            "Epoch 36/50 | Batch 84/198 | Loss: 1.5942198038101196\n",
            "Epoch 36/50 | Batch 85/198 | Loss: 1.5852713584899902\n",
            "Epoch 36/50 | Batch 86/198 | Loss: 1.4133272171020508\n",
            "Epoch 36/50 | Batch 87/198 | Loss: 1.3956522941589355\n",
            "Epoch 36/50 | Batch 88/198 | Loss: 1.3137766122817993\n",
            "Epoch 36/50 | Batch 89/198 | Loss: 1.5161864757537842\n",
            "Epoch 36/50 | Batch 90/198 | Loss: 1.418784499168396\n",
            "Epoch 36/50 | Batch 91/198 | Loss: 1.7759723663330078\n",
            "Epoch 36/50 | Batch 92/198 | Loss: 1.2324433326721191\n",
            "Epoch 36/50 | Batch 93/198 | Loss: 1.6871393918991089\n",
            "Epoch 36/50 | Batch 94/198 | Loss: 1.3996161222457886\n",
            "Epoch 36/50 | Batch 95/198 | Loss: 1.5295648574829102\n",
            "Epoch 36/50 | Batch 96/198 | Loss: 1.601283073425293\n",
            "Epoch 36/50 | Batch 97/198 | Loss: 1.1338558197021484\n",
            "Epoch 36/50 | Batch 98/198 | Loss: 1.6358976364135742\n",
            "Epoch 36/50 | Batch 99/198 | Loss: 1.5132588148117065\n",
            "Epoch 36/50 | Batch 100/198 | Loss: 1.2126356363296509\n",
            "Epoch 36/50 | Batch 101/198 | Loss: 1.6144880056381226\n",
            "Epoch 36/50 | Batch 102/198 | Loss: 1.2078595161437988\n",
            "Epoch 36/50 | Batch 103/198 | Loss: 1.6168965101242065\n",
            "Epoch 36/50 | Batch 104/198 | Loss: 1.3548390865325928\n",
            "Epoch 36/50 | Batch 105/198 | Loss: 1.3145992755889893\n",
            "Epoch 36/50 | Batch 106/198 | Loss: 1.2874430418014526\n",
            "Epoch 36/50 | Batch 107/198 | Loss: 1.5117475986480713\n",
            "Epoch 36/50 | Batch 108/198 | Loss: 1.3538897037506104\n",
            "Epoch 36/50 | Batch 109/198 | Loss: 1.7390393018722534\n",
            "Epoch 36/50 | Batch 110/198 | Loss: 1.5135293006896973\n",
            "Epoch 36/50 | Batch 111/198 | Loss: 1.5086933374404907\n",
            "Epoch 36/50 | Batch 112/198 | Loss: 1.5682272911071777\n",
            "Epoch 36/50 | Batch 113/198 | Loss: 1.423832654953003\n",
            "Epoch 36/50 | Batch 114/198 | Loss: 1.4371157884597778\n",
            "Epoch 36/50 | Batch 115/198 | Loss: 1.4187694787979126\n",
            "Epoch 36/50 | Batch 116/198 | Loss: 1.4532268047332764\n",
            "Epoch 36/50 | Batch 117/198 | Loss: 1.4259477853775024\n",
            "Epoch 36/50 | Batch 118/198 | Loss: 1.343807578086853\n",
            "Epoch 36/50 | Batch 119/198 | Loss: 1.49995756149292\n",
            "Epoch 36/50 | Batch 120/198 | Loss: 1.1769706010818481\n",
            "Epoch 36/50 | Batch 121/198 | Loss: 1.5683741569519043\n",
            "Epoch 36/50 | Batch 122/198 | Loss: 1.4727193117141724\n",
            "Epoch 36/50 | Batch 123/198 | Loss: 1.4490604400634766\n",
            "Epoch 36/50 | Batch 124/198 | Loss: 1.3947007656097412\n",
            "Epoch 36/50 | Batch 125/198 | Loss: 1.3739253282546997\n",
            "Epoch 36/50 | Batch 126/198 | Loss: 1.5141953229904175\n",
            "Epoch 36/50 | Batch 127/198 | Loss: 1.448630452156067\n",
            "Epoch 36/50 | Batch 128/198 | Loss: 1.33928382396698\n",
            "Epoch 36/50 | Batch 129/198 | Loss: 1.4150110483169556\n",
            "Epoch 36/50 | Batch 130/198 | Loss: 1.338474154472351\n",
            "Epoch 36/50 | Batch 131/198 | Loss: 1.5623687505722046\n",
            "Epoch 36/50 | Batch 132/198 | Loss: 1.438636302947998\n",
            "Epoch 36/50 | Batch 133/198 | Loss: 1.5936359167099\n",
            "Epoch 36/50 | Batch 134/198 | Loss: 1.531729817390442\n",
            "Epoch 36/50 | Batch 135/198 | Loss: 1.5515623092651367\n",
            "Epoch 36/50 | Batch 136/198 | Loss: 1.5308228731155396\n",
            "Epoch 36/50 | Batch 137/198 | Loss: 1.3808163404464722\n",
            "Epoch 36/50 | Batch 138/198 | Loss: 1.4146744012832642\n",
            "Epoch 36/50 | Batch 139/198 | Loss: 1.5018930435180664\n",
            "Epoch 36/50 | Batch 140/198 | Loss: 1.4663046598434448\n",
            "Epoch 36/50 | Batch 141/198 | Loss: 1.6120023727416992\n",
            "Epoch 36/50 | Batch 142/198 | Loss: 1.2424226999282837\n",
            "Epoch 36/50 | Batch 143/198 | Loss: 1.548545241355896\n",
            "Epoch 36/50 | Batch 144/198 | Loss: 1.5499299764633179\n",
            "Epoch 36/50 | Batch 145/198 | Loss: 1.5205050706863403\n",
            "Epoch 36/50 | Batch 146/198 | Loss: 1.3679496049880981\n",
            "Epoch 36/50 | Batch 147/198 | Loss: 1.6249839067459106\n",
            "Epoch 36/50 | Batch 148/198 | Loss: 1.5075939893722534\n",
            "Epoch 36/50 | Batch 149/198 | Loss: 1.5832523107528687\n",
            "Epoch 36/50 | Batch 150/198 | Loss: 1.4791253805160522\n",
            "Epoch 36/50 | Batch 151/198 | Loss: 1.489845871925354\n",
            "Epoch 36/50 | Batch 152/198 | Loss: 1.4115729331970215\n",
            "Epoch 36/50 | Batch 153/198 | Loss: 1.3914045095443726\n",
            "Epoch 36/50 | Batch 154/198 | Loss: 1.6376646757125854\n",
            "Epoch 36/50 | Batch 155/198 | Loss: 1.5468305349349976\n",
            "Epoch 36/50 | Batch 156/198 | Loss: 1.381480097770691\n",
            "Epoch 36/50 | Batch 157/198 | Loss: 1.507232666015625\n",
            "Epoch 36/50 | Batch 158/198 | Loss: 1.3684170246124268\n",
            "Epoch 36/50 | Batch 159/198 | Loss: 1.710126280784607\n",
            "Epoch 36/50 | Batch 160/198 | Loss: 1.4063342809677124\n",
            "Epoch 36/50 | Batch 161/198 | Loss: 1.4994593858718872\n",
            "Epoch 36/50 | Batch 162/198 | Loss: 1.6024826765060425\n",
            "Epoch 36/50 | Batch 163/198 | Loss: 1.6323484182357788\n",
            "Epoch 36/50 | Batch 164/198 | Loss: 1.4529109001159668\n",
            "Epoch 36/50 | Batch 165/198 | Loss: 1.3907626867294312\n",
            "Epoch 36/50 | Batch 166/198 | Loss: 1.5687886476516724\n",
            "Epoch 36/50 | Batch 167/198 | Loss: 1.558045744895935\n",
            "Epoch 36/50 | Batch 168/198 | Loss: 1.604598045349121\n",
            "Epoch 36/50 | Batch 169/198 | Loss: 1.428292989730835\n",
            "Epoch 36/50 | Batch 170/198 | Loss: 1.5416533946990967\n",
            "Epoch 36/50 | Batch 171/198 | Loss: 1.4911047220230103\n",
            "Epoch 36/50 | Batch 172/198 | Loss: 1.4349822998046875\n",
            "Epoch 36/50 | Batch 173/198 | Loss: 1.4830958843231201\n",
            "Epoch 36/50 | Batch 174/198 | Loss: 1.5444507598876953\n",
            "Epoch 36/50 | Batch 175/198 | Loss: 1.44727623462677\n",
            "Epoch 36/50 | Batch 176/198 | Loss: 1.4537017345428467\n",
            "Epoch 36/50 | Batch 177/198 | Loss: 1.4694918394088745\n",
            "Epoch 36/50 | Batch 178/198 | Loss: 1.5733506679534912\n",
            "Epoch 36/50 | Batch 179/198 | Loss: 1.4543671607971191\n",
            "Epoch 36/50 | Batch 180/198 | Loss: 1.581707239151001\n",
            "Epoch 36/50 | Batch 181/198 | Loss: 1.6618552207946777\n",
            "Epoch 36/50 | Batch 182/198 | Loss: 1.3924494981765747\n",
            "Epoch 36/50 | Batch 183/198 | Loss: 1.522195816040039\n",
            "Epoch 36/50 | Batch 184/198 | Loss: 1.6436899900436401\n",
            "Epoch 36/50 | Batch 185/198 | Loss: 1.6662997007369995\n",
            "Epoch 36/50 | Batch 186/198 | Loss: 1.3938679695129395\n",
            "Epoch 36/50 | Batch 187/198 | Loss: 1.4741674661636353\n",
            "Epoch 36/50 | Batch 188/198 | Loss: 1.4285744428634644\n",
            "Epoch 36/50 | Batch 189/198 | Loss: 1.5052443742752075\n",
            "Epoch 36/50 | Batch 190/198 | Loss: 1.4172821044921875\n",
            "Epoch 36/50 | Batch 191/198 | Loss: 1.2803183794021606\n",
            "Epoch 36/50 | Batch 192/198 | Loss: 1.5882811546325684\n",
            "Epoch 36/50 | Batch 193/198 | Loss: 1.5298653841018677\n",
            "Epoch 36/50 | Batch 194/198 | Loss: 1.7348271608352661\n",
            "Epoch 36/50 | Batch 195/198 | Loss: 1.8061983585357666\n",
            "Epoch 36/50 | Batch 196/198 | Loss: 1.3659731149673462\n",
            "Epoch 36/50 | Batch 197/198 | Loss: 1.7327402830123901\n",
            "Epoch 36/50 | Batch 198/198 | Loss: 1.556030511856079\n",
            "Epoch 36/50 | Average Loss: 1.4872599629440693\n",
            "Epoch 37/50 | Batch 1/198 | Loss: 1.6380659341812134\n",
            "Epoch 37/50 | Batch 2/198 | Loss: 1.550533652305603\n",
            "Epoch 37/50 | Batch 3/198 | Loss: 1.4378113746643066\n",
            "Epoch 37/50 | Batch 4/198 | Loss: 1.8005127906799316\n",
            "Epoch 37/50 | Batch 5/198 | Loss: 1.5004099607467651\n",
            "Epoch 37/50 | Batch 6/198 | Loss: 1.5217887163162231\n",
            "Epoch 37/50 | Batch 7/198 | Loss: 1.5590440034866333\n",
            "Epoch 37/50 | Batch 8/198 | Loss: 1.390232801437378\n",
            "Epoch 37/50 | Batch 9/198 | Loss: 1.5099259614944458\n",
            "Epoch 37/50 | Batch 10/198 | Loss: 1.4132856130599976\n",
            "Epoch 37/50 | Batch 11/198 | Loss: 1.6534587144851685\n",
            "Epoch 37/50 | Batch 12/198 | Loss: 1.2436507940292358\n",
            "Epoch 37/50 | Batch 13/198 | Loss: 1.5279663801193237\n",
            "Epoch 37/50 | Batch 14/198 | Loss: 1.3632248640060425\n",
            "Epoch 37/50 | Batch 15/198 | Loss: 1.4011229276657104\n",
            "Epoch 37/50 | Batch 16/198 | Loss: 1.3654688596725464\n",
            "Epoch 37/50 | Batch 17/198 | Loss: 1.3582619428634644\n",
            "Epoch 37/50 | Batch 18/198 | Loss: 1.213554859161377\n",
            "Epoch 37/50 | Batch 19/198 | Loss: 1.3378998041152954\n",
            "Epoch 37/50 | Batch 20/198 | Loss: 1.4774442911148071\n",
            "Epoch 37/50 | Batch 21/198 | Loss: 1.4572043418884277\n",
            "Epoch 37/50 | Batch 22/198 | Loss: 1.3931714296340942\n",
            "Epoch 37/50 | Batch 23/198 | Loss: 1.4440884590148926\n",
            "Epoch 37/50 | Batch 24/198 | Loss: 1.6034748554229736\n",
            "Epoch 37/50 | Batch 25/198 | Loss: 1.5745266675949097\n",
            "Epoch 37/50 | Batch 26/198 | Loss: 1.7776356935501099\n",
            "Epoch 37/50 | Batch 27/198 | Loss: 1.5205507278442383\n",
            "Epoch 37/50 | Batch 28/198 | Loss: 1.353843092918396\n",
            "Epoch 37/50 | Batch 29/198 | Loss: 1.297781229019165\n",
            "Epoch 37/50 | Batch 30/198 | Loss: 1.4744595289230347\n",
            "Epoch 37/50 | Batch 31/198 | Loss: 1.4263441562652588\n",
            "Epoch 37/50 | Batch 32/198 | Loss: 1.4775012731552124\n",
            "Epoch 37/50 | Batch 33/198 | Loss: 1.3754394054412842\n",
            "Epoch 37/50 | Batch 34/198 | Loss: 1.355799674987793\n",
            "Epoch 37/50 | Batch 35/198 | Loss: 1.622274398803711\n",
            "Epoch 37/50 | Batch 36/198 | Loss: 1.2263277769088745\n",
            "Epoch 37/50 | Batch 37/198 | Loss: 1.5892020463943481\n",
            "Epoch 37/50 | Batch 38/198 | Loss: 1.5022838115692139\n",
            "Epoch 37/50 | Batch 39/198 | Loss: 1.4030184745788574\n",
            "Epoch 37/50 | Batch 40/198 | Loss: 1.5648767948150635\n",
            "Epoch 37/50 | Batch 41/198 | Loss: 1.5361326932907104\n",
            "Epoch 37/50 | Batch 42/198 | Loss: 1.4450287818908691\n",
            "Epoch 37/50 | Batch 43/198 | Loss: 1.6191096305847168\n",
            "Epoch 37/50 | Batch 44/198 | Loss: 1.3197282552719116\n",
            "Epoch 37/50 | Batch 45/198 | Loss: 1.3613754510879517\n",
            "Epoch 37/50 | Batch 46/198 | Loss: 1.61935555934906\n",
            "Epoch 37/50 | Batch 47/198 | Loss: 1.5098155736923218\n",
            "Epoch 37/50 | Batch 48/198 | Loss: 1.3420536518096924\n",
            "Epoch 37/50 | Batch 49/198 | Loss: 1.4306737184524536\n",
            "Epoch 37/50 | Batch 50/198 | Loss: 1.4541171789169312\n",
            "Epoch 37/50 | Batch 51/198 | Loss: 1.3756158351898193\n",
            "Epoch 37/50 | Batch 52/198 | Loss: 1.6153322458267212\n",
            "Epoch 37/50 | Batch 53/198 | Loss: 1.1947537660598755\n",
            "Epoch 37/50 | Batch 54/198 | Loss: 1.5618349313735962\n",
            "Epoch 37/50 | Batch 55/198 | Loss: 1.3278001546859741\n",
            "Epoch 37/50 | Batch 56/198 | Loss: 1.44663667678833\n",
            "Epoch 37/50 | Batch 57/198 | Loss: 1.5558245182037354\n",
            "Epoch 37/50 | Batch 58/198 | Loss: 1.588932752609253\n",
            "Epoch 37/50 | Batch 59/198 | Loss: 1.447487711906433\n",
            "Epoch 37/50 | Batch 60/198 | Loss: 1.5495563745498657\n",
            "Epoch 37/50 | Batch 61/198 | Loss: 1.4010900259017944\n",
            "Epoch 37/50 | Batch 62/198 | Loss: 1.384485125541687\n",
            "Epoch 37/50 | Batch 63/198 | Loss: 1.4674487113952637\n",
            "Epoch 37/50 | Batch 64/198 | Loss: 1.201944351196289\n",
            "Epoch 37/50 | Batch 65/198 | Loss: 1.7025772333145142\n",
            "Epoch 37/50 | Batch 66/198 | Loss: 1.3129018545150757\n",
            "Epoch 37/50 | Batch 67/198 | Loss: 1.2258447408676147\n",
            "Epoch 37/50 | Batch 68/198 | Loss: 1.3564777374267578\n",
            "Epoch 37/50 | Batch 69/198 | Loss: 1.4106632471084595\n",
            "Epoch 37/50 | Batch 70/198 | Loss: 1.2708358764648438\n",
            "Epoch 37/50 | Batch 71/198 | Loss: 1.4476696252822876\n",
            "Epoch 37/50 | Batch 72/198 | Loss: 1.3428606986999512\n",
            "Epoch 37/50 | Batch 73/198 | Loss: 1.5218931436538696\n",
            "Epoch 37/50 | Batch 74/198 | Loss: 1.5621495246887207\n",
            "Epoch 37/50 | Batch 75/198 | Loss: 1.7152425050735474\n",
            "Epoch 37/50 | Batch 76/198 | Loss: 1.4337178468704224\n",
            "Epoch 37/50 | Batch 77/198 | Loss: 1.6065045595169067\n",
            "Epoch 37/50 | Batch 78/198 | Loss: 1.457061767578125\n",
            "Epoch 37/50 | Batch 79/198 | Loss: 1.4972790479660034\n",
            "Epoch 37/50 | Batch 80/198 | Loss: 1.5343538522720337\n",
            "Epoch 37/50 | Batch 81/198 | Loss: 1.391345739364624\n",
            "Epoch 37/50 | Batch 82/198 | Loss: 1.2886548042297363\n",
            "Epoch 37/50 | Batch 83/198 | Loss: 1.6362625360488892\n",
            "Epoch 37/50 | Batch 84/198 | Loss: 1.4609179496765137\n",
            "Epoch 37/50 | Batch 85/198 | Loss: 1.489551067352295\n",
            "Epoch 37/50 | Batch 86/198 | Loss: 1.6980379819869995\n",
            "Epoch 37/50 | Batch 87/198 | Loss: 1.4043278694152832\n",
            "Epoch 37/50 | Batch 88/198 | Loss: 1.3288252353668213\n",
            "Epoch 37/50 | Batch 89/198 | Loss: 1.5449575185775757\n",
            "Epoch 37/50 | Batch 90/198 | Loss: 1.594742774963379\n",
            "Epoch 37/50 | Batch 91/198 | Loss: 1.5333572626113892\n",
            "Epoch 37/50 | Batch 92/198 | Loss: 1.268675684928894\n",
            "Epoch 37/50 | Batch 93/198 | Loss: 1.586116075515747\n",
            "Epoch 37/50 | Batch 94/198 | Loss: 1.609039068222046\n",
            "Epoch 37/50 | Batch 95/198 | Loss: 1.2852706909179688\n",
            "Epoch 37/50 | Batch 96/198 | Loss: 1.301740050315857\n",
            "Epoch 37/50 | Batch 97/198 | Loss: 1.5716415643692017\n",
            "Epoch 37/50 | Batch 98/198 | Loss: 1.896099328994751\n",
            "Epoch 37/50 | Batch 99/198 | Loss: 1.4930860996246338\n",
            "Epoch 37/50 | Batch 100/198 | Loss: 1.3513017892837524\n",
            "Epoch 37/50 | Batch 101/198 | Loss: 1.550070881843567\n",
            "Epoch 37/50 | Batch 102/198 | Loss: 1.502872109413147\n",
            "Epoch 37/50 | Batch 103/198 | Loss: 1.5079885721206665\n",
            "Epoch 37/50 | Batch 104/198 | Loss: 1.6206305027008057\n",
            "Epoch 37/50 | Batch 105/198 | Loss: 1.698663592338562\n",
            "Epoch 37/50 | Batch 106/198 | Loss: 1.391616940498352\n",
            "Epoch 37/50 | Batch 107/198 | Loss: 1.3688242435455322\n",
            "Epoch 37/50 | Batch 108/198 | Loss: 1.4593063592910767\n",
            "Epoch 37/50 | Batch 109/198 | Loss: 1.4592244625091553\n",
            "Epoch 37/50 | Batch 110/198 | Loss: 1.3793245553970337\n",
            "Epoch 37/50 | Batch 111/198 | Loss: 1.2771108150482178\n",
            "Epoch 37/50 | Batch 112/198 | Loss: 1.5043139457702637\n",
            "Epoch 37/50 | Batch 113/198 | Loss: 1.407676339149475\n",
            "Epoch 37/50 | Batch 114/198 | Loss: 1.4620511531829834\n",
            "Epoch 37/50 | Batch 115/198 | Loss: 1.6771398782730103\n",
            "Epoch 37/50 | Batch 116/198 | Loss: 1.5083816051483154\n",
            "Epoch 37/50 | Batch 117/198 | Loss: 1.416573166847229\n",
            "Epoch 37/50 | Batch 118/198 | Loss: 1.4862949848175049\n",
            "Epoch 37/50 | Batch 119/198 | Loss: 1.505250096321106\n",
            "Epoch 37/50 | Batch 120/198 | Loss: 1.6636580228805542\n",
            "Epoch 37/50 | Batch 121/198 | Loss: 1.5227479934692383\n",
            "Epoch 37/50 | Batch 122/198 | Loss: 1.5841668844223022\n",
            "Epoch 37/50 | Batch 123/198 | Loss: 1.4448919296264648\n",
            "Epoch 37/50 | Batch 124/198 | Loss: 1.5192335844039917\n",
            "Epoch 37/50 | Batch 125/198 | Loss: 1.4672964811325073\n",
            "Epoch 37/50 | Batch 126/198 | Loss: 1.6092193126678467\n",
            "Epoch 37/50 | Batch 127/198 | Loss: 1.4735569953918457\n",
            "Epoch 37/50 | Batch 128/198 | Loss: 1.5209367275238037\n",
            "Epoch 37/50 | Batch 129/198 | Loss: 1.416866660118103\n",
            "Epoch 37/50 | Batch 130/198 | Loss: 1.4703333377838135\n",
            "Epoch 37/50 | Batch 131/198 | Loss: 1.5989950895309448\n",
            "Epoch 37/50 | Batch 132/198 | Loss: 1.4889302253723145\n",
            "Epoch 37/50 | Batch 133/198 | Loss: 1.5529617071151733\n",
            "Epoch 37/50 | Batch 134/198 | Loss: 1.4467085599899292\n",
            "Epoch 37/50 | Batch 135/198 | Loss: 1.448556900024414\n",
            "Epoch 37/50 | Batch 136/198 | Loss: 1.4729312658309937\n",
            "Epoch 37/50 | Batch 137/198 | Loss: 1.8191075325012207\n",
            "Epoch 37/50 | Batch 138/198 | Loss: 1.7149194478988647\n",
            "Epoch 37/50 | Batch 139/198 | Loss: 1.4820668697357178\n",
            "Epoch 37/50 | Batch 140/198 | Loss: 1.3428552150726318\n",
            "Epoch 37/50 | Batch 141/198 | Loss: 1.4583128690719604\n",
            "Epoch 37/50 | Batch 142/198 | Loss: 1.5303092002868652\n",
            "Epoch 37/50 | Batch 143/198 | Loss: 1.2319132089614868\n",
            "Epoch 37/50 | Batch 144/198 | Loss: 1.4179686307907104\n",
            "Epoch 37/50 | Batch 145/198 | Loss: 1.4747049808502197\n",
            "Epoch 37/50 | Batch 146/198 | Loss: 1.3699482679367065\n",
            "Epoch 37/50 | Batch 147/198 | Loss: 1.7276082038879395\n",
            "Epoch 37/50 | Batch 148/198 | Loss: 1.3936094045639038\n",
            "Epoch 37/50 | Batch 149/198 | Loss: 1.3847545385360718\n",
            "Epoch 37/50 | Batch 150/198 | Loss: 1.4849371910095215\n",
            "Epoch 37/50 | Batch 151/198 | Loss: 1.54657781124115\n",
            "Epoch 37/50 | Batch 152/198 | Loss: 1.6255948543548584\n",
            "Epoch 37/50 | Batch 153/198 | Loss: 1.5497924089431763\n",
            "Epoch 37/50 | Batch 154/198 | Loss: 1.297096848487854\n",
            "Epoch 37/50 | Batch 155/198 | Loss: 1.6661258935928345\n",
            "Epoch 37/50 | Batch 156/198 | Loss: 1.426517367362976\n",
            "Epoch 37/50 | Batch 157/198 | Loss: 1.501849889755249\n",
            "Epoch 37/50 | Batch 158/198 | Loss: 1.3408238887786865\n",
            "Epoch 37/50 | Batch 159/198 | Loss: 1.446312427520752\n",
            "Epoch 37/50 | Batch 160/198 | Loss: 1.5101063251495361\n",
            "Epoch 37/50 | Batch 161/198 | Loss: 1.4226040840148926\n",
            "Epoch 37/50 | Batch 162/198 | Loss: 1.5748364925384521\n",
            "Epoch 37/50 | Batch 163/198 | Loss: 1.305057406425476\n",
            "Epoch 37/50 | Batch 164/198 | Loss: 1.5184260606765747\n",
            "Epoch 37/50 | Batch 165/198 | Loss: 1.351043701171875\n",
            "Epoch 37/50 | Batch 166/198 | Loss: 1.3446043729782104\n",
            "Epoch 37/50 | Batch 167/198 | Loss: 1.5714094638824463\n",
            "Epoch 37/50 | Batch 168/198 | Loss: 1.4170300960540771\n",
            "Epoch 37/50 | Batch 169/198 | Loss: 1.5217995643615723\n",
            "Epoch 37/50 | Batch 170/198 | Loss: 1.7068887948989868\n",
            "Epoch 37/50 | Batch 171/198 | Loss: 1.5069983005523682\n",
            "Epoch 37/50 | Batch 172/198 | Loss: 1.5641906261444092\n",
            "Epoch 37/50 | Batch 173/198 | Loss: 1.565855860710144\n",
            "Epoch 37/50 | Batch 174/198 | Loss: 1.413244605064392\n",
            "Epoch 37/50 | Batch 175/198 | Loss: 1.4018597602844238\n",
            "Epoch 37/50 | Batch 176/198 | Loss: 1.6603785753250122\n",
            "Epoch 37/50 | Batch 177/198 | Loss: 1.4306457042694092\n",
            "Epoch 37/50 | Batch 178/198 | Loss: 1.4302186965942383\n",
            "Epoch 37/50 | Batch 179/198 | Loss: 1.447546362876892\n",
            "Epoch 37/50 | Batch 180/198 | Loss: 1.474393606185913\n",
            "Epoch 37/50 | Batch 181/198 | Loss: 1.3504430055618286\n",
            "Epoch 37/50 | Batch 182/198 | Loss: 1.6330933570861816\n",
            "Epoch 37/50 | Batch 183/198 | Loss: 1.507694125175476\n",
            "Epoch 37/50 | Batch 184/198 | Loss: 1.3159499168395996\n",
            "Epoch 37/50 | Batch 185/198 | Loss: 1.4481955766677856\n",
            "Epoch 37/50 | Batch 186/198 | Loss: 1.4535261392593384\n",
            "Epoch 37/50 | Batch 187/198 | Loss: 1.216323733329773\n",
            "Epoch 37/50 | Batch 188/198 | Loss: 1.593989372253418\n",
            "Epoch 37/50 | Batch 189/198 | Loss: 1.3540230989456177\n",
            "Epoch 37/50 | Batch 190/198 | Loss: 1.507765531539917\n",
            "Epoch 37/50 | Batch 191/198 | Loss: 1.2878198623657227\n",
            "Epoch 37/50 | Batch 192/198 | Loss: 1.5423452854156494\n",
            "Epoch 37/50 | Batch 193/198 | Loss: 1.198714017868042\n",
            "Epoch 37/50 | Batch 194/198 | Loss: 1.4247674942016602\n",
            "Epoch 37/50 | Batch 195/198 | Loss: 1.5615707635879517\n",
            "Epoch 37/50 | Batch 196/198 | Loss: 1.3009737730026245\n",
            "Epoch 37/50 | Batch 197/198 | Loss: 1.3112722635269165\n",
            "Epoch 37/50 | Batch 198/198 | Loss: 1.6223783493041992\n",
            "Epoch 37/50 | Average Loss: 1.4721241569278216\n",
            "Epoch 38/50 | Batch 1/198 | Loss: 1.4934355020523071\n",
            "Epoch 38/50 | Batch 2/198 | Loss: 1.5377191305160522\n",
            "Epoch 38/50 | Batch 3/198 | Loss: 1.4126156568527222\n",
            "Epoch 38/50 | Batch 4/198 | Loss: 1.3327488899230957\n",
            "Epoch 38/50 | Batch 5/198 | Loss: 1.5604944229125977\n",
            "Epoch 38/50 | Batch 6/198 | Loss: 1.7577729225158691\n",
            "Epoch 38/50 | Batch 7/198 | Loss: 1.3791362047195435\n",
            "Epoch 38/50 | Batch 8/198 | Loss: 1.5849847793579102\n",
            "Epoch 38/50 | Batch 9/198 | Loss: 1.4248666763305664\n",
            "Epoch 38/50 | Batch 10/198 | Loss: 1.4990960359573364\n",
            "Epoch 38/50 | Batch 11/198 | Loss: 1.3216838836669922\n",
            "Epoch 38/50 | Batch 12/198 | Loss: 1.4853805303573608\n",
            "Epoch 38/50 | Batch 13/198 | Loss: 1.4991780519485474\n",
            "Epoch 38/50 | Batch 14/198 | Loss: 1.4608439207077026\n",
            "Epoch 38/50 | Batch 15/198 | Loss: 1.2597302198410034\n",
            "Epoch 38/50 | Batch 16/198 | Loss: 1.4787414073944092\n",
            "Epoch 38/50 | Batch 17/198 | Loss: 1.5710586309432983\n",
            "Epoch 38/50 | Batch 18/198 | Loss: 1.4619250297546387\n",
            "Epoch 38/50 | Batch 19/198 | Loss: 1.5147708654403687\n",
            "Epoch 38/50 | Batch 20/198 | Loss: 1.4751960039138794\n",
            "Epoch 38/50 | Batch 21/198 | Loss: 1.2045642137527466\n",
            "Epoch 38/50 | Batch 22/198 | Loss: 1.4900363683700562\n",
            "Epoch 38/50 | Batch 23/198 | Loss: 1.2474944591522217\n",
            "Epoch 38/50 | Batch 24/198 | Loss: 1.365251064300537\n",
            "Epoch 38/50 | Batch 25/198 | Loss: 1.559029221534729\n",
            "Epoch 38/50 | Batch 26/198 | Loss: 1.3776789903640747\n",
            "Epoch 38/50 | Batch 27/198 | Loss: 1.6021959781646729\n",
            "Epoch 38/50 | Batch 28/198 | Loss: 1.5493392944335938\n",
            "Epoch 38/50 | Batch 29/198 | Loss: 1.223968505859375\n",
            "Epoch 38/50 | Batch 30/198 | Loss: 1.4281418323516846\n",
            "Epoch 38/50 | Batch 31/198 | Loss: 1.3165674209594727\n",
            "Epoch 38/50 | Batch 32/198 | Loss: 1.4152826070785522\n",
            "Epoch 38/50 | Batch 33/198 | Loss: 1.3710830211639404\n",
            "Epoch 38/50 | Batch 34/198 | Loss: 1.2214716672897339\n",
            "Epoch 38/50 | Batch 35/198 | Loss: 1.3339494466781616\n",
            "Epoch 38/50 | Batch 36/198 | Loss: 1.3403655290603638\n",
            "Epoch 38/50 | Batch 37/198 | Loss: 1.317197322845459\n",
            "Epoch 38/50 | Batch 38/198 | Loss: 1.4192370176315308\n",
            "Epoch 38/50 | Batch 39/198 | Loss: 1.5372081995010376\n",
            "Epoch 38/50 | Batch 40/198 | Loss: 1.252909779548645\n",
            "Epoch 38/50 | Batch 41/198 | Loss: 1.48183012008667\n",
            "Epoch 38/50 | Batch 42/198 | Loss: 1.4606229066848755\n",
            "Epoch 38/50 | Batch 43/198 | Loss: 1.3562452793121338\n",
            "Epoch 38/50 | Batch 44/198 | Loss: 1.31674063205719\n",
            "Epoch 38/50 | Batch 45/198 | Loss: 1.3441294431686401\n",
            "Epoch 38/50 | Batch 46/198 | Loss: 1.2280781269073486\n",
            "Epoch 38/50 | Batch 47/198 | Loss: 1.6004457473754883\n",
            "Epoch 38/50 | Batch 48/198 | Loss: 1.36129891872406\n",
            "Epoch 38/50 | Batch 49/198 | Loss: 1.5247212648391724\n",
            "Epoch 38/50 | Batch 50/198 | Loss: 1.5646709203720093\n",
            "Epoch 38/50 | Batch 51/198 | Loss: 1.4262747764587402\n",
            "Epoch 38/50 | Batch 52/198 | Loss: 1.579892873764038\n",
            "Epoch 38/50 | Batch 53/198 | Loss: 1.3701242208480835\n",
            "Epoch 38/50 | Batch 54/198 | Loss: 1.6342211961746216\n",
            "Epoch 38/50 | Batch 55/198 | Loss: 1.395261287689209\n",
            "Epoch 38/50 | Batch 56/198 | Loss: 1.6045348644256592\n",
            "Epoch 38/50 | Batch 57/198 | Loss: 1.4009610414505005\n",
            "Epoch 38/50 | Batch 58/198 | Loss: 1.4929213523864746\n",
            "Epoch 38/50 | Batch 59/198 | Loss: 1.4797083139419556\n",
            "Epoch 38/50 | Batch 60/198 | Loss: 1.6431885957717896\n",
            "Epoch 38/50 | Batch 61/198 | Loss: 1.4477471113204956\n",
            "Epoch 38/50 | Batch 62/198 | Loss: 1.401663899421692\n",
            "Epoch 38/50 | Batch 63/198 | Loss: 1.6032243967056274\n",
            "Epoch 38/50 | Batch 64/198 | Loss: 1.423254370689392\n",
            "Epoch 38/50 | Batch 65/198 | Loss: 1.4685842990875244\n",
            "Epoch 38/50 | Batch 66/198 | Loss: 1.2791393995285034\n",
            "Epoch 38/50 | Batch 67/198 | Loss: 1.459586501121521\n",
            "Epoch 38/50 | Batch 68/198 | Loss: 1.4333419799804688\n",
            "Epoch 38/50 | Batch 69/198 | Loss: 1.5058627128601074\n",
            "Epoch 38/50 | Batch 70/198 | Loss: 1.4165529012680054\n",
            "Epoch 38/50 | Batch 71/198 | Loss: 1.6298326253890991\n",
            "Epoch 38/50 | Batch 72/198 | Loss: 1.4198731184005737\n",
            "Epoch 38/50 | Batch 73/198 | Loss: 1.5855258703231812\n",
            "Epoch 38/50 | Batch 74/198 | Loss: 1.6813428401947021\n",
            "Epoch 38/50 | Batch 75/198 | Loss: 1.3791571855545044\n",
            "Epoch 38/50 | Batch 76/198 | Loss: 1.3532336950302124\n",
            "Epoch 38/50 | Batch 77/198 | Loss: 1.3882334232330322\n",
            "Epoch 38/50 | Batch 78/198 | Loss: 1.6487250328063965\n",
            "Epoch 38/50 | Batch 79/198 | Loss: 1.587393879890442\n",
            "Epoch 38/50 | Batch 80/198 | Loss: 1.410536527633667\n",
            "Epoch 38/50 | Batch 81/198 | Loss: 1.530916690826416\n",
            "Epoch 38/50 | Batch 82/198 | Loss: 1.450334072113037\n",
            "Epoch 38/50 | Batch 83/198 | Loss: 1.3224636316299438\n",
            "Epoch 38/50 | Batch 84/198 | Loss: 1.5657665729522705\n",
            "Epoch 38/50 | Batch 85/198 | Loss: 1.3906551599502563\n",
            "Epoch 38/50 | Batch 86/198 | Loss: 1.3207148313522339\n",
            "Epoch 38/50 | Batch 87/198 | Loss: 1.424467921257019\n",
            "Epoch 38/50 | Batch 88/198 | Loss: 1.5370464324951172\n",
            "Epoch 38/50 | Batch 89/198 | Loss: 1.5212243795394897\n",
            "Epoch 38/50 | Batch 90/198 | Loss: 1.5412013530731201\n",
            "Epoch 38/50 | Batch 91/198 | Loss: 1.369388461112976\n",
            "Epoch 38/50 | Batch 92/198 | Loss: 1.439211368560791\n",
            "Epoch 38/50 | Batch 93/198 | Loss: 1.6474820375442505\n",
            "Epoch 38/50 | Batch 94/198 | Loss: 1.4917728900909424\n",
            "Epoch 38/50 | Batch 95/198 | Loss: 1.3132561445236206\n",
            "Epoch 38/50 | Batch 96/198 | Loss: 1.4261201620101929\n",
            "Epoch 38/50 | Batch 97/198 | Loss: 1.609283208847046\n",
            "Epoch 38/50 | Batch 98/198 | Loss: 1.4722563028335571\n",
            "Epoch 38/50 | Batch 99/198 | Loss: 1.3177858591079712\n",
            "Epoch 38/50 | Batch 100/198 | Loss: 1.5589615106582642\n",
            "Epoch 38/50 | Batch 101/198 | Loss: 1.7034943103790283\n",
            "Epoch 38/50 | Batch 102/198 | Loss: 1.4324325323104858\n",
            "Epoch 38/50 | Batch 103/198 | Loss: 1.2551127672195435\n",
            "Epoch 38/50 | Batch 104/198 | Loss: 1.5041351318359375\n",
            "Epoch 38/50 | Batch 105/198 | Loss: 1.5394091606140137\n",
            "Epoch 38/50 | Batch 106/198 | Loss: 1.3784658908843994\n",
            "Epoch 38/50 | Batch 107/198 | Loss: 1.4753069877624512\n",
            "Epoch 38/50 | Batch 108/198 | Loss: 1.6176708936691284\n",
            "Epoch 38/50 | Batch 109/198 | Loss: 1.3700884580612183\n",
            "Epoch 38/50 | Batch 110/198 | Loss: 1.289062738418579\n",
            "Epoch 38/50 | Batch 111/198 | Loss: 1.3653146028518677\n",
            "Epoch 38/50 | Batch 112/198 | Loss: 1.4801366329193115\n",
            "Epoch 38/50 | Batch 113/198 | Loss: 1.6757088899612427\n",
            "Epoch 38/50 | Batch 114/198 | Loss: 1.6217396259307861\n",
            "Epoch 38/50 | Batch 115/198 | Loss: 1.4534693956375122\n",
            "Epoch 38/50 | Batch 116/198 | Loss: 1.6735087633132935\n",
            "Epoch 38/50 | Batch 117/198 | Loss: 1.4519726037979126\n",
            "Epoch 38/50 | Batch 118/198 | Loss: 1.5485109090805054\n",
            "Epoch 38/50 | Batch 119/198 | Loss: 1.473471999168396\n",
            "Epoch 38/50 | Batch 120/198 | Loss: 1.494881272315979\n",
            "Epoch 38/50 | Batch 121/198 | Loss: 1.2944332361221313\n",
            "Epoch 38/50 | Batch 122/198 | Loss: 1.4662771224975586\n",
            "Epoch 38/50 | Batch 123/198 | Loss: 1.2976917028427124\n",
            "Epoch 38/50 | Batch 124/198 | Loss: 1.4657832384109497\n",
            "Epoch 38/50 | Batch 125/198 | Loss: 1.152100920677185\n",
            "Epoch 38/50 | Batch 126/198 | Loss: 1.386795997619629\n",
            "Epoch 38/50 | Batch 127/198 | Loss: 1.3573613166809082\n",
            "Epoch 38/50 | Batch 128/198 | Loss: 1.5389024019241333\n",
            "Epoch 38/50 | Batch 129/198 | Loss: 1.6482409238815308\n",
            "Epoch 38/50 | Batch 130/198 | Loss: 1.3831933736801147\n",
            "Epoch 38/50 | Batch 131/198 | Loss: 1.2176623344421387\n",
            "Epoch 38/50 | Batch 132/198 | Loss: 1.5823427438735962\n",
            "Epoch 38/50 | Batch 133/198 | Loss: 1.527530312538147\n",
            "Epoch 38/50 | Batch 134/198 | Loss: 1.5257089138031006\n",
            "Epoch 38/50 | Batch 135/198 | Loss: 1.394383192062378\n",
            "Epoch 38/50 | Batch 136/198 | Loss: 1.4144384860992432\n",
            "Epoch 38/50 | Batch 137/198 | Loss: 1.3608158826828003\n",
            "Epoch 38/50 | Batch 138/198 | Loss: 1.5111017227172852\n",
            "Epoch 38/50 | Batch 139/198 | Loss: 1.4065158367156982\n",
            "Epoch 38/50 | Batch 140/198 | Loss: 1.5314444303512573\n",
            "Epoch 38/50 | Batch 141/198 | Loss: 1.4969561100006104\n",
            "Epoch 38/50 | Batch 142/198 | Loss: 1.5286234617233276\n",
            "Epoch 38/50 | Batch 143/198 | Loss: 1.4577311277389526\n",
            "Epoch 38/50 | Batch 144/198 | Loss: 1.6086349487304688\n",
            "Epoch 38/50 | Batch 145/198 | Loss: 1.6039795875549316\n",
            "Epoch 38/50 | Batch 146/198 | Loss: 1.52803635597229\n",
            "Epoch 38/50 | Batch 147/198 | Loss: 1.4641765356063843\n",
            "Epoch 38/50 | Batch 148/198 | Loss: 1.3216580152511597\n",
            "Epoch 38/50 | Batch 149/198 | Loss: 1.4597967863082886\n",
            "Epoch 38/50 | Batch 150/198 | Loss: 1.7137665748596191\n",
            "Epoch 38/50 | Batch 151/198 | Loss: 1.4845640659332275\n",
            "Epoch 38/50 | Batch 152/198 | Loss: 1.675291657447815\n",
            "Epoch 38/50 | Batch 153/198 | Loss: 1.5233654975891113\n",
            "Epoch 38/50 | Batch 154/198 | Loss: 1.540920615196228\n",
            "Epoch 38/50 | Batch 155/198 | Loss: 1.4994807243347168\n",
            "Epoch 38/50 | Batch 156/198 | Loss: 1.4139031171798706\n",
            "Epoch 38/50 | Batch 157/198 | Loss: 1.3505887985229492\n",
            "Epoch 38/50 | Batch 158/198 | Loss: 1.4225409030914307\n",
            "Epoch 38/50 | Batch 159/198 | Loss: 1.2080559730529785\n",
            "Epoch 38/50 | Batch 160/198 | Loss: 1.5792471170425415\n",
            "Epoch 38/50 | Batch 161/198 | Loss: 1.366115689277649\n",
            "Epoch 38/50 | Batch 162/198 | Loss: 1.4588607549667358\n",
            "Epoch 38/50 | Batch 163/198 | Loss: 1.525858759880066\n",
            "Epoch 38/50 | Batch 164/198 | Loss: 1.448898196220398\n",
            "Epoch 38/50 | Batch 165/198 | Loss: 1.420809268951416\n",
            "Epoch 38/50 | Batch 166/198 | Loss: 1.509421467781067\n",
            "Epoch 38/50 | Batch 167/198 | Loss: 1.4957438707351685\n",
            "Epoch 38/50 | Batch 168/198 | Loss: 1.4644838571548462\n",
            "Epoch 38/50 | Batch 169/198 | Loss: 1.4985601902008057\n",
            "Epoch 38/50 | Batch 170/198 | Loss: 1.3487797975540161\n",
            "Epoch 38/50 | Batch 171/198 | Loss: 1.6780452728271484\n",
            "Epoch 38/50 | Batch 172/198 | Loss: 1.304093360900879\n",
            "Epoch 38/50 | Batch 173/198 | Loss: 1.3760074377059937\n",
            "Epoch 38/50 | Batch 174/198 | Loss: 1.5202350616455078\n",
            "Epoch 38/50 | Batch 175/198 | Loss: 1.524656057357788\n",
            "Epoch 38/50 | Batch 176/198 | Loss: 1.4090853929519653\n",
            "Epoch 38/50 | Batch 177/198 | Loss: 1.5325987339019775\n",
            "Epoch 38/50 | Batch 178/198 | Loss: 1.401263952255249\n",
            "Epoch 38/50 | Batch 179/198 | Loss: 1.6365225315093994\n",
            "Epoch 38/50 | Batch 180/198 | Loss: 1.367317795753479\n",
            "Epoch 38/50 | Batch 181/198 | Loss: 1.4832271337509155\n",
            "Epoch 38/50 | Batch 182/198 | Loss: 1.3512955904006958\n",
            "Epoch 38/50 | Batch 183/198 | Loss: 1.3862882852554321\n",
            "Epoch 38/50 | Batch 184/198 | Loss: 1.4026856422424316\n",
            "Epoch 38/50 | Batch 185/198 | Loss: 1.3312724828720093\n",
            "Epoch 38/50 | Batch 186/198 | Loss: 1.4582545757293701\n",
            "Epoch 38/50 | Batch 187/198 | Loss: 1.741971492767334\n",
            "Epoch 38/50 | Batch 188/198 | Loss: 1.4872651100158691\n",
            "Epoch 38/50 | Batch 189/198 | Loss: 1.378165602684021\n",
            "Epoch 38/50 | Batch 190/198 | Loss: 1.5507988929748535\n",
            "Epoch 38/50 | Batch 191/198 | Loss: 1.3891656398773193\n",
            "Epoch 38/50 | Batch 192/198 | Loss: 1.3144851922988892\n",
            "Epoch 38/50 | Batch 193/198 | Loss: 1.3761560916900635\n",
            "Epoch 38/50 | Batch 194/198 | Loss: 1.5597952604293823\n",
            "Epoch 38/50 | Batch 195/198 | Loss: 1.4152920246124268\n",
            "Epoch 38/50 | Batch 196/198 | Loss: 1.611288070678711\n",
            "Epoch 38/50 | Batch 197/198 | Loss: 1.494332194328308\n",
            "Epoch 38/50 | Batch 198/198 | Loss: 1.426375150680542\n",
            "Epoch 38/50 | Average Loss: 1.4581654083849205\n",
            "Epoch 39/50 | Batch 1/198 | Loss: 1.5380679368972778\n",
            "Epoch 39/50 | Batch 2/198 | Loss: 1.294419527053833\n",
            "Epoch 39/50 | Batch 3/198 | Loss: 1.5117311477661133\n",
            "Epoch 39/50 | Batch 4/198 | Loss: 1.300711750984192\n",
            "Epoch 39/50 | Batch 5/198 | Loss: 1.3755013942718506\n",
            "Epoch 39/50 | Batch 6/198 | Loss: 1.4123516082763672\n",
            "Epoch 39/50 | Batch 7/198 | Loss: 1.4296492338180542\n",
            "Epoch 39/50 | Batch 8/198 | Loss: 1.5437666177749634\n",
            "Epoch 39/50 | Batch 9/198 | Loss: 1.4699115753173828\n",
            "Epoch 39/50 | Batch 10/198 | Loss: 1.43921959400177\n",
            "Epoch 39/50 | Batch 11/198 | Loss: 1.4953103065490723\n",
            "Epoch 39/50 | Batch 12/198 | Loss: 1.4941457509994507\n",
            "Epoch 39/50 | Batch 13/198 | Loss: 1.586930513381958\n",
            "Epoch 39/50 | Batch 14/198 | Loss: 1.455452561378479\n",
            "Epoch 39/50 | Batch 15/198 | Loss: 1.3661295175552368\n",
            "Epoch 39/50 | Batch 16/198 | Loss: 1.286049246788025\n",
            "Epoch 39/50 | Batch 17/198 | Loss: 1.45325767993927\n",
            "Epoch 39/50 | Batch 18/198 | Loss: 1.4634958505630493\n",
            "Epoch 39/50 | Batch 19/198 | Loss: 1.5912113189697266\n",
            "Epoch 39/50 | Batch 20/198 | Loss: 1.4252928495407104\n",
            "Epoch 39/50 | Batch 21/198 | Loss: 1.3021762371063232\n",
            "Epoch 39/50 | Batch 22/198 | Loss: 1.402504324913025\n",
            "Epoch 39/50 | Batch 23/198 | Loss: 1.5556387901306152\n",
            "Epoch 39/50 | Batch 24/198 | Loss: 1.4854053258895874\n",
            "Epoch 39/50 | Batch 25/198 | Loss: 1.368599534034729\n",
            "Epoch 39/50 | Batch 26/198 | Loss: 1.3712489604949951\n",
            "Epoch 39/50 | Batch 27/198 | Loss: 1.6071664094924927\n",
            "Epoch 39/50 | Batch 28/198 | Loss: 1.4803012609481812\n",
            "Epoch 39/50 | Batch 29/198 | Loss: 1.4189462661743164\n",
            "Epoch 39/50 | Batch 30/198 | Loss: 1.4454814195632935\n",
            "Epoch 39/50 | Batch 31/198 | Loss: 1.4171829223632812\n",
            "Epoch 39/50 | Batch 32/198 | Loss: 1.573176622390747\n",
            "Epoch 39/50 | Batch 33/198 | Loss: 1.5215264558792114\n",
            "Epoch 39/50 | Batch 34/198 | Loss: 1.499284267425537\n",
            "Epoch 39/50 | Batch 35/198 | Loss: 1.2637919187545776\n",
            "Epoch 39/50 | Batch 36/198 | Loss: 1.4864444732666016\n",
            "Epoch 39/50 | Batch 37/198 | Loss: 1.3218564987182617\n",
            "Epoch 39/50 | Batch 38/198 | Loss: 1.3981961011886597\n",
            "Epoch 39/50 | Batch 39/198 | Loss: 1.501803994178772\n",
            "Epoch 39/50 | Batch 40/198 | Loss: 1.4532957077026367\n",
            "Epoch 39/50 | Batch 41/198 | Loss: 1.576981544494629\n",
            "Epoch 39/50 | Batch 42/198 | Loss: 1.5758869647979736\n",
            "Epoch 39/50 | Batch 43/198 | Loss: 1.2719449996948242\n",
            "Epoch 39/50 | Batch 44/198 | Loss: 1.4858356714248657\n",
            "Epoch 39/50 | Batch 45/198 | Loss: 1.77439284324646\n",
            "Epoch 39/50 | Batch 46/198 | Loss: 1.5303806066513062\n",
            "Epoch 39/50 | Batch 47/198 | Loss: 1.49252188205719\n",
            "Epoch 39/50 | Batch 48/198 | Loss: 1.5251425504684448\n",
            "Epoch 39/50 | Batch 49/198 | Loss: 1.6533313989639282\n",
            "Epoch 39/50 | Batch 50/198 | Loss: 1.3760957717895508\n",
            "Epoch 39/50 | Batch 51/198 | Loss: 1.4649076461791992\n",
            "Epoch 39/50 | Batch 52/198 | Loss: 1.33413827419281\n",
            "Epoch 39/50 | Batch 53/198 | Loss: 1.2540358304977417\n",
            "Epoch 39/50 | Batch 54/198 | Loss: 1.7528616189956665\n",
            "Epoch 39/50 | Batch 55/198 | Loss: 1.3962936401367188\n",
            "Epoch 39/50 | Batch 56/198 | Loss: 1.259444236755371\n",
            "Epoch 39/50 | Batch 57/198 | Loss: 1.2656084299087524\n",
            "Epoch 39/50 | Batch 58/198 | Loss: 1.5437453985214233\n",
            "Epoch 39/50 | Batch 59/198 | Loss: 1.4688290357589722\n",
            "Epoch 39/50 | Batch 60/198 | Loss: 1.503481149673462\n",
            "Epoch 39/50 | Batch 61/198 | Loss: 1.0822111368179321\n",
            "Epoch 39/50 | Batch 62/198 | Loss: 1.5853288173675537\n",
            "Epoch 39/50 | Batch 63/198 | Loss: 1.4568371772766113\n",
            "Epoch 39/50 | Batch 64/198 | Loss: 1.4236252307891846\n",
            "Epoch 39/50 | Batch 65/198 | Loss: 1.3645833730697632\n",
            "Epoch 39/50 | Batch 66/198 | Loss: 1.4674363136291504\n",
            "Epoch 39/50 | Batch 67/198 | Loss: 1.4297224283218384\n",
            "Epoch 39/50 | Batch 68/198 | Loss: 1.3177545070648193\n",
            "Epoch 39/50 | Batch 69/198 | Loss: 1.3427274227142334\n",
            "Epoch 39/50 | Batch 70/198 | Loss: 1.3144975900650024\n",
            "Epoch 39/50 | Batch 71/198 | Loss: 1.3226475715637207\n",
            "Epoch 39/50 | Batch 72/198 | Loss: 1.3862764835357666\n",
            "Epoch 39/50 | Batch 73/198 | Loss: 1.4274113178253174\n",
            "Epoch 39/50 | Batch 74/198 | Loss: 1.3832048177719116\n",
            "Epoch 39/50 | Batch 75/198 | Loss: 1.437519907951355\n",
            "Epoch 39/50 | Batch 76/198 | Loss: 1.4913229942321777\n",
            "Epoch 39/50 | Batch 77/198 | Loss: 1.3270390033721924\n",
            "Epoch 39/50 | Batch 78/198 | Loss: 1.6275513172149658\n",
            "Epoch 39/50 | Batch 79/198 | Loss: 1.3929355144500732\n",
            "Epoch 39/50 | Batch 80/198 | Loss: 1.2706295251846313\n",
            "Epoch 39/50 | Batch 81/198 | Loss: 1.5444562435150146\n",
            "Epoch 39/50 | Batch 82/198 | Loss: 1.5667057037353516\n",
            "Epoch 39/50 | Batch 83/198 | Loss: 1.2669919729232788\n",
            "Epoch 39/50 | Batch 84/198 | Loss: 1.3298496007919312\n",
            "Epoch 39/50 | Batch 85/198 | Loss: 1.4733152389526367\n",
            "Epoch 39/50 | Batch 86/198 | Loss: 1.6234091520309448\n",
            "Epoch 39/50 | Batch 87/198 | Loss: 1.4609194993972778\n",
            "Epoch 39/50 | Batch 88/198 | Loss: 1.667073369026184\n",
            "Epoch 39/50 | Batch 89/198 | Loss: 1.3463083505630493\n",
            "Epoch 39/50 | Batch 90/198 | Loss: 1.3383311033248901\n",
            "Epoch 39/50 | Batch 91/198 | Loss: 1.3950797319412231\n",
            "Epoch 39/50 | Batch 92/198 | Loss: 1.730985403060913\n",
            "Epoch 39/50 | Batch 93/198 | Loss: 1.3845933675765991\n",
            "Epoch 39/50 | Batch 94/198 | Loss: 1.607931137084961\n",
            "Epoch 39/50 | Batch 95/198 | Loss: 1.5599734783172607\n",
            "Epoch 39/50 | Batch 96/198 | Loss: 1.4025834798812866\n",
            "Epoch 39/50 | Batch 97/198 | Loss: 1.337147831916809\n",
            "Epoch 39/50 | Batch 98/198 | Loss: 1.4803482294082642\n",
            "Epoch 39/50 | Batch 99/198 | Loss: 1.5422881841659546\n",
            "Epoch 39/50 | Batch 100/198 | Loss: 1.5689352750778198\n",
            "Epoch 39/50 | Batch 101/198 | Loss: 1.4914898872375488\n",
            "Epoch 39/50 | Batch 102/198 | Loss: 1.4733872413635254\n",
            "Epoch 39/50 | Batch 103/198 | Loss: 1.34398353099823\n",
            "Epoch 39/50 | Batch 104/198 | Loss: 1.5106298923492432\n",
            "Epoch 39/50 | Batch 105/198 | Loss: 1.495918869972229\n",
            "Epoch 39/50 | Batch 106/198 | Loss: 1.242613673210144\n",
            "Epoch 39/50 | Batch 107/198 | Loss: 1.6940897703170776\n",
            "Epoch 39/50 | Batch 108/198 | Loss: 1.4674986600875854\n",
            "Epoch 39/50 | Batch 109/198 | Loss: 1.5309683084487915\n",
            "Epoch 39/50 | Batch 110/198 | Loss: 1.2630642652511597\n",
            "Epoch 39/50 | Batch 111/198 | Loss: 1.4644520282745361\n",
            "Epoch 39/50 | Batch 112/198 | Loss: 1.404916763305664\n",
            "Epoch 39/50 | Batch 113/198 | Loss: 1.5693422555923462\n",
            "Epoch 39/50 | Batch 114/198 | Loss: 1.3813022375106812\n",
            "Epoch 39/50 | Batch 115/198 | Loss: 1.597375750541687\n",
            "Epoch 39/50 | Batch 116/198 | Loss: 1.4191066026687622\n",
            "Epoch 39/50 | Batch 117/198 | Loss: 1.6511921882629395\n",
            "Epoch 39/50 | Batch 118/198 | Loss: 1.3571175336837769\n",
            "Epoch 39/50 | Batch 119/198 | Loss: 1.3240001201629639\n",
            "Epoch 39/50 | Batch 120/198 | Loss: 1.5689060688018799\n",
            "Epoch 39/50 | Batch 121/198 | Loss: 1.4190592765808105\n",
            "Epoch 39/50 | Batch 122/198 | Loss: 1.3935800790786743\n",
            "Epoch 39/50 | Batch 123/198 | Loss: 1.3348362445831299\n",
            "Epoch 39/50 | Batch 124/198 | Loss: 1.3792186975479126\n",
            "Epoch 39/50 | Batch 125/198 | Loss: 1.3593506813049316\n",
            "Epoch 39/50 | Batch 126/198 | Loss: 1.4027212858200073\n",
            "Epoch 39/50 | Batch 127/198 | Loss: 1.3687621355056763\n",
            "Epoch 39/50 | Batch 128/198 | Loss: 1.2859530448913574\n",
            "Epoch 39/50 | Batch 129/198 | Loss: 1.4673974514007568\n",
            "Epoch 39/50 | Batch 130/198 | Loss: 1.5056184530258179\n",
            "Epoch 39/50 | Batch 131/198 | Loss: 1.3748376369476318\n",
            "Epoch 39/50 | Batch 132/198 | Loss: 1.5572171211242676\n",
            "Epoch 39/50 | Batch 133/198 | Loss: 1.4804044961929321\n",
            "Epoch 39/50 | Batch 134/198 | Loss: 1.6316531896591187\n",
            "Epoch 39/50 | Batch 135/198 | Loss: 1.3243590593338013\n",
            "Epoch 39/50 | Batch 136/198 | Loss: 1.5166949033737183\n",
            "Epoch 39/50 | Batch 137/198 | Loss: 1.4393422603607178\n",
            "Epoch 39/50 | Batch 138/198 | Loss: 1.3871294260025024\n",
            "Epoch 39/50 | Batch 139/198 | Loss: 1.6617977619171143\n",
            "Epoch 39/50 | Batch 140/198 | Loss: 1.556107997894287\n",
            "Epoch 39/50 | Batch 141/198 | Loss: 1.4024144411087036\n",
            "Epoch 39/50 | Batch 142/198 | Loss: 1.433333396911621\n",
            "Epoch 39/50 | Batch 143/198 | Loss: 1.5469715595245361\n",
            "Epoch 39/50 | Batch 144/198 | Loss: 1.3618587255477905\n",
            "Epoch 39/50 | Batch 145/198 | Loss: 1.3330186605453491\n",
            "Epoch 39/50 | Batch 146/198 | Loss: 1.4399049282073975\n",
            "Epoch 39/50 | Batch 147/198 | Loss: 1.6362593173980713\n",
            "Epoch 39/50 | Batch 148/198 | Loss: 1.5904170274734497\n",
            "Epoch 39/50 | Batch 149/198 | Loss: 1.3486844301223755\n",
            "Epoch 39/50 | Batch 150/198 | Loss: 1.4021916389465332\n",
            "Epoch 39/50 | Batch 151/198 | Loss: 1.4980093240737915\n",
            "Epoch 39/50 | Batch 152/198 | Loss: 1.389723777770996\n",
            "Epoch 39/50 | Batch 153/198 | Loss: 1.372976303100586\n",
            "Epoch 39/50 | Batch 154/198 | Loss: 1.59389328956604\n",
            "Epoch 39/50 | Batch 155/198 | Loss: 1.353944182395935\n",
            "Epoch 39/50 | Batch 156/198 | Loss: 1.4759876728057861\n",
            "Epoch 39/50 | Batch 157/198 | Loss: 1.3808865547180176\n",
            "Epoch 39/50 | Batch 158/198 | Loss: 1.2048534154891968\n",
            "Epoch 39/50 | Batch 159/198 | Loss: 1.5207676887512207\n",
            "Epoch 39/50 | Batch 160/198 | Loss: 1.4490991830825806\n",
            "Epoch 39/50 | Batch 161/198 | Loss: 1.4896280765533447\n",
            "Epoch 39/50 | Batch 162/198 | Loss: 1.3038403987884521\n",
            "Epoch 39/50 | Batch 163/198 | Loss: 1.3949779272079468\n",
            "Epoch 39/50 | Batch 164/198 | Loss: 1.5768463611602783\n",
            "Epoch 39/50 | Batch 165/198 | Loss: 1.2694997787475586\n",
            "Epoch 39/50 | Batch 166/198 | Loss: 1.3401204347610474\n",
            "Epoch 39/50 | Batch 167/198 | Loss: 1.3769983053207397\n",
            "Epoch 39/50 | Batch 168/198 | Loss: 1.3277485370635986\n",
            "Epoch 39/50 | Batch 169/198 | Loss: 1.5230776071548462\n",
            "Epoch 39/50 | Batch 170/198 | Loss: 1.3602187633514404\n",
            "Epoch 39/50 | Batch 171/198 | Loss: 1.446441650390625\n",
            "Epoch 39/50 | Batch 172/198 | Loss: 1.4608701467514038\n",
            "Epoch 39/50 | Batch 173/198 | Loss: 1.3359663486480713\n",
            "Epoch 39/50 | Batch 174/198 | Loss: 1.4298597574234009\n",
            "Epoch 39/50 | Batch 175/198 | Loss: 1.4183672666549683\n",
            "Epoch 39/50 | Batch 176/198 | Loss: 1.3269097805023193\n",
            "Epoch 39/50 | Batch 177/198 | Loss: 1.3698110580444336\n",
            "Epoch 39/50 | Batch 178/198 | Loss: 1.3591231107711792\n",
            "Epoch 39/50 | Batch 179/198 | Loss: 1.2716313600540161\n",
            "Epoch 39/50 | Batch 180/198 | Loss: 1.538807988166809\n",
            "Epoch 39/50 | Batch 181/198 | Loss: 1.4700300693511963\n",
            "Epoch 39/50 | Batch 182/198 | Loss: 1.5725908279418945\n",
            "Epoch 39/50 | Batch 183/198 | Loss: 1.467604398727417\n",
            "Epoch 39/50 | Batch 184/198 | Loss: 1.4626131057739258\n",
            "Epoch 39/50 | Batch 185/198 | Loss: 1.4052485227584839\n",
            "Epoch 39/50 | Batch 186/198 | Loss: 1.3278974294662476\n",
            "Epoch 39/50 | Batch 187/198 | Loss: 1.5403603315353394\n",
            "Epoch 39/50 | Batch 188/198 | Loss: 1.5505534410476685\n",
            "Epoch 39/50 | Batch 189/198 | Loss: 1.4437199831008911\n",
            "Epoch 39/50 | Batch 190/198 | Loss: 1.3716378211975098\n",
            "Epoch 39/50 | Batch 191/198 | Loss: 1.2995846271514893\n",
            "Epoch 39/50 | Batch 192/198 | Loss: 1.7197307348251343\n",
            "Epoch 39/50 | Batch 193/198 | Loss: 1.4951494932174683\n",
            "Epoch 39/50 | Batch 194/198 | Loss: 1.5144760608673096\n",
            "Epoch 39/50 | Batch 195/198 | Loss: 1.3262807130813599\n",
            "Epoch 39/50 | Batch 196/198 | Loss: 1.528350830078125\n",
            "Epoch 39/50 | Batch 197/198 | Loss: 1.5640981197357178\n",
            "Epoch 39/50 | Batch 198/198 | Loss: 1.6781078577041626\n",
            "Epoch 39/50 | Average Loss: 1.4458704705190177\n",
            "Epoch 40/50 | Batch 1/198 | Loss: 1.4134048223495483\n",
            "Epoch 40/50 | Batch 2/198 | Loss: 1.6026490926742554\n",
            "Epoch 40/50 | Batch 3/198 | Loss: 1.3113689422607422\n",
            "Epoch 40/50 | Batch 4/198 | Loss: 1.4841276407241821\n",
            "Epoch 40/50 | Batch 5/198 | Loss: 1.3033086061477661\n",
            "Epoch 40/50 | Batch 6/198 | Loss: 1.4720041751861572\n",
            "Epoch 40/50 | Batch 7/198 | Loss: 1.3865063190460205\n",
            "Epoch 40/50 | Batch 8/198 | Loss: 1.4425150156021118\n",
            "Epoch 40/50 | Batch 9/198 | Loss: 1.401579737663269\n",
            "Epoch 40/50 | Batch 10/198 | Loss: 1.1883236169815063\n",
            "Epoch 40/50 | Batch 11/198 | Loss: 1.4172347784042358\n",
            "Epoch 40/50 | Batch 12/198 | Loss: 1.4284511804580688\n",
            "Epoch 40/50 | Batch 13/198 | Loss: 1.3974313735961914\n",
            "Epoch 40/50 | Batch 14/198 | Loss: 1.4837762117385864\n",
            "Epoch 40/50 | Batch 15/198 | Loss: 1.4108431339263916\n",
            "Epoch 40/50 | Batch 16/198 | Loss: 1.646101713180542\n",
            "Epoch 40/50 | Batch 17/198 | Loss: 1.760013222694397\n",
            "Epoch 40/50 | Batch 18/198 | Loss: 1.407139778137207\n",
            "Epoch 40/50 | Batch 19/198 | Loss: 1.4030832052230835\n",
            "Epoch 40/50 | Batch 20/198 | Loss: 1.1150542497634888\n",
            "Epoch 40/50 | Batch 21/198 | Loss: 1.4824886322021484\n",
            "Epoch 40/50 | Batch 22/198 | Loss: 1.332278847694397\n",
            "Epoch 40/50 | Batch 23/198 | Loss: 1.446412444114685\n",
            "Epoch 40/50 | Batch 24/198 | Loss: 1.3267894983291626\n",
            "Epoch 40/50 | Batch 25/198 | Loss: 1.38950514793396\n",
            "Epoch 40/50 | Batch 26/198 | Loss: 1.4183062314987183\n",
            "Epoch 40/50 | Batch 27/198 | Loss: 1.2652902603149414\n",
            "Epoch 40/50 | Batch 28/198 | Loss: 1.4304932355880737\n",
            "Epoch 40/50 | Batch 29/198 | Loss: 1.548862338066101\n",
            "Epoch 40/50 | Batch 30/198 | Loss: 1.429211974143982\n",
            "Epoch 40/50 | Batch 31/198 | Loss: 1.3356099128723145\n",
            "Epoch 40/50 | Batch 32/198 | Loss: 1.4397059679031372\n",
            "Epoch 40/50 | Batch 33/198 | Loss: 1.3616604804992676\n",
            "Epoch 40/50 | Batch 34/198 | Loss: 1.4177765846252441\n",
            "Epoch 40/50 | Batch 35/198 | Loss: 1.3480271100997925\n",
            "Epoch 40/50 | Batch 36/198 | Loss: 1.3959565162658691\n",
            "Epoch 40/50 | Batch 37/198 | Loss: 1.5526899099349976\n",
            "Epoch 40/50 | Batch 38/198 | Loss: 1.2156367301940918\n",
            "Epoch 40/50 | Batch 39/198 | Loss: 1.390203595161438\n",
            "Epoch 40/50 | Batch 40/198 | Loss: 1.391597867012024\n",
            "Epoch 40/50 | Batch 41/198 | Loss: 1.270287036895752\n",
            "Epoch 40/50 | Batch 42/198 | Loss: 1.5161168575286865\n",
            "Epoch 40/50 | Batch 43/198 | Loss: 1.3080801963806152\n",
            "Epoch 40/50 | Batch 44/198 | Loss: 1.6117178201675415\n",
            "Epoch 40/50 | Batch 45/198 | Loss: 1.3425103425979614\n",
            "Epoch 40/50 | Batch 46/198 | Loss: 1.636985421180725\n",
            "Epoch 40/50 | Batch 47/198 | Loss: 1.6317763328552246\n",
            "Epoch 40/50 | Batch 48/198 | Loss: 1.6138535737991333\n",
            "Epoch 40/50 | Batch 49/198 | Loss: 1.6804052591323853\n",
            "Epoch 40/50 | Batch 50/198 | Loss: 1.5514434576034546\n",
            "Epoch 40/50 | Batch 51/198 | Loss: 1.434653401374817\n",
            "Epoch 40/50 | Batch 52/198 | Loss: 1.4947876930236816\n",
            "Epoch 40/50 | Batch 53/198 | Loss: 1.3773573637008667\n",
            "Epoch 40/50 | Batch 54/198 | Loss: 1.3617117404937744\n",
            "Epoch 40/50 | Batch 55/198 | Loss: 1.3229613304138184\n",
            "Epoch 40/50 | Batch 56/198 | Loss: 1.3405004739761353\n",
            "Epoch 40/50 | Batch 57/198 | Loss: 1.4536722898483276\n",
            "Epoch 40/50 | Batch 58/198 | Loss: 1.454618215560913\n",
            "Epoch 40/50 | Batch 59/198 | Loss: 1.4436416625976562\n",
            "Epoch 40/50 | Batch 60/198 | Loss: 1.531089186668396\n",
            "Epoch 40/50 | Batch 61/198 | Loss: 1.2418431043624878\n",
            "Epoch 40/50 | Batch 62/198 | Loss: 1.462990641593933\n",
            "Epoch 40/50 | Batch 63/198 | Loss: 1.4987772703170776\n",
            "Epoch 40/50 | Batch 64/198 | Loss: 1.4966866970062256\n",
            "Epoch 40/50 | Batch 65/198 | Loss: 1.6534367799758911\n",
            "Epoch 40/50 | Batch 66/198 | Loss: 1.3722084760665894\n",
            "Epoch 40/50 | Batch 67/198 | Loss: 1.5822664499282837\n",
            "Epoch 40/50 | Batch 68/198 | Loss: 1.2976328134536743\n",
            "Epoch 40/50 | Batch 69/198 | Loss: 1.6279962062835693\n",
            "Epoch 40/50 | Batch 70/198 | Loss: 1.750224232673645\n",
            "Epoch 40/50 | Batch 71/198 | Loss: 1.4146554470062256\n",
            "Epoch 40/50 | Batch 72/198 | Loss: 1.4174636602401733\n",
            "Epoch 40/50 | Batch 73/198 | Loss: 1.5421785116195679\n",
            "Epoch 40/50 | Batch 74/198 | Loss: 1.4271831512451172\n",
            "Epoch 40/50 | Batch 75/198 | Loss: 1.6638766527175903\n",
            "Epoch 40/50 | Batch 76/198 | Loss: 1.6393102407455444\n",
            "Epoch 40/50 | Batch 77/198 | Loss: 1.412212610244751\n",
            "Epoch 40/50 | Batch 78/198 | Loss: 1.465344786643982\n",
            "Epoch 40/50 | Batch 79/198 | Loss: 1.557626485824585\n",
            "Epoch 40/50 | Batch 80/198 | Loss: 1.3616175651550293\n",
            "Epoch 40/50 | Batch 81/198 | Loss: 1.3523153066635132\n",
            "Epoch 40/50 | Batch 82/198 | Loss: 1.369832992553711\n",
            "Epoch 40/50 | Batch 83/198 | Loss: 1.2393717765808105\n",
            "Epoch 40/50 | Batch 84/198 | Loss: 1.4008281230926514\n",
            "Epoch 40/50 | Batch 85/198 | Loss: 1.5509041547775269\n",
            "Epoch 40/50 | Batch 86/198 | Loss: 1.4383054971694946\n",
            "Epoch 40/50 | Batch 87/198 | Loss: 1.495240330696106\n",
            "Epoch 40/50 | Batch 88/198 | Loss: 1.5405795574188232\n",
            "Epoch 40/50 | Batch 89/198 | Loss: 1.3171327114105225\n",
            "Epoch 40/50 | Batch 90/198 | Loss: 1.4640860557556152\n",
            "Epoch 40/50 | Batch 91/198 | Loss: 1.261199951171875\n",
            "Epoch 40/50 | Batch 92/198 | Loss: 1.316909909248352\n",
            "Epoch 40/50 | Batch 93/198 | Loss: 1.390146255493164\n",
            "Epoch 40/50 | Batch 94/198 | Loss: 1.5443893671035767\n",
            "Epoch 40/50 | Batch 95/198 | Loss: 1.3989741802215576\n",
            "Epoch 40/50 | Batch 96/198 | Loss: 1.4186383485794067\n",
            "Epoch 40/50 | Batch 97/198 | Loss: 1.5216951370239258\n",
            "Epoch 40/50 | Batch 98/198 | Loss: 1.4330066442489624\n",
            "Epoch 40/50 | Batch 99/198 | Loss: 1.5076005458831787\n",
            "Epoch 40/50 | Batch 100/198 | Loss: 1.3632616996765137\n",
            "Epoch 40/50 | Batch 101/198 | Loss: 1.4640580415725708\n",
            "Epoch 40/50 | Batch 102/198 | Loss: 1.5658074617385864\n",
            "Epoch 40/50 | Batch 103/198 | Loss: 1.4825382232666016\n",
            "Epoch 40/50 | Batch 104/198 | Loss: 1.621139645576477\n",
            "Epoch 40/50 | Batch 105/198 | Loss: 1.1638727188110352\n",
            "Epoch 40/50 | Batch 106/198 | Loss: 1.2669755220413208\n",
            "Epoch 40/50 | Batch 107/198 | Loss: 1.4282548427581787\n",
            "Epoch 40/50 | Batch 108/198 | Loss: 1.5155969858169556\n",
            "Epoch 40/50 | Batch 109/198 | Loss: 1.4183417558670044\n",
            "Epoch 40/50 | Batch 110/198 | Loss: 1.305476427078247\n",
            "Epoch 40/50 | Batch 111/198 | Loss: 1.3158761262893677\n",
            "Epoch 40/50 | Batch 112/198 | Loss: 1.2795554399490356\n",
            "Epoch 40/50 | Batch 113/198 | Loss: 1.4750351905822754\n",
            "Epoch 40/50 | Batch 114/198 | Loss: 1.3377809524536133\n",
            "Epoch 40/50 | Batch 115/198 | Loss: 1.360216736793518\n",
            "Epoch 40/50 | Batch 116/198 | Loss: 1.4965711832046509\n",
            "Epoch 40/50 | Batch 117/198 | Loss: 1.4602371454238892\n",
            "Epoch 40/50 | Batch 118/198 | Loss: 1.1717909574508667\n",
            "Epoch 40/50 | Batch 119/198 | Loss: 1.5134977102279663\n",
            "Epoch 40/50 | Batch 120/198 | Loss: 1.3973156213760376\n",
            "Epoch 40/50 | Batch 121/198 | Loss: 1.3198071718215942\n",
            "Epoch 40/50 | Batch 122/198 | Loss: 1.3224849700927734\n",
            "Epoch 40/50 | Batch 123/198 | Loss: 1.3644261360168457\n",
            "Epoch 40/50 | Batch 124/198 | Loss: 1.2953451871871948\n",
            "Epoch 40/50 | Batch 125/198 | Loss: 1.4376120567321777\n",
            "Epoch 40/50 | Batch 126/198 | Loss: 1.3001480102539062\n",
            "Epoch 40/50 | Batch 127/198 | Loss: 1.4747958183288574\n",
            "Epoch 40/50 | Batch 128/198 | Loss: 1.6174676418304443\n",
            "Epoch 40/50 | Batch 129/198 | Loss: 1.5446327924728394\n",
            "Epoch 40/50 | Batch 130/198 | Loss: 1.5565237998962402\n",
            "Epoch 40/50 | Batch 131/198 | Loss: 1.449657917022705\n",
            "Epoch 40/50 | Batch 132/198 | Loss: 1.350395679473877\n",
            "Epoch 40/50 | Batch 133/198 | Loss: 1.4553039073944092\n",
            "Epoch 40/50 | Batch 134/198 | Loss: 1.0677217245101929\n",
            "Epoch 40/50 | Batch 135/198 | Loss: 1.5285834074020386\n",
            "Epoch 40/50 | Batch 136/198 | Loss: 1.4263383150100708\n",
            "Epoch 40/50 | Batch 137/198 | Loss: 1.4133299589157104\n",
            "Epoch 40/50 | Batch 138/198 | Loss: 1.347676157951355\n",
            "Epoch 40/50 | Batch 139/198 | Loss: 1.4186259508132935\n",
            "Epoch 40/50 | Batch 140/198 | Loss: 1.3244247436523438\n",
            "Epoch 40/50 | Batch 141/198 | Loss: 1.5129318237304688\n",
            "Epoch 40/50 | Batch 142/198 | Loss: 1.518960952758789\n",
            "Epoch 40/50 | Batch 143/198 | Loss: 1.6185616254806519\n",
            "Epoch 40/50 | Batch 144/198 | Loss: 1.4772334098815918\n",
            "Epoch 40/50 | Batch 145/198 | Loss: 1.28756844997406\n",
            "Epoch 40/50 | Batch 146/198 | Loss: 1.7385125160217285\n",
            "Epoch 40/50 | Batch 147/198 | Loss: 1.2885655164718628\n",
            "Epoch 40/50 | Batch 148/198 | Loss: 1.5114176273345947\n",
            "Epoch 40/50 | Batch 149/198 | Loss: 1.4512978792190552\n",
            "Epoch 40/50 | Batch 150/198 | Loss: 1.3281234502792358\n",
            "Epoch 40/50 | Batch 151/198 | Loss: 1.2598973512649536\n",
            "Epoch 40/50 | Batch 152/198 | Loss: 1.5674020051956177\n",
            "Epoch 40/50 | Batch 153/198 | Loss: 1.3941761255264282\n",
            "Epoch 40/50 | Batch 154/198 | Loss: 1.515195608139038\n",
            "Epoch 40/50 | Batch 155/198 | Loss: 1.4859286546707153\n",
            "Epoch 40/50 | Batch 156/198 | Loss: 1.3945509195327759\n",
            "Epoch 40/50 | Batch 157/198 | Loss: 1.6322753429412842\n",
            "Epoch 40/50 | Batch 158/198 | Loss: 1.517501950263977\n",
            "Epoch 40/50 | Batch 159/198 | Loss: 1.4227665662765503\n",
            "Epoch 40/50 | Batch 160/198 | Loss: 1.4339326620101929\n",
            "Epoch 40/50 | Batch 161/198 | Loss: 1.399737000465393\n",
            "Epoch 40/50 | Batch 162/198 | Loss: 1.3511064052581787\n",
            "Epoch 40/50 | Batch 163/198 | Loss: 1.3101853132247925\n",
            "Epoch 40/50 | Batch 164/198 | Loss: 1.5693573951721191\n",
            "Epoch 40/50 | Batch 165/198 | Loss: 1.5474002361297607\n",
            "Epoch 40/50 | Batch 166/198 | Loss: 1.4986218214035034\n",
            "Epoch 40/50 | Batch 167/198 | Loss: 1.4476697444915771\n",
            "Epoch 40/50 | Batch 168/198 | Loss: 1.4213021993637085\n",
            "Epoch 40/50 | Batch 169/198 | Loss: 1.3332031965255737\n",
            "Epoch 40/50 | Batch 170/198 | Loss: 1.3712780475616455\n",
            "Epoch 40/50 | Batch 171/198 | Loss: 1.5902379751205444\n",
            "Epoch 40/50 | Batch 172/198 | Loss: 1.2560545206069946\n",
            "Epoch 40/50 | Batch 173/198 | Loss: 1.4253264665603638\n",
            "Epoch 40/50 | Batch 174/198 | Loss: 1.5120809078216553\n",
            "Epoch 40/50 | Batch 175/198 | Loss: 1.5230649709701538\n",
            "Epoch 40/50 | Batch 176/198 | Loss: 1.6363402605056763\n",
            "Epoch 40/50 | Batch 177/198 | Loss: 1.5614938735961914\n",
            "Epoch 40/50 | Batch 178/198 | Loss: 1.2948393821716309\n",
            "Epoch 40/50 | Batch 179/198 | Loss: 1.587741732597351\n",
            "Epoch 40/50 | Batch 180/198 | Loss: 1.515128493309021\n",
            "Epoch 40/50 | Batch 181/198 | Loss: 1.5132695436477661\n",
            "Epoch 40/50 | Batch 182/198 | Loss: 1.4561214447021484\n",
            "Epoch 40/50 | Batch 183/198 | Loss: 1.37860107421875\n",
            "Epoch 40/50 | Batch 184/198 | Loss: 1.4314507246017456\n",
            "Epoch 40/50 | Batch 185/198 | Loss: 1.4079928398132324\n",
            "Epoch 40/50 | Batch 186/198 | Loss: 1.3811063766479492\n",
            "Epoch 40/50 | Batch 187/198 | Loss: 1.299986720085144\n",
            "Epoch 40/50 | Batch 188/198 | Loss: 1.4860551357269287\n",
            "Epoch 40/50 | Batch 189/198 | Loss: 1.4246565103530884\n",
            "Epoch 40/50 | Batch 190/198 | Loss: 1.4605975151062012\n",
            "Epoch 40/50 | Batch 191/198 | Loss: 1.475677728652954\n",
            "Epoch 40/50 | Batch 192/198 | Loss: 1.195339322090149\n",
            "Epoch 40/50 | Batch 193/198 | Loss: 1.3847548961639404\n",
            "Epoch 40/50 | Batch 194/198 | Loss: 1.3863636255264282\n",
            "Epoch 40/50 | Batch 195/198 | Loss: 1.3355112075805664\n",
            "Epoch 40/50 | Batch 196/198 | Loss: 1.2993521690368652\n",
            "Epoch 40/50 | Batch 197/198 | Loss: 1.637787938117981\n",
            "Epoch 40/50 | Batch 198/198 | Loss: 1.5401853322982788\n",
            "Epoch 40/50 | Average Loss: 1.4345482858744534\n",
            "Epoch 41/50 | Batch 1/198 | Loss: 1.3701082468032837\n",
            "Epoch 41/50 | Batch 2/198 | Loss: 1.4994136095046997\n",
            "Epoch 41/50 | Batch 3/198 | Loss: 1.2432210445404053\n",
            "Epoch 41/50 | Batch 4/198 | Loss: 1.4751460552215576\n",
            "Epoch 41/50 | Batch 5/198 | Loss: 1.3162405490875244\n",
            "Epoch 41/50 | Batch 6/198 | Loss: 1.3856512308120728\n",
            "Epoch 41/50 | Batch 7/198 | Loss: 1.595636010169983\n",
            "Epoch 41/50 | Batch 8/198 | Loss: 1.4364334344863892\n",
            "Epoch 41/50 | Batch 9/198 | Loss: 1.393359899520874\n",
            "Epoch 41/50 | Batch 10/198 | Loss: 1.315887212753296\n",
            "Epoch 41/50 | Batch 11/198 | Loss: 1.464864730834961\n",
            "Epoch 41/50 | Batch 12/198 | Loss: 1.4561855792999268\n",
            "Epoch 41/50 | Batch 13/198 | Loss: 1.5388399362564087\n",
            "Epoch 41/50 | Batch 14/198 | Loss: 1.3917466402053833\n",
            "Epoch 41/50 | Batch 15/198 | Loss: 1.501829743385315\n",
            "Epoch 41/50 | Batch 16/198 | Loss: 1.5292495489120483\n",
            "Epoch 41/50 | Batch 17/198 | Loss: 1.4807982444763184\n",
            "Epoch 41/50 | Batch 18/198 | Loss: 1.3206225633621216\n",
            "Epoch 41/50 | Batch 19/198 | Loss: 1.3950837850570679\n",
            "Epoch 41/50 | Batch 20/198 | Loss: 1.523579478263855\n",
            "Epoch 41/50 | Batch 21/198 | Loss: 1.5182663202285767\n",
            "Epoch 41/50 | Batch 22/198 | Loss: 1.3580464124679565\n",
            "Epoch 41/50 | Batch 23/198 | Loss: 1.45205819606781\n",
            "Epoch 41/50 | Batch 24/198 | Loss: 1.4014791250228882\n",
            "Epoch 41/50 | Batch 25/198 | Loss: 1.2223424911499023\n",
            "Epoch 41/50 | Batch 26/198 | Loss: 1.2652356624603271\n",
            "Epoch 41/50 | Batch 27/198 | Loss: 1.4196056127548218\n",
            "Epoch 41/50 | Batch 28/198 | Loss: 1.4497528076171875\n",
            "Epoch 41/50 | Batch 29/198 | Loss: 1.4183244705200195\n",
            "Epoch 41/50 | Batch 30/198 | Loss: 1.3494549989700317\n",
            "Epoch 41/50 | Batch 31/198 | Loss: 1.5215768814086914\n",
            "Epoch 41/50 | Batch 32/198 | Loss: 1.6860440969467163\n",
            "Epoch 41/50 | Batch 33/198 | Loss: 1.2447429895401\n",
            "Epoch 41/50 | Batch 34/198 | Loss: 1.4940418004989624\n",
            "Epoch 41/50 | Batch 35/198 | Loss: 1.3481539487838745\n",
            "Epoch 41/50 | Batch 36/198 | Loss: 1.4030609130859375\n",
            "Epoch 41/50 | Batch 37/198 | Loss: 1.2559020519256592\n",
            "Epoch 41/50 | Batch 38/198 | Loss: 1.4767524003982544\n",
            "Epoch 41/50 | Batch 39/198 | Loss: 1.4992976188659668\n",
            "Epoch 41/50 | Batch 40/198 | Loss: 1.517074465751648\n",
            "Epoch 41/50 | Batch 41/198 | Loss: 1.468939185142517\n",
            "Epoch 41/50 | Batch 42/198 | Loss: 1.5562142133712769\n",
            "Epoch 41/50 | Batch 43/198 | Loss: 1.4324064254760742\n",
            "Epoch 41/50 | Batch 44/198 | Loss: 1.5413647890090942\n",
            "Epoch 41/50 | Batch 45/198 | Loss: 1.4851057529449463\n",
            "Epoch 41/50 | Batch 46/198 | Loss: 1.247127890586853\n",
            "Epoch 41/50 | Batch 47/198 | Loss: 1.3722096681594849\n",
            "Epoch 41/50 | Batch 48/198 | Loss: 1.5721328258514404\n",
            "Epoch 41/50 | Batch 49/198 | Loss: 1.3723104000091553\n",
            "Epoch 41/50 | Batch 50/198 | Loss: 1.5113149881362915\n",
            "Epoch 41/50 | Batch 51/198 | Loss: 1.6513220071792603\n",
            "Epoch 41/50 | Batch 52/198 | Loss: 1.3939189910888672\n",
            "Epoch 41/50 | Batch 53/198 | Loss: 1.3435689210891724\n",
            "Epoch 41/50 | Batch 54/198 | Loss: 1.4302328824996948\n",
            "Epoch 41/50 | Batch 55/198 | Loss: 1.4742451906204224\n",
            "Epoch 41/50 | Batch 56/198 | Loss: 1.4998568296432495\n",
            "Epoch 41/50 | Batch 57/198 | Loss: 1.5313968658447266\n",
            "Epoch 41/50 | Batch 58/198 | Loss: 1.4345210790634155\n",
            "Epoch 41/50 | Batch 59/198 | Loss: 1.3973212242126465\n",
            "Epoch 41/50 | Batch 60/198 | Loss: 1.4695345163345337\n",
            "Epoch 41/50 | Batch 61/198 | Loss: 1.4548320770263672\n",
            "Epoch 41/50 | Batch 62/198 | Loss: 1.4778748750686646\n",
            "Epoch 41/50 | Batch 63/198 | Loss: 1.5409456491470337\n",
            "Epoch 41/50 | Batch 64/198 | Loss: 1.3270175457000732\n",
            "Epoch 41/50 | Batch 65/198 | Loss: 1.3708306550979614\n",
            "Epoch 41/50 | Batch 66/198 | Loss: 1.4046212434768677\n",
            "Epoch 41/50 | Batch 67/198 | Loss: 1.4931528568267822\n",
            "Epoch 41/50 | Batch 68/198 | Loss: 1.3565499782562256\n",
            "Epoch 41/50 | Batch 69/198 | Loss: 1.479933500289917\n",
            "Epoch 41/50 | Batch 70/198 | Loss: 1.5031827688217163\n",
            "Epoch 41/50 | Batch 71/198 | Loss: 1.240026831626892\n",
            "Epoch 41/50 | Batch 72/198 | Loss: 1.5645313262939453\n",
            "Epoch 41/50 | Batch 73/198 | Loss: 1.2312560081481934\n",
            "Epoch 41/50 | Batch 74/198 | Loss: 1.4729458093643188\n",
            "Epoch 41/50 | Batch 75/198 | Loss: 1.4843988418579102\n",
            "Epoch 41/50 | Batch 76/198 | Loss: 1.553804636001587\n",
            "Epoch 41/50 | Batch 77/198 | Loss: 1.5038155317306519\n",
            "Epoch 41/50 | Batch 78/198 | Loss: 1.591213345527649\n",
            "Epoch 41/50 | Batch 79/198 | Loss: 1.4835455417633057\n",
            "Epoch 41/50 | Batch 80/198 | Loss: 1.3685823678970337\n",
            "Epoch 41/50 | Batch 81/198 | Loss: 1.3629229068756104\n",
            "Epoch 41/50 | Batch 82/198 | Loss: 1.5654040575027466\n",
            "Epoch 41/50 | Batch 83/198 | Loss: 1.311004638671875\n",
            "Epoch 41/50 | Batch 84/198 | Loss: 1.512411117553711\n",
            "Epoch 41/50 | Batch 85/198 | Loss: 1.4187703132629395\n",
            "Epoch 41/50 | Batch 86/198 | Loss: 1.391864538192749\n",
            "Epoch 41/50 | Batch 87/198 | Loss: 1.3878639936447144\n",
            "Epoch 41/50 | Batch 88/198 | Loss: 1.3511340618133545\n",
            "Epoch 41/50 | Batch 89/198 | Loss: 1.3010027408599854\n",
            "Epoch 41/50 | Batch 90/198 | Loss: 1.5557911396026611\n",
            "Epoch 41/50 | Batch 91/198 | Loss: 1.4547535181045532\n",
            "Epoch 41/50 | Batch 92/198 | Loss: 1.3571274280548096\n",
            "Epoch 41/50 | Batch 93/198 | Loss: 1.6004174947738647\n",
            "Epoch 41/50 | Batch 94/198 | Loss: 1.6038885116577148\n",
            "Epoch 41/50 | Batch 95/198 | Loss: 1.318185567855835\n",
            "Epoch 41/50 | Batch 96/198 | Loss: 1.394248366355896\n",
            "Epoch 41/50 | Batch 97/198 | Loss: 1.482905626296997\n",
            "Epoch 41/50 | Batch 98/198 | Loss: 1.398427128791809\n",
            "Epoch 41/50 | Batch 99/198 | Loss: 1.4343576431274414\n",
            "Epoch 41/50 | Batch 100/198 | Loss: 1.4438838958740234\n",
            "Epoch 41/50 | Batch 101/198 | Loss: 1.3630237579345703\n",
            "Epoch 41/50 | Batch 102/198 | Loss: 1.3448745012283325\n",
            "Epoch 41/50 | Batch 103/198 | Loss: 1.5612592697143555\n",
            "Epoch 41/50 | Batch 104/198 | Loss: 1.3038212060928345\n",
            "Epoch 41/50 | Batch 105/198 | Loss: 1.4806885719299316\n",
            "Epoch 41/50 | Batch 106/198 | Loss: 1.380589246749878\n",
            "Epoch 41/50 | Batch 107/198 | Loss: 1.4189095497131348\n",
            "Epoch 41/50 | Batch 108/198 | Loss: 1.6986850500106812\n",
            "Epoch 41/50 | Batch 109/198 | Loss: 1.3108644485473633\n",
            "Epoch 41/50 | Batch 110/198 | Loss: 1.2595018148422241\n",
            "Epoch 41/50 | Batch 111/198 | Loss: 1.3131375312805176\n",
            "Epoch 41/50 | Batch 112/198 | Loss: 1.2683924436569214\n",
            "Epoch 41/50 | Batch 113/198 | Loss: 1.595564842224121\n",
            "Epoch 41/50 | Batch 114/198 | Loss: 1.4156765937805176\n",
            "Epoch 41/50 | Batch 115/198 | Loss: 1.4664584398269653\n",
            "Epoch 41/50 | Batch 116/198 | Loss: 1.14453125\n",
            "Epoch 41/50 | Batch 117/198 | Loss: 1.4689542055130005\n",
            "Epoch 41/50 | Batch 118/198 | Loss: 1.5837225914001465\n",
            "Epoch 41/50 | Batch 119/198 | Loss: 1.3460935354232788\n",
            "Epoch 41/50 | Batch 120/198 | Loss: 1.6386914253234863\n",
            "Epoch 41/50 | Batch 121/198 | Loss: 1.3921054601669312\n",
            "Epoch 41/50 | Batch 122/198 | Loss: 1.1503973007202148\n",
            "Epoch 41/50 | Batch 123/198 | Loss: 1.5686578750610352\n",
            "Epoch 41/50 | Batch 124/198 | Loss: 1.5823179483413696\n",
            "Epoch 41/50 | Batch 125/198 | Loss: 1.4337060451507568\n",
            "Epoch 41/50 | Batch 126/198 | Loss: 1.2512600421905518\n",
            "Epoch 41/50 | Batch 127/198 | Loss: 1.400789737701416\n",
            "Epoch 41/50 | Batch 128/198 | Loss: 1.3778496980667114\n",
            "Epoch 41/50 | Batch 129/198 | Loss: 1.402493953704834\n",
            "Epoch 41/50 | Batch 130/198 | Loss: 1.3104512691497803\n",
            "Epoch 41/50 | Batch 131/198 | Loss: 1.4388891458511353\n",
            "Epoch 41/50 | Batch 132/198 | Loss: 1.322555422782898\n",
            "Epoch 41/50 | Batch 133/198 | Loss: 1.426357626914978\n",
            "Epoch 41/50 | Batch 134/198 | Loss: 1.4917553663253784\n",
            "Epoch 41/50 | Batch 135/198 | Loss: 1.5653917789459229\n",
            "Epoch 41/50 | Batch 136/198 | Loss: 1.5452312231063843\n",
            "Epoch 41/50 | Batch 137/198 | Loss: 1.3751024007797241\n",
            "Epoch 41/50 | Batch 138/198 | Loss: 1.2733628749847412\n",
            "Epoch 41/50 | Batch 139/198 | Loss: 1.5330537557601929\n",
            "Epoch 41/50 | Batch 140/198 | Loss: 1.3828715085983276\n",
            "Epoch 41/50 | Batch 141/198 | Loss: 1.4488312005996704\n",
            "Epoch 41/50 | Batch 142/198 | Loss: 1.4036303758621216\n",
            "Epoch 41/50 | Batch 143/198 | Loss: 1.53071928024292\n",
            "Epoch 41/50 | Batch 144/198 | Loss: 1.3872941732406616\n",
            "Epoch 41/50 | Batch 145/198 | Loss: 1.396008849143982\n",
            "Epoch 41/50 | Batch 146/198 | Loss: 1.3172062635421753\n",
            "Epoch 41/50 | Batch 147/198 | Loss: 1.5537736415863037\n",
            "Epoch 41/50 | Batch 148/198 | Loss: 1.383427381515503\n",
            "Epoch 41/50 | Batch 149/198 | Loss: 1.6134461164474487\n",
            "Epoch 41/50 | Batch 150/198 | Loss: 1.569508671760559\n",
            "Epoch 41/50 | Batch 151/198 | Loss: 1.5246684551239014\n",
            "Epoch 41/50 | Batch 152/198 | Loss: 1.2100106477737427\n",
            "Epoch 41/50 | Batch 153/198 | Loss: 1.2989563941955566\n",
            "Epoch 41/50 | Batch 154/198 | Loss: 1.3419280052185059\n",
            "Epoch 41/50 | Batch 155/198 | Loss: 1.5211695432662964\n",
            "Epoch 41/50 | Batch 156/198 | Loss: 1.476710557937622\n",
            "Epoch 41/50 | Batch 157/198 | Loss: 1.367952585220337\n",
            "Epoch 41/50 | Batch 158/198 | Loss: 1.3188472986221313\n",
            "Epoch 41/50 | Batch 159/198 | Loss: 1.21324622631073\n",
            "Epoch 41/50 | Batch 160/198 | Loss: 1.497531771659851\n",
            "Epoch 41/50 | Batch 161/198 | Loss: 1.183377981185913\n",
            "Epoch 41/50 | Batch 162/198 | Loss: 1.265285849571228\n",
            "Epoch 41/50 | Batch 163/198 | Loss: 1.3775269985198975\n",
            "Epoch 41/50 | Batch 164/198 | Loss: 1.4379143714904785\n",
            "Epoch 41/50 | Batch 165/198 | Loss: 1.5740172863006592\n",
            "Epoch 41/50 | Batch 166/198 | Loss: 1.1375174522399902\n",
            "Epoch 41/50 | Batch 167/198 | Loss: 1.5327004194259644\n",
            "Epoch 41/50 | Batch 168/198 | Loss: 1.3513387441635132\n",
            "Epoch 41/50 | Batch 169/198 | Loss: 1.2840877771377563\n",
            "Epoch 41/50 | Batch 170/198 | Loss: 1.5597009658813477\n",
            "Epoch 41/50 | Batch 171/198 | Loss: 1.1844725608825684\n",
            "Epoch 41/50 | Batch 172/198 | Loss: 1.4105010032653809\n",
            "Epoch 41/50 | Batch 173/198 | Loss: 1.3922613859176636\n",
            "Epoch 41/50 | Batch 174/198 | Loss: 1.4401687383651733\n",
            "Epoch 41/50 | Batch 175/198 | Loss: 1.5547125339508057\n",
            "Epoch 41/50 | Batch 176/198 | Loss: 1.5516600608825684\n",
            "Epoch 41/50 | Batch 177/198 | Loss: 1.515847086906433\n",
            "Epoch 41/50 | Batch 178/198 | Loss: 1.2845731973648071\n",
            "Epoch 41/50 | Batch 179/198 | Loss: 1.3851741552352905\n",
            "Epoch 41/50 | Batch 180/198 | Loss: 1.3619049787521362\n",
            "Epoch 41/50 | Batch 181/198 | Loss: 1.710891842842102\n",
            "Epoch 41/50 | Batch 182/198 | Loss: 1.5236166715621948\n",
            "Epoch 41/50 | Batch 183/198 | Loss: 1.3484010696411133\n",
            "Epoch 41/50 | Batch 184/198 | Loss: 1.2949771881103516\n",
            "Epoch 41/50 | Batch 185/198 | Loss: 1.2833468914031982\n",
            "Epoch 41/50 | Batch 186/198 | Loss: 1.2495932579040527\n",
            "Epoch 41/50 | Batch 187/198 | Loss: 1.6212395429611206\n",
            "Epoch 41/50 | Batch 188/198 | Loss: 1.5194497108459473\n",
            "Epoch 41/50 | Batch 189/198 | Loss: 1.4417967796325684\n",
            "Epoch 41/50 | Batch 190/198 | Loss: 1.6713632345199585\n",
            "Epoch 41/50 | Batch 191/198 | Loss: 1.3853178024291992\n",
            "Epoch 41/50 | Batch 192/198 | Loss: 1.191752552986145\n",
            "Epoch 41/50 | Batch 193/198 | Loss: 1.5416408777236938\n",
            "Epoch 41/50 | Batch 194/198 | Loss: 1.302525281906128\n",
            "Epoch 41/50 | Batch 195/198 | Loss: 1.2762553691864014\n",
            "Epoch 41/50 | Batch 196/198 | Loss: 1.5980031490325928\n",
            "Epoch 41/50 | Batch 197/198 | Loss: 1.3680466413497925\n",
            "Epoch 41/50 | Batch 198/198 | Loss: 1.2191972732543945\n",
            "Epoch 41/50 | Average Loss: 1.4240638806362345\n",
            "Epoch 42/50 | Batch 1/198 | Loss: 1.4766054153442383\n",
            "Epoch 42/50 | Batch 2/198 | Loss: 1.2744916677474976\n",
            "Epoch 42/50 | Batch 3/198 | Loss: 1.4508057832717896\n",
            "Epoch 42/50 | Batch 4/198 | Loss: 1.2770626544952393\n",
            "Epoch 42/50 | Batch 5/198 | Loss: 1.3424054384231567\n",
            "Epoch 42/50 | Batch 6/198 | Loss: 1.3690800666809082\n",
            "Epoch 42/50 | Batch 7/198 | Loss: 1.4490350484848022\n",
            "Epoch 42/50 | Batch 8/198 | Loss: 1.3675347566604614\n",
            "Epoch 42/50 | Batch 9/198 | Loss: 1.4085034132003784\n",
            "Epoch 42/50 | Batch 10/198 | Loss: 1.5957858562469482\n",
            "Epoch 42/50 | Batch 11/198 | Loss: 1.372097134590149\n",
            "Epoch 42/50 | Batch 12/198 | Loss: 1.3579329252243042\n",
            "Epoch 42/50 | Batch 13/198 | Loss: 1.4438964128494263\n",
            "Epoch 42/50 | Batch 14/198 | Loss: 1.3979507684707642\n",
            "Epoch 42/50 | Batch 15/198 | Loss: 1.4393099546432495\n",
            "Epoch 42/50 | Batch 16/198 | Loss: 1.4131988286972046\n",
            "Epoch 42/50 | Batch 17/198 | Loss: 1.483279824256897\n",
            "Epoch 42/50 | Batch 18/198 | Loss: 1.4162187576293945\n",
            "Epoch 42/50 | Batch 19/198 | Loss: 1.3257497549057007\n",
            "Epoch 42/50 | Batch 20/198 | Loss: 1.3891575336456299\n",
            "Epoch 42/50 | Batch 21/198 | Loss: 1.387133240699768\n",
            "Epoch 42/50 | Batch 22/198 | Loss: 1.3016842603683472\n",
            "Epoch 42/50 | Batch 23/198 | Loss: 1.2651761770248413\n",
            "Epoch 42/50 | Batch 24/198 | Loss: 1.6038771867752075\n",
            "Epoch 42/50 | Batch 25/198 | Loss: 1.3607158660888672\n",
            "Epoch 42/50 | Batch 26/198 | Loss: 1.384589672088623\n",
            "Epoch 42/50 | Batch 27/198 | Loss: 1.2470290660858154\n",
            "Epoch 42/50 | Batch 28/198 | Loss: 1.4525015354156494\n",
            "Epoch 42/50 | Batch 29/198 | Loss: 1.6141904592514038\n",
            "Epoch 42/50 | Batch 30/198 | Loss: 1.5396541357040405\n",
            "Epoch 42/50 | Batch 31/198 | Loss: 1.2879908084869385\n",
            "Epoch 42/50 | Batch 32/198 | Loss: 1.5788180828094482\n",
            "Epoch 42/50 | Batch 33/198 | Loss: 1.428947925567627\n",
            "Epoch 42/50 | Batch 34/198 | Loss: 1.4756814241409302\n",
            "Epoch 42/50 | Batch 35/198 | Loss: 1.4259201288223267\n",
            "Epoch 42/50 | Batch 36/198 | Loss: 1.284239411354065\n",
            "Epoch 42/50 | Batch 37/198 | Loss: 1.3886263370513916\n",
            "Epoch 42/50 | Batch 38/198 | Loss: 1.517034649848938\n",
            "Epoch 42/50 | Batch 39/198 | Loss: 1.4449018239974976\n",
            "Epoch 42/50 | Batch 40/198 | Loss: 1.403381586074829\n",
            "Epoch 42/50 | Batch 41/198 | Loss: 1.4623010158538818\n",
            "Epoch 42/50 | Batch 42/198 | Loss: 1.4087921380996704\n",
            "Epoch 42/50 | Batch 43/198 | Loss: 1.3591058254241943\n",
            "Epoch 42/50 | Batch 44/198 | Loss: 1.375812292098999\n",
            "Epoch 42/50 | Batch 45/198 | Loss: 1.5044931173324585\n",
            "Epoch 42/50 | Batch 46/198 | Loss: 1.4474422931671143\n",
            "Epoch 42/50 | Batch 47/198 | Loss: 1.5408577919006348\n",
            "Epoch 42/50 | Batch 48/198 | Loss: 1.3319066762924194\n",
            "Epoch 42/50 | Batch 49/198 | Loss: 1.4453396797180176\n",
            "Epoch 42/50 | Batch 50/198 | Loss: 1.4879395961761475\n",
            "Epoch 42/50 | Batch 51/198 | Loss: 1.166904330253601\n",
            "Epoch 42/50 | Batch 52/198 | Loss: 1.3910291194915771\n",
            "Epoch 42/50 | Batch 53/198 | Loss: 1.4306652545928955\n",
            "Epoch 42/50 | Batch 54/198 | Loss: 1.432961106300354\n",
            "Epoch 42/50 | Batch 55/198 | Loss: 1.3739162683486938\n",
            "Epoch 42/50 | Batch 56/198 | Loss: 1.2486026287078857\n",
            "Epoch 42/50 | Batch 57/198 | Loss: 1.5638238191604614\n",
            "Epoch 42/50 | Batch 58/198 | Loss: 1.6463881731033325\n",
            "Epoch 42/50 | Batch 59/198 | Loss: 1.494076132774353\n",
            "Epoch 42/50 | Batch 60/198 | Loss: 1.4396008253097534\n",
            "Epoch 42/50 | Batch 61/198 | Loss: 1.4627599716186523\n",
            "Epoch 42/50 | Batch 62/198 | Loss: 1.3094767332077026\n",
            "Epoch 42/50 | Batch 63/198 | Loss: 1.3789561986923218\n",
            "Epoch 42/50 | Batch 64/198 | Loss: 1.4387242794036865\n",
            "Epoch 42/50 | Batch 65/198 | Loss: 1.2938917875289917\n",
            "Epoch 42/50 | Batch 66/198 | Loss: 1.2823129892349243\n",
            "Epoch 42/50 | Batch 67/198 | Loss: 1.3318490982055664\n",
            "Epoch 42/50 | Batch 68/198 | Loss: 1.1829832792282104\n",
            "Epoch 42/50 | Batch 69/198 | Loss: 1.4309436082839966\n",
            "Epoch 42/50 | Batch 70/198 | Loss: 1.3827269077301025\n",
            "Epoch 42/50 | Batch 71/198 | Loss: 1.5986642837524414\n",
            "Epoch 42/50 | Batch 72/198 | Loss: 1.2948287725448608\n",
            "Epoch 42/50 | Batch 73/198 | Loss: 1.7516371011734009\n",
            "Epoch 42/50 | Batch 74/198 | Loss: 1.5819512605667114\n",
            "Epoch 42/50 | Batch 75/198 | Loss: 1.5889700651168823\n",
            "Epoch 42/50 | Batch 76/198 | Loss: 1.533060908317566\n",
            "Epoch 42/50 | Batch 77/198 | Loss: 1.4880945682525635\n",
            "Epoch 42/50 | Batch 78/198 | Loss: 1.5676764249801636\n",
            "Epoch 42/50 | Batch 79/198 | Loss: 1.4382590055465698\n",
            "Epoch 42/50 | Batch 80/198 | Loss: 1.4288537502288818\n",
            "Epoch 42/50 | Batch 81/198 | Loss: 1.3810230493545532\n",
            "Epoch 42/50 | Batch 82/198 | Loss: 1.4521334171295166\n",
            "Epoch 42/50 | Batch 83/198 | Loss: 1.7138845920562744\n",
            "Epoch 42/50 | Batch 84/198 | Loss: 1.2060929536819458\n",
            "Epoch 42/50 | Batch 85/198 | Loss: 1.371335506439209\n",
            "Epoch 42/50 | Batch 86/198 | Loss: 1.4905956983566284\n",
            "Epoch 42/50 | Batch 87/198 | Loss: 1.3035967350006104\n",
            "Epoch 42/50 | Batch 88/198 | Loss: 1.330687403678894\n",
            "Epoch 42/50 | Batch 89/198 | Loss: 1.3886839151382446\n",
            "Epoch 42/50 | Batch 90/198 | Loss: 1.1909316778182983\n",
            "Epoch 42/50 | Batch 91/198 | Loss: 1.3941015005111694\n",
            "Epoch 42/50 | Batch 92/198 | Loss: 1.4533389806747437\n",
            "Epoch 42/50 | Batch 93/198 | Loss: 1.369361400604248\n",
            "Epoch 42/50 | Batch 94/198 | Loss: 1.628758192062378\n",
            "Epoch 42/50 | Batch 95/198 | Loss: 1.4460780620574951\n",
            "Epoch 42/50 | Batch 96/198 | Loss: 1.410652756690979\n",
            "Epoch 42/50 | Batch 97/198 | Loss: 1.6039183139801025\n",
            "Epoch 42/50 | Batch 98/198 | Loss: 1.4559886455535889\n",
            "Epoch 42/50 | Batch 99/198 | Loss: 1.500981330871582\n",
            "Epoch 42/50 | Batch 100/198 | Loss: 1.4645874500274658\n",
            "Epoch 42/50 | Batch 101/198 | Loss: 1.2498699426651\n",
            "Epoch 42/50 | Batch 102/198 | Loss: 1.4122161865234375\n",
            "Epoch 42/50 | Batch 103/198 | Loss: 1.3593791723251343\n",
            "Epoch 42/50 | Batch 104/198 | Loss: 1.310996413230896\n",
            "Epoch 42/50 | Batch 105/198 | Loss: 1.4362452030181885\n",
            "Epoch 42/50 | Batch 106/198 | Loss: 1.2817071676254272\n",
            "Epoch 42/50 | Batch 107/198 | Loss: 1.4810339212417603\n",
            "Epoch 42/50 | Batch 108/198 | Loss: 1.4748427867889404\n",
            "Epoch 42/50 | Batch 109/198 | Loss: 1.4086731672286987\n",
            "Epoch 42/50 | Batch 110/198 | Loss: 1.5121817588806152\n",
            "Epoch 42/50 | Batch 111/198 | Loss: 1.522348403930664\n",
            "Epoch 42/50 | Batch 112/198 | Loss: 1.39676833152771\n",
            "Epoch 42/50 | Batch 113/198 | Loss: 1.4558361768722534\n",
            "Epoch 42/50 | Batch 114/198 | Loss: 1.4968750476837158\n",
            "Epoch 42/50 | Batch 115/198 | Loss: 1.6216470003128052\n",
            "Epoch 42/50 | Batch 116/198 | Loss: 1.386743187904358\n",
            "Epoch 42/50 | Batch 117/198 | Loss: 1.5983184576034546\n",
            "Epoch 42/50 | Batch 118/198 | Loss: 1.554089069366455\n",
            "Epoch 42/50 | Batch 119/198 | Loss: 1.405333161354065\n",
            "Epoch 42/50 | Batch 120/198 | Loss: 1.2137911319732666\n",
            "Epoch 42/50 | Batch 121/198 | Loss: 1.3329211473464966\n",
            "Epoch 42/50 | Batch 122/198 | Loss: 1.2855496406555176\n",
            "Epoch 42/50 | Batch 123/198 | Loss: 1.3461287021636963\n",
            "Epoch 42/50 | Batch 124/198 | Loss: 1.3419989347457886\n",
            "Epoch 42/50 | Batch 125/198 | Loss: 1.424153447151184\n",
            "Epoch 42/50 | Batch 126/198 | Loss: 1.3645663261413574\n",
            "Epoch 42/50 | Batch 127/198 | Loss: 1.3612242937088013\n",
            "Epoch 42/50 | Batch 128/198 | Loss: 1.405070185661316\n",
            "Epoch 42/50 | Batch 129/198 | Loss: 1.5258392095565796\n",
            "Epoch 42/50 | Batch 130/198 | Loss: 1.413211464881897\n",
            "Epoch 42/50 | Batch 131/198 | Loss: 1.3398754596710205\n",
            "Epoch 42/50 | Batch 132/198 | Loss: 1.2316949367523193\n",
            "Epoch 42/50 | Batch 133/198 | Loss: 1.4569438695907593\n",
            "Epoch 42/50 | Batch 134/198 | Loss: 1.4291452169418335\n",
            "Epoch 42/50 | Batch 135/198 | Loss: 1.3681362867355347\n",
            "Epoch 42/50 | Batch 136/198 | Loss: 1.5051339864730835\n",
            "Epoch 42/50 | Batch 137/198 | Loss: 1.6020711660385132\n",
            "Epoch 42/50 | Batch 138/198 | Loss: 1.2646111249923706\n",
            "Epoch 42/50 | Batch 139/198 | Loss: 1.3476670980453491\n",
            "Epoch 42/50 | Batch 140/198 | Loss: 1.13741135597229\n",
            "Epoch 42/50 | Batch 141/198 | Loss: 1.4304229021072388\n",
            "Epoch 42/50 | Batch 142/198 | Loss: 1.4710750579833984\n",
            "Epoch 42/50 | Batch 143/198 | Loss: 1.4286425113677979\n",
            "Epoch 42/50 | Batch 144/198 | Loss: 1.2855377197265625\n",
            "Epoch 42/50 | Batch 145/198 | Loss: 1.4507806301116943\n",
            "Epoch 42/50 | Batch 146/198 | Loss: 1.4297112226486206\n",
            "Epoch 42/50 | Batch 147/198 | Loss: 1.3521037101745605\n",
            "Epoch 42/50 | Batch 148/198 | Loss: 1.3919538259506226\n",
            "Epoch 42/50 | Batch 149/198 | Loss: 1.3475703001022339\n",
            "Epoch 42/50 | Batch 150/198 | Loss: 1.3677701950073242\n",
            "Epoch 42/50 | Batch 151/198 | Loss: 1.4871832132339478\n",
            "Epoch 42/50 | Batch 152/198 | Loss: 1.5433788299560547\n",
            "Epoch 42/50 | Batch 153/198 | Loss: 1.4126118421554565\n",
            "Epoch 42/50 | Batch 154/198 | Loss: 1.2066140174865723\n",
            "Epoch 42/50 | Batch 155/198 | Loss: 1.3997797966003418\n",
            "Epoch 42/50 | Batch 156/198 | Loss: 1.399006724357605\n",
            "Epoch 42/50 | Batch 157/198 | Loss: 1.560410499572754\n",
            "Epoch 42/50 | Batch 158/198 | Loss: 1.6397708654403687\n",
            "Epoch 42/50 | Batch 159/198 | Loss: 1.3346796035766602\n",
            "Epoch 42/50 | Batch 160/198 | Loss: 1.577079176902771\n",
            "Epoch 42/50 | Batch 161/198 | Loss: 1.4266709089279175\n",
            "Epoch 42/50 | Batch 162/198 | Loss: 1.4350262880325317\n",
            "Epoch 42/50 | Batch 163/198 | Loss: 1.5028917789459229\n",
            "Epoch 42/50 | Batch 164/198 | Loss: 1.356589913368225\n",
            "Epoch 42/50 | Batch 165/198 | Loss: 1.4709346294403076\n",
            "Epoch 42/50 | Batch 166/198 | Loss: 1.2405651807785034\n",
            "Epoch 42/50 | Batch 167/198 | Loss: 1.4401309490203857\n",
            "Epoch 42/50 | Batch 168/198 | Loss: 1.362646460533142\n",
            "Epoch 42/50 | Batch 169/198 | Loss: 1.30141019821167\n",
            "Epoch 42/50 | Batch 170/198 | Loss: 1.4652135372161865\n",
            "Epoch 42/50 | Batch 171/198 | Loss: 1.5328317880630493\n",
            "Epoch 42/50 | Batch 172/198 | Loss: 1.5362484455108643\n",
            "Epoch 42/50 | Batch 173/198 | Loss: 1.4325000047683716\n",
            "Epoch 42/50 | Batch 174/198 | Loss: 1.2873871326446533\n",
            "Epoch 42/50 | Batch 175/198 | Loss: 1.2745695114135742\n",
            "Epoch 42/50 | Batch 176/198 | Loss: 1.2585217952728271\n",
            "Epoch 42/50 | Batch 177/198 | Loss: 1.2750599384307861\n",
            "Epoch 42/50 | Batch 178/198 | Loss: 1.3228044509887695\n",
            "Epoch 42/50 | Batch 179/198 | Loss: 1.4475871324539185\n",
            "Epoch 42/50 | Batch 180/198 | Loss: 1.566731333732605\n",
            "Epoch 42/50 | Batch 181/198 | Loss: 1.326341152191162\n",
            "Epoch 42/50 | Batch 182/198 | Loss: 1.4108525514602661\n",
            "Epoch 42/50 | Batch 183/198 | Loss: 1.4522944688796997\n",
            "Epoch 42/50 | Batch 184/198 | Loss: 1.3174197673797607\n",
            "Epoch 42/50 | Batch 185/198 | Loss: 1.5782040357589722\n",
            "Epoch 42/50 | Batch 186/198 | Loss: 1.348882794380188\n",
            "Epoch 42/50 | Batch 187/198 | Loss: 1.2247328758239746\n",
            "Epoch 42/50 | Batch 188/198 | Loss: 1.5269485712051392\n",
            "Epoch 42/50 | Batch 189/198 | Loss: 1.4377267360687256\n",
            "Epoch 42/50 | Batch 190/198 | Loss: 1.4143279790878296\n",
            "Epoch 42/50 | Batch 191/198 | Loss: 1.4715219736099243\n",
            "Epoch 42/50 | Batch 192/198 | Loss: 1.49111807346344\n",
            "Epoch 42/50 | Batch 193/198 | Loss: 1.1029218435287476\n",
            "Epoch 42/50 | Batch 194/198 | Loss: 1.4891785383224487\n",
            "Epoch 42/50 | Batch 195/198 | Loss: 1.6038808822631836\n",
            "Epoch 42/50 | Batch 196/198 | Loss: 1.2881600856781006\n",
            "Epoch 42/50 | Batch 197/198 | Loss: 1.2829569578170776\n",
            "Epoch 42/50 | Batch 198/198 | Loss: 1.6026766300201416\n",
            "Epoch 42/50 | Average Loss: 1.4151631611766238\n",
            "Epoch 43/50 | Batch 1/198 | Loss: 1.5608805418014526\n",
            "Epoch 43/50 | Batch 2/198 | Loss: 1.4141572713851929\n",
            "Epoch 43/50 | Batch 3/198 | Loss: 1.4196655750274658\n",
            "Epoch 43/50 | Batch 4/198 | Loss: 1.284191370010376\n",
            "Epoch 43/50 | Batch 5/198 | Loss: 1.2910830974578857\n",
            "Epoch 43/50 | Batch 6/198 | Loss: 1.1895108222961426\n",
            "Epoch 43/50 | Batch 7/198 | Loss: 1.3514639139175415\n",
            "Epoch 43/50 | Batch 8/198 | Loss: 1.487574577331543\n",
            "Epoch 43/50 | Batch 9/198 | Loss: 1.3600887060165405\n",
            "Epoch 43/50 | Batch 10/198 | Loss: 1.3504544496536255\n",
            "Epoch 43/50 | Batch 11/198 | Loss: 1.303666353225708\n",
            "Epoch 43/50 | Batch 12/198 | Loss: 1.405281662940979\n",
            "Epoch 43/50 | Batch 13/198 | Loss: 1.5276551246643066\n",
            "Epoch 43/50 | Batch 14/198 | Loss: 1.4366639852523804\n",
            "Epoch 43/50 | Batch 15/198 | Loss: 1.3566880226135254\n",
            "Epoch 43/50 | Batch 16/198 | Loss: 1.569969654083252\n",
            "Epoch 43/50 | Batch 17/198 | Loss: 1.3404529094696045\n",
            "Epoch 43/50 | Batch 18/198 | Loss: 1.5332599878311157\n",
            "Epoch 43/50 | Batch 19/198 | Loss: 1.3552637100219727\n",
            "Epoch 43/50 | Batch 20/198 | Loss: 1.5122593641281128\n",
            "Epoch 43/50 | Batch 21/198 | Loss: 1.1183456182479858\n",
            "Epoch 43/50 | Batch 22/198 | Loss: 1.5279821157455444\n",
            "Epoch 43/50 | Batch 23/198 | Loss: 1.2506670951843262\n",
            "Epoch 43/50 | Batch 24/198 | Loss: 1.3186455965042114\n",
            "Epoch 43/50 | Batch 25/198 | Loss: 1.3472981452941895\n",
            "Epoch 43/50 | Batch 26/198 | Loss: 1.388193130493164\n",
            "Epoch 43/50 | Batch 27/198 | Loss: 1.2747251987457275\n",
            "Epoch 43/50 | Batch 28/198 | Loss: 1.2664324045181274\n",
            "Epoch 43/50 | Batch 29/198 | Loss: 1.406705617904663\n",
            "Epoch 43/50 | Batch 30/198 | Loss: 1.3807610273361206\n",
            "Epoch 43/50 | Batch 31/198 | Loss: 1.4518259763717651\n",
            "Epoch 43/50 | Batch 32/198 | Loss: 1.3543833494186401\n",
            "Epoch 43/50 | Batch 33/198 | Loss: 1.3968404531478882\n",
            "Epoch 43/50 | Batch 34/198 | Loss: 1.3547037839889526\n",
            "Epoch 43/50 | Batch 35/198 | Loss: 1.4791091680526733\n",
            "Epoch 43/50 | Batch 36/198 | Loss: 1.492799162864685\n",
            "Epoch 43/50 | Batch 37/198 | Loss: 1.353222131729126\n",
            "Epoch 43/50 | Batch 38/198 | Loss: 1.2831041812896729\n",
            "Epoch 43/50 | Batch 39/198 | Loss: 1.3525793552398682\n",
            "Epoch 43/50 | Batch 40/198 | Loss: 1.4425057172775269\n",
            "Epoch 43/50 | Batch 41/198 | Loss: 1.3964158296585083\n",
            "Epoch 43/50 | Batch 42/198 | Loss: 1.3973852396011353\n",
            "Epoch 43/50 | Batch 43/198 | Loss: 1.6826781034469604\n",
            "Epoch 43/50 | Batch 44/198 | Loss: 1.414673089981079\n",
            "Epoch 43/50 | Batch 45/198 | Loss: 1.4783519506454468\n",
            "Epoch 43/50 | Batch 46/198 | Loss: 1.3353825807571411\n",
            "Epoch 43/50 | Batch 47/198 | Loss: 1.292479395866394\n",
            "Epoch 43/50 | Batch 48/198 | Loss: 1.2913758754730225\n",
            "Epoch 43/50 | Batch 49/198 | Loss: 1.3580050468444824\n",
            "Epoch 43/50 | Batch 50/198 | Loss: 1.4804983139038086\n",
            "Epoch 43/50 | Batch 51/198 | Loss: 1.430356740951538\n",
            "Epoch 43/50 | Batch 52/198 | Loss: 1.3209664821624756\n",
            "Epoch 43/50 | Batch 53/198 | Loss: 1.5005228519439697\n",
            "Epoch 43/50 | Batch 54/198 | Loss: 1.5281434059143066\n",
            "Epoch 43/50 | Batch 55/198 | Loss: 1.563816785812378\n",
            "Epoch 43/50 | Batch 56/198 | Loss: 1.3775571584701538\n",
            "Epoch 43/50 | Batch 57/198 | Loss: 1.4639042615890503\n",
            "Epoch 43/50 | Batch 58/198 | Loss: 1.62241530418396\n",
            "Epoch 43/50 | Batch 59/198 | Loss: 1.4896249771118164\n",
            "Epoch 43/50 | Batch 60/198 | Loss: 1.3967353105545044\n",
            "Epoch 43/50 | Batch 61/198 | Loss: 1.4795700311660767\n",
            "Epoch 43/50 | Batch 62/198 | Loss: 1.4505910873413086\n",
            "Epoch 43/50 | Batch 63/198 | Loss: 1.3405358791351318\n",
            "Epoch 43/50 | Batch 64/198 | Loss: 1.536774754524231\n",
            "Epoch 43/50 | Batch 65/198 | Loss: 1.3517574071884155\n",
            "Epoch 43/50 | Batch 66/198 | Loss: 1.3079980611801147\n",
            "Epoch 43/50 | Batch 67/198 | Loss: 1.3631881475448608\n",
            "Epoch 43/50 | Batch 68/198 | Loss: 1.146540880203247\n",
            "Epoch 43/50 | Batch 69/198 | Loss: 1.465853214263916\n",
            "Epoch 43/50 | Batch 70/198 | Loss: 1.5413323640823364\n",
            "Epoch 43/50 | Batch 71/198 | Loss: 1.396889567375183\n",
            "Epoch 43/50 | Batch 72/198 | Loss: 1.3650696277618408\n",
            "Epoch 43/50 | Batch 73/198 | Loss: 1.526989221572876\n",
            "Epoch 43/50 | Batch 74/198 | Loss: 1.7043646574020386\n",
            "Epoch 43/50 | Batch 75/198 | Loss: 1.3869504928588867\n",
            "Epoch 43/50 | Batch 76/198 | Loss: 1.2891525030136108\n",
            "Epoch 43/50 | Batch 77/198 | Loss: 1.457574725151062\n",
            "Epoch 43/50 | Batch 78/198 | Loss: 1.3399850130081177\n",
            "Epoch 43/50 | Batch 79/198 | Loss: 1.3946890830993652\n",
            "Epoch 43/50 | Batch 80/198 | Loss: 1.3489973545074463\n",
            "Epoch 43/50 | Batch 81/198 | Loss: 1.2362204790115356\n",
            "Epoch 43/50 | Batch 82/198 | Loss: 1.2628371715545654\n",
            "Epoch 43/50 | Batch 83/198 | Loss: 1.5031501054763794\n",
            "Epoch 43/50 | Batch 84/198 | Loss: 1.5968657732009888\n",
            "Epoch 43/50 | Batch 85/198 | Loss: 1.414985179901123\n",
            "Epoch 43/50 | Batch 86/198 | Loss: 1.3363678455352783\n",
            "Epoch 43/50 | Batch 87/198 | Loss: 1.5467878580093384\n",
            "Epoch 43/50 | Batch 88/198 | Loss: 1.4087110757827759\n",
            "Epoch 43/50 | Batch 89/198 | Loss: 1.3341628313064575\n",
            "Epoch 43/50 | Batch 90/198 | Loss: 1.3987332582473755\n",
            "Epoch 43/50 | Batch 91/198 | Loss: 1.552200198173523\n",
            "Epoch 43/50 | Batch 92/198 | Loss: 1.519188404083252\n",
            "Epoch 43/50 | Batch 93/198 | Loss: 1.4799617528915405\n",
            "Epoch 43/50 | Batch 94/198 | Loss: 1.3116810321807861\n",
            "Epoch 43/50 | Batch 95/198 | Loss: 1.377350091934204\n",
            "Epoch 43/50 | Batch 96/198 | Loss: 1.5064904689788818\n",
            "Epoch 43/50 | Batch 97/198 | Loss: 1.544411301612854\n",
            "Epoch 43/50 | Batch 98/198 | Loss: 1.3170270919799805\n",
            "Epoch 43/50 | Batch 99/198 | Loss: 1.4347538948059082\n",
            "Epoch 43/50 | Batch 100/198 | Loss: 1.5818578004837036\n",
            "Epoch 43/50 | Batch 101/198 | Loss: 1.211869716644287\n",
            "Epoch 43/50 | Batch 102/198 | Loss: 1.6115446090698242\n",
            "Epoch 43/50 | Batch 103/198 | Loss: 1.3666017055511475\n",
            "Epoch 43/50 | Batch 104/198 | Loss: 1.5661211013793945\n",
            "Epoch 43/50 | Batch 105/198 | Loss: 1.4036805629730225\n",
            "Epoch 43/50 | Batch 106/198 | Loss: 1.4959782361984253\n",
            "Epoch 43/50 | Batch 107/198 | Loss: 1.4917547702789307\n",
            "Epoch 43/50 | Batch 108/198 | Loss: 1.402127981185913\n",
            "Epoch 43/50 | Batch 109/198 | Loss: 1.5087430477142334\n",
            "Epoch 43/50 | Batch 110/198 | Loss: 1.5132745504379272\n",
            "Epoch 43/50 | Batch 111/198 | Loss: 1.5566481351852417\n",
            "Epoch 43/50 | Batch 112/198 | Loss: 1.187244176864624\n",
            "Epoch 43/50 | Batch 113/198 | Loss: 1.4286575317382812\n",
            "Epoch 43/50 | Batch 114/198 | Loss: 1.4277423620224\n",
            "Epoch 43/50 | Batch 115/198 | Loss: 1.3621424436569214\n",
            "Epoch 43/50 | Batch 116/198 | Loss: 1.5164053440093994\n",
            "Epoch 43/50 | Batch 117/198 | Loss: 1.3721283674240112\n",
            "Epoch 43/50 | Batch 118/198 | Loss: 1.437506079673767\n",
            "Epoch 43/50 | Batch 119/198 | Loss: 1.2223604917526245\n",
            "Epoch 43/50 | Batch 120/198 | Loss: 1.4378472566604614\n",
            "Epoch 43/50 | Batch 121/198 | Loss: 1.4145097732543945\n",
            "Epoch 43/50 | Batch 122/198 | Loss: 1.3223706483840942\n",
            "Epoch 43/50 | Batch 123/198 | Loss: 1.2695151567459106\n",
            "Epoch 43/50 | Batch 124/198 | Loss: 1.3041878938674927\n",
            "Epoch 43/50 | Batch 125/198 | Loss: 1.5092934370040894\n",
            "Epoch 43/50 | Batch 126/198 | Loss: 1.3284605741500854\n",
            "Epoch 43/50 | Batch 127/198 | Loss: 1.6460496187210083\n",
            "Epoch 43/50 | Batch 128/198 | Loss: 1.4393038749694824\n",
            "Epoch 43/50 | Batch 129/198 | Loss: 1.300983190536499\n",
            "Epoch 43/50 | Batch 130/198 | Loss: 1.2610790729522705\n",
            "Epoch 43/50 | Batch 131/198 | Loss: 1.287380337715149\n",
            "Epoch 43/50 | Batch 132/198 | Loss: 1.401167631149292\n",
            "Epoch 43/50 | Batch 133/198 | Loss: 1.3569765090942383\n",
            "Epoch 43/50 | Batch 134/198 | Loss: 1.412539005279541\n",
            "Epoch 43/50 | Batch 135/198 | Loss: 1.2471261024475098\n",
            "Epoch 43/50 | Batch 136/198 | Loss: 1.3459699153900146\n",
            "Epoch 43/50 | Batch 137/198 | Loss: 1.357710361480713\n",
            "Epoch 43/50 | Batch 138/198 | Loss: 1.4165399074554443\n",
            "Epoch 43/50 | Batch 139/198 | Loss: 1.2696210145950317\n",
            "Epoch 43/50 | Batch 140/198 | Loss: 1.4179024696350098\n",
            "Epoch 43/50 | Batch 141/198 | Loss: 1.4183435440063477\n",
            "Epoch 43/50 | Batch 142/198 | Loss: 1.2323319911956787\n",
            "Epoch 43/50 | Batch 143/198 | Loss: 1.2957748174667358\n",
            "Epoch 43/50 | Batch 144/198 | Loss: 1.5449084043502808\n",
            "Epoch 43/50 | Batch 145/198 | Loss: 1.4547604322433472\n",
            "Epoch 43/50 | Batch 146/198 | Loss: 1.4728761911392212\n",
            "Epoch 43/50 | Batch 147/198 | Loss: 1.4707988500595093\n",
            "Epoch 43/50 | Batch 148/198 | Loss: 1.140305757522583\n",
            "Epoch 43/50 | Batch 149/198 | Loss: 1.3154710531234741\n",
            "Epoch 43/50 | Batch 150/198 | Loss: 1.267702579498291\n",
            "Epoch 43/50 | Batch 151/198 | Loss: 1.6124343872070312\n",
            "Epoch 43/50 | Batch 152/198 | Loss: 1.390467882156372\n",
            "Epoch 43/50 | Batch 153/198 | Loss: 1.3840138912200928\n",
            "Epoch 43/50 | Batch 154/198 | Loss: 1.6530163288116455\n",
            "Epoch 43/50 | Batch 155/198 | Loss: 1.4399819374084473\n",
            "Epoch 43/50 | Batch 156/198 | Loss: 1.2603205442428589\n",
            "Epoch 43/50 | Batch 157/198 | Loss: 1.646225094795227\n",
            "Epoch 43/50 | Batch 158/198 | Loss: 1.4653217792510986\n",
            "Epoch 43/50 | Batch 159/198 | Loss: 1.5827574729919434\n",
            "Epoch 43/50 | Batch 160/198 | Loss: 1.502481460571289\n",
            "Epoch 43/50 | Batch 161/198 | Loss: 1.1402758359909058\n",
            "Epoch 43/50 | Batch 162/198 | Loss: 1.2369234561920166\n",
            "Epoch 43/50 | Batch 163/198 | Loss: 1.4241082668304443\n",
            "Epoch 43/50 | Batch 164/198 | Loss: 1.310880422592163\n",
            "Epoch 43/50 | Batch 165/198 | Loss: 1.590133786201477\n",
            "Epoch 43/50 | Batch 166/198 | Loss: 1.358857274055481\n",
            "Epoch 43/50 | Batch 167/198 | Loss: 1.2767826318740845\n",
            "Epoch 43/50 | Batch 168/198 | Loss: 1.5041754245758057\n",
            "Epoch 43/50 | Batch 169/198 | Loss: 1.306179165840149\n",
            "Epoch 43/50 | Batch 170/198 | Loss: 1.5844824314117432\n",
            "Epoch 43/50 | Batch 171/198 | Loss: 1.346088171005249\n",
            "Epoch 43/50 | Batch 172/198 | Loss: 1.407509684562683\n",
            "Epoch 43/50 | Batch 173/198 | Loss: 1.3598575592041016\n",
            "Epoch 43/50 | Batch 174/198 | Loss: 1.622921347618103\n",
            "Epoch 43/50 | Batch 175/198 | Loss: 1.4926563501358032\n",
            "Epoch 43/50 | Batch 176/198 | Loss: 1.3644931316375732\n",
            "Epoch 43/50 | Batch 177/198 | Loss: 1.364867091178894\n",
            "Epoch 43/50 | Batch 178/198 | Loss: 1.3268903493881226\n",
            "Epoch 43/50 | Batch 179/198 | Loss: 1.3534408807754517\n",
            "Epoch 43/50 | Batch 180/198 | Loss: 1.4434391260147095\n",
            "Epoch 43/50 | Batch 181/198 | Loss: 1.430922508239746\n",
            "Epoch 43/50 | Batch 182/198 | Loss: 1.287028431892395\n",
            "Epoch 43/50 | Batch 183/198 | Loss: 1.676138162612915\n",
            "Epoch 43/50 | Batch 184/198 | Loss: 1.2760916948318481\n",
            "Epoch 43/50 | Batch 185/198 | Loss: 1.2248351573944092\n",
            "Epoch 43/50 | Batch 186/198 | Loss: 1.4035931825637817\n",
            "Epoch 43/50 | Batch 187/198 | Loss: 1.443339467048645\n",
            "Epoch 43/50 | Batch 188/198 | Loss: 1.4407576322555542\n",
            "Epoch 43/50 | Batch 189/198 | Loss: 1.4610927104949951\n",
            "Epoch 43/50 | Batch 190/198 | Loss: 1.4595998525619507\n",
            "Epoch 43/50 | Batch 191/198 | Loss: 1.4987965822219849\n",
            "Epoch 43/50 | Batch 192/198 | Loss: 1.3305071592330933\n",
            "Epoch 43/50 | Batch 193/198 | Loss: 1.47431218624115\n",
            "Epoch 43/50 | Batch 194/198 | Loss: 1.3517577648162842\n",
            "Epoch 43/50 | Batch 195/198 | Loss: 1.5093930959701538\n",
            "Epoch 43/50 | Batch 196/198 | Loss: 1.4054369926452637\n",
            "Epoch 43/50 | Batch 197/198 | Loss: 1.3162637948989868\n",
            "Epoch 43/50 | Batch 198/198 | Loss: 1.356952428817749\n",
            "Epoch 43/50 | Average Loss: 1.406626450895059\n",
            "Epoch 44/50 | Batch 1/198 | Loss: 1.4182547330856323\n",
            "Epoch 44/50 | Batch 2/198 | Loss: 1.3618977069854736\n",
            "Epoch 44/50 | Batch 3/198 | Loss: 1.5415493249893188\n",
            "Epoch 44/50 | Batch 4/198 | Loss: 1.4915295839309692\n",
            "Epoch 44/50 | Batch 5/198 | Loss: 1.2759523391723633\n",
            "Epoch 44/50 | Batch 6/198 | Loss: 1.4404571056365967\n",
            "Epoch 44/50 | Batch 7/198 | Loss: 1.2243406772613525\n",
            "Epoch 44/50 | Batch 8/198 | Loss: 1.389890193939209\n",
            "Epoch 44/50 | Batch 9/198 | Loss: 1.3082364797592163\n",
            "Epoch 44/50 | Batch 10/198 | Loss: 1.3357726335525513\n",
            "Epoch 44/50 | Batch 11/198 | Loss: 1.3211313486099243\n",
            "Epoch 44/50 | Batch 12/198 | Loss: 1.523248314857483\n",
            "Epoch 44/50 | Batch 13/198 | Loss: 1.3476548194885254\n",
            "Epoch 44/50 | Batch 14/198 | Loss: 1.579416036605835\n",
            "Epoch 44/50 | Batch 15/198 | Loss: 1.4418766498565674\n",
            "Epoch 44/50 | Batch 16/198 | Loss: 1.4646422863006592\n",
            "Epoch 44/50 | Batch 17/198 | Loss: 1.306481957435608\n",
            "Epoch 44/50 | Batch 18/198 | Loss: 1.3127635717391968\n",
            "Epoch 44/50 | Batch 19/198 | Loss: 1.2984157800674438\n",
            "Epoch 44/50 | Batch 20/198 | Loss: 1.4811909198760986\n",
            "Epoch 44/50 | Batch 21/198 | Loss: 1.398073434829712\n",
            "Epoch 44/50 | Batch 22/198 | Loss: 1.456900954246521\n",
            "Epoch 44/50 | Batch 23/198 | Loss: 1.423866868019104\n",
            "Epoch 44/50 | Batch 24/198 | Loss: 1.426191806793213\n",
            "Epoch 44/50 | Batch 25/198 | Loss: 1.4852001667022705\n",
            "Epoch 44/50 | Batch 26/198 | Loss: 1.354898452758789\n",
            "Epoch 44/50 | Batch 27/198 | Loss: 1.5170035362243652\n",
            "Epoch 44/50 | Batch 28/198 | Loss: 1.4934697151184082\n",
            "Epoch 44/50 | Batch 29/198 | Loss: 1.1088770627975464\n",
            "Epoch 44/50 | Batch 30/198 | Loss: 1.2609679698944092\n",
            "Epoch 44/50 | Batch 31/198 | Loss: 1.6215122938156128\n",
            "Epoch 44/50 | Batch 32/198 | Loss: 1.4573966264724731\n",
            "Epoch 44/50 | Batch 33/198 | Loss: 1.3710330724716187\n",
            "Epoch 44/50 | Batch 34/198 | Loss: 1.6325148344039917\n",
            "Epoch 44/50 | Batch 35/198 | Loss: 1.1879281997680664\n",
            "Epoch 44/50 | Batch 36/198 | Loss: 1.3671308755874634\n",
            "Epoch 44/50 | Batch 37/198 | Loss: 1.232442855834961\n",
            "Epoch 44/50 | Batch 38/198 | Loss: 1.46096670627594\n",
            "Epoch 44/50 | Batch 39/198 | Loss: 1.3899788856506348\n",
            "Epoch 44/50 | Batch 40/198 | Loss: 1.2321879863739014\n",
            "Epoch 44/50 | Batch 41/198 | Loss: 1.5414131879806519\n",
            "Epoch 44/50 | Batch 42/198 | Loss: 1.3297715187072754\n",
            "Epoch 44/50 | Batch 43/198 | Loss: 1.3803678750991821\n",
            "Epoch 44/50 | Batch 44/198 | Loss: 1.2575595378875732\n",
            "Epoch 44/50 | Batch 45/198 | Loss: 1.2725403308868408\n",
            "Epoch 44/50 | Batch 46/198 | Loss: 1.4151363372802734\n",
            "Epoch 44/50 | Batch 47/198 | Loss: 1.4486836194992065\n",
            "Epoch 44/50 | Batch 48/198 | Loss: 1.4152569770812988\n",
            "Epoch 44/50 | Batch 49/198 | Loss: 1.3089855909347534\n",
            "Epoch 44/50 | Batch 50/198 | Loss: 1.375827670097351\n",
            "Epoch 44/50 | Batch 51/198 | Loss: 1.5088568925857544\n",
            "Epoch 44/50 | Batch 52/198 | Loss: 1.398498773574829\n",
            "Epoch 44/50 | Batch 53/198 | Loss: 1.5078598260879517\n",
            "Epoch 44/50 | Batch 54/198 | Loss: 1.3727190494537354\n",
            "Epoch 44/50 | Batch 55/198 | Loss: 1.3450074195861816\n",
            "Epoch 44/50 | Batch 56/198 | Loss: 1.1968153715133667\n",
            "Epoch 44/50 | Batch 57/198 | Loss: 1.3279995918273926\n",
            "Epoch 44/50 | Batch 58/198 | Loss: 1.6006556749343872\n",
            "Epoch 44/50 | Batch 59/198 | Loss: 1.5277154445648193\n",
            "Epoch 44/50 | Batch 60/198 | Loss: 1.3105837106704712\n",
            "Epoch 44/50 | Batch 61/198 | Loss: 1.4343132972717285\n",
            "Epoch 44/50 | Batch 62/198 | Loss: 1.3744783401489258\n",
            "Epoch 44/50 | Batch 63/198 | Loss: 1.2450512647628784\n",
            "Epoch 44/50 | Batch 64/198 | Loss: 1.4073282480239868\n",
            "Epoch 44/50 | Batch 65/198 | Loss: 1.2812082767486572\n",
            "Epoch 44/50 | Batch 66/198 | Loss: 1.5943783521652222\n",
            "Epoch 44/50 | Batch 67/198 | Loss: 1.4445836544036865\n",
            "Epoch 44/50 | Batch 68/198 | Loss: 1.4247593879699707\n",
            "Epoch 44/50 | Batch 69/198 | Loss: 1.3842090368270874\n",
            "Epoch 44/50 | Batch 70/198 | Loss: 1.3921300172805786\n",
            "Epoch 44/50 | Batch 71/198 | Loss: 1.342584252357483\n",
            "Epoch 44/50 | Batch 72/198 | Loss: 1.3888545036315918\n",
            "Epoch 44/50 | Batch 73/198 | Loss: 1.4959502220153809\n",
            "Epoch 44/50 | Batch 74/198 | Loss: 1.4833072423934937\n",
            "Epoch 44/50 | Batch 75/198 | Loss: 1.3618825674057007\n",
            "Epoch 44/50 | Batch 76/198 | Loss: 1.4000942707061768\n",
            "Epoch 44/50 | Batch 77/198 | Loss: 1.4315868616104126\n",
            "Epoch 44/50 | Batch 78/198 | Loss: 1.3030065298080444\n",
            "Epoch 44/50 | Batch 79/198 | Loss: 1.2574138641357422\n",
            "Epoch 44/50 | Batch 80/198 | Loss: 1.528631329536438\n",
            "Epoch 44/50 | Batch 81/198 | Loss: 1.1427640914916992\n",
            "Epoch 44/50 | Batch 82/198 | Loss: 1.5941569805145264\n",
            "Epoch 44/50 | Batch 83/198 | Loss: 1.4577651023864746\n",
            "Epoch 44/50 | Batch 84/198 | Loss: 1.2909866571426392\n",
            "Epoch 44/50 | Batch 85/198 | Loss: 1.2566074132919312\n",
            "Epoch 44/50 | Batch 86/198 | Loss: 1.3596360683441162\n",
            "Epoch 44/50 | Batch 87/198 | Loss: 1.3868671655654907\n",
            "Epoch 44/50 | Batch 88/198 | Loss: 1.3180159330368042\n",
            "Epoch 44/50 | Batch 89/198 | Loss: 1.520830750465393\n",
            "Epoch 44/50 | Batch 90/198 | Loss: 1.4036800861358643\n",
            "Epoch 44/50 | Batch 91/198 | Loss: 1.4405803680419922\n",
            "Epoch 44/50 | Batch 92/198 | Loss: 1.294036865234375\n",
            "Epoch 44/50 | Batch 93/198 | Loss: 1.420127034187317\n",
            "Epoch 44/50 | Batch 94/198 | Loss: 1.462498426437378\n",
            "Epoch 44/50 | Batch 95/198 | Loss: 1.4013510942459106\n",
            "Epoch 44/50 | Batch 96/198 | Loss: 1.4616341590881348\n",
            "Epoch 44/50 | Batch 97/198 | Loss: 1.2527087926864624\n",
            "Epoch 44/50 | Batch 98/198 | Loss: 1.330514907836914\n",
            "Epoch 44/50 | Batch 99/198 | Loss: 1.2472292184829712\n",
            "Epoch 44/50 | Batch 100/198 | Loss: 1.3748887777328491\n",
            "Epoch 44/50 | Batch 101/198 | Loss: 1.5019010305404663\n",
            "Epoch 44/50 | Batch 102/198 | Loss: 1.3820241689682007\n",
            "Epoch 44/50 | Batch 103/198 | Loss: 1.3025648593902588\n",
            "Epoch 44/50 | Batch 104/198 | Loss: 1.18253755569458\n",
            "Epoch 44/50 | Batch 105/198 | Loss: 1.4320389032363892\n",
            "Epoch 44/50 | Batch 106/198 | Loss: 1.4724712371826172\n",
            "Epoch 44/50 | Batch 107/198 | Loss: 1.295150876045227\n",
            "Epoch 44/50 | Batch 108/198 | Loss: 1.502237319946289\n",
            "Epoch 44/50 | Batch 109/198 | Loss: 1.2169852256774902\n",
            "Epoch 44/50 | Batch 110/198 | Loss: 1.3652822971343994\n",
            "Epoch 44/50 | Batch 111/198 | Loss: 1.3299325704574585\n",
            "Epoch 44/50 | Batch 112/198 | Loss: 1.3832889795303345\n",
            "Epoch 44/50 | Batch 113/198 | Loss: 1.1002217531204224\n",
            "Epoch 44/50 | Batch 114/198 | Loss: 1.516282320022583\n",
            "Epoch 44/50 | Batch 115/198 | Loss: 1.4122774600982666\n",
            "Epoch 44/50 | Batch 116/198 | Loss: 1.4570307731628418\n",
            "Epoch 44/50 | Batch 117/198 | Loss: 1.540501356124878\n",
            "Epoch 44/50 | Batch 118/198 | Loss: 1.6265928745269775\n",
            "Epoch 44/50 | Batch 119/198 | Loss: 1.3553346395492554\n",
            "Epoch 44/50 | Batch 120/198 | Loss: 1.447158694267273\n",
            "Epoch 44/50 | Batch 121/198 | Loss: 1.486172080039978\n",
            "Epoch 44/50 | Batch 122/198 | Loss: 1.4325371980667114\n",
            "Epoch 44/50 | Batch 123/198 | Loss: 1.352038025856018\n",
            "Epoch 44/50 | Batch 124/198 | Loss: 1.5347555875778198\n",
            "Epoch 44/50 | Batch 125/198 | Loss: 1.3656009435653687\n",
            "Epoch 44/50 | Batch 126/198 | Loss: 1.4361238479614258\n",
            "Epoch 44/50 | Batch 127/198 | Loss: 1.4664064645767212\n",
            "Epoch 44/50 | Batch 128/198 | Loss: 1.360459327697754\n",
            "Epoch 44/50 | Batch 129/198 | Loss: 1.2044626474380493\n",
            "Epoch 44/50 | Batch 130/198 | Loss: 1.54623544216156\n",
            "Epoch 44/50 | Batch 131/198 | Loss: 1.4234052896499634\n",
            "Epoch 44/50 | Batch 132/198 | Loss: 1.5031932592391968\n",
            "Epoch 44/50 | Batch 133/198 | Loss: 1.542052149772644\n",
            "Epoch 44/50 | Batch 134/198 | Loss: 1.6043580770492554\n",
            "Epoch 44/50 | Batch 135/198 | Loss: 1.3590768575668335\n",
            "Epoch 44/50 | Batch 136/198 | Loss: 1.0694231986999512\n",
            "Epoch 44/50 | Batch 137/198 | Loss: 1.4514614343643188\n",
            "Epoch 44/50 | Batch 138/198 | Loss: 1.503722071647644\n",
            "Epoch 44/50 | Batch 139/198 | Loss: 1.472805380821228\n",
            "Epoch 44/50 | Batch 140/198 | Loss: 1.3485395908355713\n",
            "Epoch 44/50 | Batch 141/198 | Loss: 1.4200143814086914\n",
            "Epoch 44/50 | Batch 142/198 | Loss: 1.1414971351623535\n",
            "Epoch 44/50 | Batch 143/198 | Loss: 1.5577400922775269\n",
            "Epoch 44/50 | Batch 144/198 | Loss: 1.4330662488937378\n",
            "Epoch 44/50 | Batch 145/198 | Loss: 1.340859055519104\n",
            "Epoch 44/50 | Batch 146/198 | Loss: 1.409103512763977\n",
            "Epoch 44/50 | Batch 147/198 | Loss: 1.4751298427581787\n",
            "Epoch 44/50 | Batch 148/198 | Loss: 1.2849072217941284\n",
            "Epoch 44/50 | Batch 149/198 | Loss: 1.487879991531372\n",
            "Epoch 44/50 | Batch 150/198 | Loss: 1.4260586500167847\n",
            "Epoch 44/50 | Batch 151/198 | Loss: 1.4260281324386597\n",
            "Epoch 44/50 | Batch 152/198 | Loss: 1.439162254333496\n",
            "Epoch 44/50 | Batch 153/198 | Loss: 1.4689662456512451\n",
            "Epoch 44/50 | Batch 154/198 | Loss: 1.4671279191970825\n",
            "Epoch 44/50 | Batch 155/198 | Loss: 1.342781901359558\n",
            "Epoch 44/50 | Batch 156/198 | Loss: 1.517563819885254\n",
            "Epoch 44/50 | Batch 157/198 | Loss: 1.4891973733901978\n",
            "Epoch 44/50 | Batch 158/198 | Loss: 1.3839091062545776\n",
            "Epoch 44/50 | Batch 159/198 | Loss: 1.3137844800949097\n",
            "Epoch 44/50 | Batch 160/198 | Loss: 1.3580209016799927\n",
            "Epoch 44/50 | Batch 161/198 | Loss: 1.467049479484558\n",
            "Epoch 44/50 | Batch 162/198 | Loss: 1.313157558441162\n",
            "Epoch 44/50 | Batch 163/198 | Loss: 1.4894720315933228\n",
            "Epoch 44/50 | Batch 164/198 | Loss: 1.338376760482788\n",
            "Epoch 44/50 | Batch 165/198 | Loss: 1.3660277128219604\n",
            "Epoch 44/50 | Batch 166/198 | Loss: 1.3672208786010742\n",
            "Epoch 44/50 | Batch 167/198 | Loss: 1.3634581565856934\n",
            "Epoch 44/50 | Batch 168/198 | Loss: 1.5333553552627563\n",
            "Epoch 44/50 | Batch 169/198 | Loss: 1.2507984638214111\n",
            "Epoch 44/50 | Batch 170/198 | Loss: 1.5016775131225586\n",
            "Epoch 44/50 | Batch 171/198 | Loss: 1.3312149047851562\n",
            "Epoch 44/50 | Batch 172/198 | Loss: 1.380747675895691\n",
            "Epoch 44/50 | Batch 173/198 | Loss: 1.1049262285232544\n",
            "Epoch 44/50 | Batch 174/198 | Loss: 1.4499083757400513\n",
            "Epoch 44/50 | Batch 175/198 | Loss: 1.5768482685089111\n",
            "Epoch 44/50 | Batch 176/198 | Loss: 1.2825746536254883\n",
            "Epoch 44/50 | Batch 177/198 | Loss: 1.3560192584991455\n",
            "Epoch 44/50 | Batch 178/198 | Loss: 1.407558798789978\n",
            "Epoch 44/50 | Batch 179/198 | Loss: 1.568947434425354\n",
            "Epoch 44/50 | Batch 180/198 | Loss: 1.4923081398010254\n",
            "Epoch 44/50 | Batch 181/198 | Loss: 1.202793836593628\n",
            "Epoch 44/50 | Batch 182/198 | Loss: 1.4105517864227295\n",
            "Epoch 44/50 | Batch 183/198 | Loss: 1.3175296783447266\n",
            "Epoch 44/50 | Batch 184/198 | Loss: 1.4533368349075317\n",
            "Epoch 44/50 | Batch 185/198 | Loss: 1.520935297012329\n",
            "Epoch 44/50 | Batch 186/198 | Loss: 1.5074681043624878\n",
            "Epoch 44/50 | Batch 187/198 | Loss: 1.4927221536636353\n",
            "Epoch 44/50 | Batch 188/198 | Loss: 1.3769649267196655\n",
            "Epoch 44/50 | Batch 189/198 | Loss: 1.6022521257400513\n",
            "Epoch 44/50 | Batch 190/198 | Loss: 1.513261079788208\n",
            "Epoch 44/50 | Batch 191/198 | Loss: 1.6158030033111572\n",
            "Epoch 44/50 | Batch 192/198 | Loss: 1.3026715517044067\n",
            "Epoch 44/50 | Batch 193/198 | Loss: 1.3030835390090942\n",
            "Epoch 44/50 | Batch 194/198 | Loss: 1.6149580478668213\n",
            "Epoch 44/50 | Batch 195/198 | Loss: 1.344146490097046\n",
            "Epoch 44/50 | Batch 196/198 | Loss: 1.3146586418151855\n",
            "Epoch 44/50 | Batch 197/198 | Loss: 1.5021225214004517\n",
            "Epoch 44/50 | Batch 198/198 | Loss: 1.227641224861145\n",
            "Epoch 44/50 | Average Loss: 1.3990410156924316\n",
            "Epoch 45/50 | Batch 1/198 | Loss: 1.461879014968872\n",
            "Epoch 45/50 | Batch 2/198 | Loss: 1.4264764785766602\n",
            "Epoch 45/50 | Batch 3/198 | Loss: 1.5533349514007568\n",
            "Epoch 45/50 | Batch 4/198 | Loss: 1.311330795288086\n",
            "Epoch 45/50 | Batch 5/198 | Loss: 1.3875224590301514\n",
            "Epoch 45/50 | Batch 6/198 | Loss: 1.3225849866867065\n",
            "Epoch 45/50 | Batch 7/198 | Loss: 1.4811781644821167\n",
            "Epoch 45/50 | Batch 8/198 | Loss: 1.6185210943222046\n",
            "Epoch 45/50 | Batch 9/198 | Loss: 1.3536701202392578\n",
            "Epoch 45/50 | Batch 10/198 | Loss: 1.3377710580825806\n",
            "Epoch 45/50 | Batch 11/198 | Loss: 1.2359071969985962\n",
            "Epoch 45/50 | Batch 12/198 | Loss: 1.5626888275146484\n",
            "Epoch 45/50 | Batch 13/198 | Loss: 1.554848551750183\n",
            "Epoch 45/50 | Batch 14/198 | Loss: 1.4199483394622803\n",
            "Epoch 45/50 | Batch 15/198 | Loss: 1.3218814134597778\n",
            "Epoch 45/50 | Batch 16/198 | Loss: 1.2111682891845703\n",
            "Epoch 45/50 | Batch 17/198 | Loss: 1.2785249948501587\n",
            "Epoch 45/50 | Batch 18/198 | Loss: 1.473509669303894\n",
            "Epoch 45/50 | Batch 19/198 | Loss: 1.1324584484100342\n",
            "Epoch 45/50 | Batch 20/198 | Loss: 1.4107484817504883\n",
            "Epoch 45/50 | Batch 21/198 | Loss: 1.3310452699661255\n",
            "Epoch 45/50 | Batch 22/198 | Loss: 1.3813624382019043\n",
            "Epoch 45/50 | Batch 23/198 | Loss: 1.1604201793670654\n",
            "Epoch 45/50 | Batch 24/198 | Loss: 1.4586622714996338\n",
            "Epoch 45/50 | Batch 25/198 | Loss: 1.2518537044525146\n",
            "Epoch 45/50 | Batch 26/198 | Loss: 1.3291552066802979\n",
            "Epoch 45/50 | Batch 27/198 | Loss: 1.2656326293945312\n",
            "Epoch 45/50 | Batch 28/198 | Loss: 1.5532549619674683\n",
            "Epoch 45/50 | Batch 29/198 | Loss: 1.5239135026931763\n",
            "Epoch 45/50 | Batch 30/198 | Loss: 1.3302265405654907\n",
            "Epoch 45/50 | Batch 31/198 | Loss: 1.4500547647476196\n",
            "Epoch 45/50 | Batch 32/198 | Loss: 1.208043098449707\n",
            "Epoch 45/50 | Batch 33/198 | Loss: 1.3526332378387451\n",
            "Epoch 45/50 | Batch 34/198 | Loss: 1.4518773555755615\n",
            "Epoch 45/50 | Batch 35/198 | Loss: 1.3787617683410645\n",
            "Epoch 45/50 | Batch 36/198 | Loss: 1.4341200590133667\n",
            "Epoch 45/50 | Batch 37/198 | Loss: 1.2776224613189697\n",
            "Epoch 45/50 | Batch 38/198 | Loss: 1.5140857696533203\n",
            "Epoch 45/50 | Batch 39/198 | Loss: 1.4487909078598022\n",
            "Epoch 45/50 | Batch 40/198 | Loss: 1.3499871492385864\n",
            "Epoch 45/50 | Batch 41/198 | Loss: 1.404999852180481\n",
            "Epoch 45/50 | Batch 42/198 | Loss: 1.470047950744629\n",
            "Epoch 45/50 | Batch 43/198 | Loss: 1.288957118988037\n",
            "Epoch 45/50 | Batch 44/198 | Loss: 1.4933266639709473\n",
            "Epoch 45/50 | Batch 45/198 | Loss: 1.4375783205032349\n",
            "Epoch 45/50 | Batch 46/198 | Loss: 1.33517587184906\n",
            "Epoch 45/50 | Batch 47/198 | Loss: 1.2586787939071655\n",
            "Epoch 45/50 | Batch 48/198 | Loss: 1.3775454759597778\n",
            "Epoch 45/50 | Batch 49/198 | Loss: 1.442543387413025\n",
            "Epoch 45/50 | Batch 50/198 | Loss: 1.3113545179367065\n",
            "Epoch 45/50 | Batch 51/198 | Loss: 1.7106434106826782\n",
            "Epoch 45/50 | Batch 52/198 | Loss: 1.3870759010314941\n",
            "Epoch 45/50 | Batch 53/198 | Loss: 1.2628439664840698\n",
            "Epoch 45/50 | Batch 54/198 | Loss: 1.2344378232955933\n",
            "Epoch 45/50 | Batch 55/198 | Loss: 1.4514591693878174\n",
            "Epoch 45/50 | Batch 56/198 | Loss: 1.3785444498062134\n",
            "Epoch 45/50 | Batch 57/198 | Loss: 1.5159363746643066\n",
            "Epoch 45/50 | Batch 58/198 | Loss: 1.242090106010437\n",
            "Epoch 45/50 | Batch 59/198 | Loss: 1.3485982418060303\n",
            "Epoch 45/50 | Batch 60/198 | Loss: 1.3415635824203491\n",
            "Epoch 45/50 | Batch 61/198 | Loss: 1.347280740737915\n",
            "Epoch 45/50 | Batch 62/198 | Loss: 1.3747892379760742\n",
            "Epoch 45/50 | Batch 63/198 | Loss: 1.427863597869873\n",
            "Epoch 45/50 | Batch 64/198 | Loss: 1.5074265003204346\n",
            "Epoch 45/50 | Batch 65/198 | Loss: 1.218602180480957\n",
            "Epoch 45/50 | Batch 66/198 | Loss: 1.3701400756835938\n",
            "Epoch 45/50 | Batch 67/198 | Loss: 1.3310102224349976\n",
            "Epoch 45/50 | Batch 68/198 | Loss: 1.6632647514343262\n",
            "Epoch 45/50 | Batch 69/198 | Loss: 1.4879614114761353\n",
            "Epoch 45/50 | Batch 70/198 | Loss: 1.4882135391235352\n",
            "Epoch 45/50 | Batch 71/198 | Loss: 1.2675951719284058\n",
            "Epoch 45/50 | Batch 72/198 | Loss: 1.4053019285202026\n",
            "Epoch 45/50 | Batch 73/198 | Loss: 1.4865574836730957\n",
            "Epoch 45/50 | Batch 74/198 | Loss: 1.4564108848571777\n",
            "Epoch 45/50 | Batch 75/198 | Loss: 1.4479923248291016\n",
            "Epoch 45/50 | Batch 76/198 | Loss: 1.3420897722244263\n",
            "Epoch 45/50 | Batch 77/198 | Loss: 1.4193428754806519\n",
            "Epoch 45/50 | Batch 78/198 | Loss: 1.382218599319458\n",
            "Epoch 45/50 | Batch 79/198 | Loss: 1.440410852432251\n",
            "Epoch 45/50 | Batch 80/198 | Loss: 1.3794137239456177\n",
            "Epoch 45/50 | Batch 81/198 | Loss: 1.237768292427063\n",
            "Epoch 45/50 | Batch 82/198 | Loss: 1.5575405359268188\n",
            "Epoch 45/50 | Batch 83/198 | Loss: 1.332595944404602\n",
            "Epoch 45/50 | Batch 84/198 | Loss: 1.4405755996704102\n",
            "Epoch 45/50 | Batch 85/198 | Loss: 1.5405598878860474\n",
            "Epoch 45/50 | Batch 86/198 | Loss: 1.5362507104873657\n",
            "Epoch 45/50 | Batch 87/198 | Loss: 1.1799752712249756\n",
            "Epoch 45/50 | Batch 88/198 | Loss: 1.399699091911316\n",
            "Epoch 45/50 | Batch 89/198 | Loss: 1.459957480430603\n",
            "Epoch 45/50 | Batch 90/198 | Loss: 1.4621354341506958\n",
            "Epoch 45/50 | Batch 91/198 | Loss: 1.3846770524978638\n",
            "Epoch 45/50 | Batch 92/198 | Loss: 1.4213659763336182\n",
            "Epoch 45/50 | Batch 93/198 | Loss: 1.2818857431411743\n",
            "Epoch 45/50 | Batch 94/198 | Loss: 1.4013131856918335\n",
            "Epoch 45/50 | Batch 95/198 | Loss: 1.4314053058624268\n",
            "Epoch 45/50 | Batch 96/198 | Loss: 1.1808873414993286\n",
            "Epoch 45/50 | Batch 97/198 | Loss: 1.3953813314437866\n",
            "Epoch 45/50 | Batch 98/198 | Loss: 1.3807353973388672\n",
            "Epoch 45/50 | Batch 99/198 | Loss: 1.3008161783218384\n",
            "Epoch 45/50 | Batch 100/198 | Loss: 1.4161986112594604\n",
            "Epoch 45/50 | Batch 101/198 | Loss: 1.2706058025360107\n",
            "Epoch 45/50 | Batch 102/198 | Loss: 1.44528329372406\n",
            "Epoch 45/50 | Batch 103/198 | Loss: 1.4055655002593994\n",
            "Epoch 45/50 | Batch 104/198 | Loss: 1.290595531463623\n",
            "Epoch 45/50 | Batch 105/198 | Loss: 1.2153528928756714\n",
            "Epoch 45/50 | Batch 106/198 | Loss: 1.424680233001709\n",
            "Epoch 45/50 | Batch 107/198 | Loss: 1.3956284523010254\n",
            "Epoch 45/50 | Batch 108/198 | Loss: 1.3223068714141846\n",
            "Epoch 45/50 | Batch 109/198 | Loss: 1.3218226432800293\n",
            "Epoch 45/50 | Batch 110/198 | Loss: 1.3389476537704468\n",
            "Epoch 45/50 | Batch 111/198 | Loss: 1.3602160215377808\n",
            "Epoch 45/50 | Batch 112/198 | Loss: 1.3644709587097168\n",
            "Epoch 45/50 | Batch 113/198 | Loss: 1.4974056482315063\n",
            "Epoch 45/50 | Batch 114/198 | Loss: 1.3892344236373901\n",
            "Epoch 45/50 | Batch 115/198 | Loss: 1.5687057971954346\n",
            "Epoch 45/50 | Batch 116/198 | Loss: 1.270508885383606\n",
            "Epoch 45/50 | Batch 117/198 | Loss: 1.4587146043777466\n",
            "Epoch 45/50 | Batch 118/198 | Loss: 1.5329526662826538\n",
            "Epoch 45/50 | Batch 119/198 | Loss: 1.4160281419754028\n",
            "Epoch 45/50 | Batch 120/198 | Loss: 1.3197458982467651\n",
            "Epoch 45/50 | Batch 121/198 | Loss: 1.3722753524780273\n",
            "Epoch 45/50 | Batch 122/198 | Loss: 1.4632676839828491\n",
            "Epoch 45/50 | Batch 123/198 | Loss: 1.3162932395935059\n",
            "Epoch 45/50 | Batch 124/198 | Loss: 1.3486206531524658\n",
            "Epoch 45/50 | Batch 125/198 | Loss: 1.391126036643982\n",
            "Epoch 45/50 | Batch 126/198 | Loss: 1.6243871450424194\n",
            "Epoch 45/50 | Batch 127/198 | Loss: 1.3267282247543335\n",
            "Epoch 45/50 | Batch 128/198 | Loss: 1.3134928941726685\n",
            "Epoch 45/50 | Batch 129/198 | Loss: 1.4648865461349487\n",
            "Epoch 45/50 | Batch 130/198 | Loss: 1.507409691810608\n",
            "Epoch 45/50 | Batch 131/198 | Loss: 1.4891515970230103\n",
            "Epoch 45/50 | Batch 132/198 | Loss: 1.4763143062591553\n",
            "Epoch 45/50 | Batch 133/198 | Loss: 1.5458124876022339\n",
            "Epoch 45/50 | Batch 134/198 | Loss: 1.5195744037628174\n",
            "Epoch 45/50 | Batch 135/198 | Loss: 1.244720697402954\n",
            "Epoch 45/50 | Batch 136/198 | Loss: 1.3799980878829956\n",
            "Epoch 45/50 | Batch 137/198 | Loss: 1.3716984987258911\n",
            "Epoch 45/50 | Batch 138/198 | Loss: 1.3231281042099\n",
            "Epoch 45/50 | Batch 139/198 | Loss: 1.5399580001831055\n",
            "Epoch 45/50 | Batch 140/198 | Loss: 1.4593305587768555\n",
            "Epoch 45/50 | Batch 141/198 | Loss: 1.3945680856704712\n",
            "Epoch 45/50 | Batch 142/198 | Loss: 1.2525100708007812\n",
            "Epoch 45/50 | Batch 143/198 | Loss: 1.3394699096679688\n",
            "Epoch 45/50 | Batch 144/198 | Loss: 1.3702865839004517\n",
            "Epoch 45/50 | Batch 145/198 | Loss: 1.3707621097564697\n",
            "Epoch 45/50 | Batch 146/198 | Loss: 1.4551409482955933\n",
            "Epoch 45/50 | Batch 147/198 | Loss: 1.3566478490829468\n",
            "Epoch 45/50 | Batch 148/198 | Loss: 1.4600619077682495\n",
            "Epoch 45/50 | Batch 149/198 | Loss: 1.2161588668823242\n",
            "Epoch 45/50 | Batch 150/198 | Loss: 1.4897555112838745\n",
            "Epoch 45/50 | Batch 151/198 | Loss: 1.3256176710128784\n",
            "Epoch 45/50 | Batch 152/198 | Loss: 1.3751857280731201\n",
            "Epoch 45/50 | Batch 153/198 | Loss: 1.4045329093933105\n",
            "Epoch 45/50 | Batch 154/198 | Loss: 1.552297592163086\n",
            "Epoch 45/50 | Batch 155/198 | Loss: 1.3948966264724731\n",
            "Epoch 45/50 | Batch 156/198 | Loss: 1.4532606601715088\n",
            "Epoch 45/50 | Batch 157/198 | Loss: 1.4291107654571533\n",
            "Epoch 45/50 | Batch 158/198 | Loss: 1.3588404655456543\n",
            "Epoch 45/50 | Batch 159/198 | Loss: 1.2914154529571533\n",
            "Epoch 45/50 | Batch 160/198 | Loss: 1.3399465084075928\n",
            "Epoch 45/50 | Batch 161/198 | Loss: 1.5190449953079224\n",
            "Epoch 45/50 | Batch 162/198 | Loss: 1.4473310708999634\n",
            "Epoch 45/50 | Batch 163/198 | Loss: 1.1726945638656616\n",
            "Epoch 45/50 | Batch 164/198 | Loss: 1.2980828285217285\n",
            "Epoch 45/50 | Batch 165/198 | Loss: 1.364393949508667\n",
            "Epoch 45/50 | Batch 166/198 | Loss: 1.3816349506378174\n",
            "Epoch 45/50 | Batch 167/198 | Loss: 1.36880362033844\n",
            "Epoch 45/50 | Batch 168/198 | Loss: 1.2043683528900146\n",
            "Epoch 45/50 | Batch 169/198 | Loss: 1.2359029054641724\n",
            "Epoch 45/50 | Batch 170/198 | Loss: 1.6918262243270874\n",
            "Epoch 45/50 | Batch 171/198 | Loss: 1.6829454898834229\n",
            "Epoch 45/50 | Batch 172/198 | Loss: 1.4251470565795898\n",
            "Epoch 45/50 | Batch 173/198 | Loss: 1.4834603071212769\n",
            "Epoch 45/50 | Batch 174/198 | Loss: 1.3697625398635864\n",
            "Epoch 45/50 | Batch 175/198 | Loss: 1.493868112564087\n",
            "Epoch 45/50 | Batch 176/198 | Loss: 1.426531434059143\n",
            "Epoch 45/50 | Batch 177/198 | Loss: 1.5016859769821167\n",
            "Epoch 45/50 | Batch 178/198 | Loss: 1.4330904483795166\n",
            "Epoch 45/50 | Batch 179/198 | Loss: 1.2945661544799805\n",
            "Epoch 45/50 | Batch 180/198 | Loss: 1.4385476112365723\n",
            "Epoch 45/50 | Batch 181/198 | Loss: 1.4015734195709229\n",
            "Epoch 45/50 | Batch 182/198 | Loss: 1.5959748029708862\n",
            "Epoch 45/50 | Batch 183/198 | Loss: 1.289228916168213\n",
            "Epoch 45/50 | Batch 184/198 | Loss: 1.4296766519546509\n",
            "Epoch 45/50 | Batch 185/198 | Loss: 1.151591181755066\n",
            "Epoch 45/50 | Batch 186/198 | Loss: 1.2912646532058716\n",
            "Epoch 45/50 | Batch 187/198 | Loss: 1.2711206674575806\n",
            "Epoch 45/50 | Batch 188/198 | Loss: 1.2183867692947388\n",
            "Epoch 45/50 | Batch 189/198 | Loss: 1.2725403308868408\n",
            "Epoch 45/50 | Batch 190/198 | Loss: 1.4822126626968384\n",
            "Epoch 45/50 | Batch 191/198 | Loss: 1.5794436931610107\n",
            "Epoch 45/50 | Batch 192/198 | Loss: 1.4079132080078125\n",
            "Epoch 45/50 | Batch 193/198 | Loss: 1.3335189819335938\n",
            "Epoch 45/50 | Batch 194/198 | Loss: 1.3518465757369995\n",
            "Epoch 45/50 | Batch 195/198 | Loss: 1.3205586671829224\n",
            "Epoch 45/50 | Batch 196/198 | Loss: 1.383550763130188\n",
            "Epoch 45/50 | Batch 197/198 | Loss: 1.550158143043518\n",
            "Epoch 45/50 | Batch 198/198 | Loss: 1.4229828119277954\n",
            "Epoch 45/50 | Average Loss: 1.3923793519386138\n",
            "Epoch 46/50 | Batch 1/198 | Loss: 1.2894402742385864\n",
            "Epoch 46/50 | Batch 2/198 | Loss: 1.5462009906768799\n",
            "Epoch 46/50 | Batch 3/198 | Loss: 1.370200753211975\n",
            "Epoch 46/50 | Batch 4/198 | Loss: 1.507585048675537\n",
            "Epoch 46/50 | Batch 5/198 | Loss: 1.4222803115844727\n",
            "Epoch 46/50 | Batch 6/198 | Loss: 1.4301722049713135\n",
            "Epoch 46/50 | Batch 7/198 | Loss: 1.4587830305099487\n",
            "Epoch 46/50 | Batch 8/198 | Loss: 1.3784923553466797\n",
            "Epoch 46/50 | Batch 9/198 | Loss: 1.3880254030227661\n",
            "Epoch 46/50 | Batch 10/198 | Loss: 1.391404628753662\n",
            "Epoch 46/50 | Batch 11/198 | Loss: 1.4229446649551392\n",
            "Epoch 46/50 | Batch 12/198 | Loss: 1.3089810609817505\n",
            "Epoch 46/50 | Batch 13/198 | Loss: 1.3385064601898193\n",
            "Epoch 46/50 | Batch 14/198 | Loss: 1.314631462097168\n",
            "Epoch 46/50 | Batch 15/198 | Loss: 1.5372400283813477\n",
            "Epoch 46/50 | Batch 16/198 | Loss: 1.2660936117172241\n",
            "Epoch 46/50 | Batch 17/198 | Loss: 1.2118617296218872\n",
            "Epoch 46/50 | Batch 18/198 | Loss: 1.3131221532821655\n",
            "Epoch 46/50 | Batch 19/198 | Loss: 1.29791259765625\n",
            "Epoch 46/50 | Batch 20/198 | Loss: 1.4070841073989868\n",
            "Epoch 46/50 | Batch 21/198 | Loss: 1.4822638034820557\n",
            "Epoch 46/50 | Batch 22/198 | Loss: 1.2564010620117188\n",
            "Epoch 46/50 | Batch 23/198 | Loss: 1.4443737268447876\n",
            "Epoch 46/50 | Batch 24/198 | Loss: 1.322726845741272\n",
            "Epoch 46/50 | Batch 25/198 | Loss: 1.5780160427093506\n",
            "Epoch 46/50 | Batch 26/198 | Loss: 1.4028517007827759\n",
            "Epoch 46/50 | Batch 27/198 | Loss: 1.4259159564971924\n",
            "Epoch 46/50 | Batch 28/198 | Loss: 1.2305693626403809\n",
            "Epoch 46/50 | Batch 29/198 | Loss: 1.3910176753997803\n",
            "Epoch 46/50 | Batch 30/198 | Loss: 1.2679065465927124\n",
            "Epoch 46/50 | Batch 31/198 | Loss: 1.3322762250900269\n",
            "Epoch 46/50 | Batch 32/198 | Loss: 1.6176587343215942\n",
            "Epoch 46/50 | Batch 33/198 | Loss: 1.289851188659668\n",
            "Epoch 46/50 | Batch 34/198 | Loss: 1.6169427633285522\n",
            "Epoch 46/50 | Batch 35/198 | Loss: 1.2846468687057495\n",
            "Epoch 46/50 | Batch 36/198 | Loss: 1.3836992979049683\n",
            "Epoch 46/50 | Batch 37/198 | Loss: 1.466720700263977\n",
            "Epoch 46/50 | Batch 38/198 | Loss: 1.416229248046875\n",
            "Epoch 46/50 | Batch 39/198 | Loss: 1.6670095920562744\n",
            "Epoch 46/50 | Batch 40/198 | Loss: 1.2248929738998413\n",
            "Epoch 46/50 | Batch 41/198 | Loss: 1.2793755531311035\n",
            "Epoch 46/50 | Batch 42/198 | Loss: 1.3149912357330322\n",
            "Epoch 46/50 | Batch 43/198 | Loss: 1.4456775188446045\n",
            "Epoch 46/50 | Batch 44/198 | Loss: 1.4814807176589966\n",
            "Epoch 46/50 | Batch 45/198 | Loss: 1.6395426988601685\n",
            "Epoch 46/50 | Batch 46/198 | Loss: 1.5280301570892334\n",
            "Epoch 46/50 | Batch 47/198 | Loss: 1.5604997873306274\n",
            "Epoch 46/50 | Batch 48/198 | Loss: 1.2597994804382324\n",
            "Epoch 46/50 | Batch 49/198 | Loss: 1.4292736053466797\n",
            "Epoch 46/50 | Batch 50/198 | Loss: 1.4269956350326538\n",
            "Epoch 46/50 | Batch 51/198 | Loss: 1.3855561017990112\n",
            "Epoch 46/50 | Batch 52/198 | Loss: 1.1868362426757812\n",
            "Epoch 46/50 | Batch 53/198 | Loss: 1.572774887084961\n",
            "Epoch 46/50 | Batch 54/198 | Loss: 1.4039859771728516\n",
            "Epoch 46/50 | Batch 55/198 | Loss: 1.394541621208191\n",
            "Epoch 46/50 | Batch 56/198 | Loss: 1.3875056505203247\n",
            "Epoch 46/50 | Batch 57/198 | Loss: 1.3207353353500366\n",
            "Epoch 46/50 | Batch 58/198 | Loss: 1.5694985389709473\n",
            "Epoch 46/50 | Batch 59/198 | Loss: 1.420786738395691\n",
            "Epoch 46/50 | Batch 60/198 | Loss: 1.1275577545166016\n",
            "Epoch 46/50 | Batch 61/198 | Loss: 1.4862101078033447\n",
            "Epoch 46/50 | Batch 62/198 | Loss: 1.4144582748413086\n",
            "Epoch 46/50 | Batch 63/198 | Loss: 1.2015933990478516\n",
            "Epoch 46/50 | Batch 64/198 | Loss: 1.4324281215667725\n",
            "Epoch 46/50 | Batch 65/198 | Loss: 1.2695492506027222\n",
            "Epoch 46/50 | Batch 66/198 | Loss: 1.577916145324707\n",
            "Epoch 46/50 | Batch 67/198 | Loss: 1.5002658367156982\n",
            "Epoch 46/50 | Batch 68/198 | Loss: 1.2580196857452393\n",
            "Epoch 46/50 | Batch 69/198 | Loss: 1.5210341215133667\n",
            "Epoch 46/50 | Batch 70/198 | Loss: 1.2166285514831543\n",
            "Epoch 46/50 | Batch 71/198 | Loss: 1.5621588230133057\n",
            "Epoch 46/50 | Batch 72/198 | Loss: 1.4054728746414185\n",
            "Epoch 46/50 | Batch 73/198 | Loss: 1.368270993232727\n",
            "Epoch 46/50 | Batch 74/198 | Loss: 1.2177976369857788\n",
            "Epoch 46/50 | Batch 75/198 | Loss: 1.267917275428772\n",
            "Epoch 46/50 | Batch 76/198 | Loss: 1.355700135231018\n",
            "Epoch 46/50 | Batch 77/198 | Loss: 1.3020298480987549\n",
            "Epoch 46/50 | Batch 78/198 | Loss: 1.3843141794204712\n",
            "Epoch 46/50 | Batch 79/198 | Loss: 1.34218168258667\n",
            "Epoch 46/50 | Batch 80/198 | Loss: 1.44828462600708\n",
            "Epoch 46/50 | Batch 81/198 | Loss: 1.4956998825073242\n",
            "Epoch 46/50 | Batch 82/198 | Loss: 1.4782341718673706\n",
            "Epoch 46/50 | Batch 83/198 | Loss: 1.2363061904907227\n",
            "Epoch 46/50 | Batch 84/198 | Loss: 1.401600956916809\n",
            "Epoch 46/50 | Batch 85/198 | Loss: 1.3626832962036133\n",
            "Epoch 46/50 | Batch 86/198 | Loss: 1.3776369094848633\n",
            "Epoch 46/50 | Batch 87/198 | Loss: 1.5204631090164185\n",
            "Epoch 46/50 | Batch 88/198 | Loss: 1.4758111238479614\n",
            "Epoch 46/50 | Batch 89/198 | Loss: 1.355262279510498\n",
            "Epoch 46/50 | Batch 90/198 | Loss: 1.2822265625\n",
            "Epoch 46/50 | Batch 91/198 | Loss: 1.3230501413345337\n",
            "Epoch 46/50 | Batch 92/198 | Loss: 1.328281044960022\n",
            "Epoch 46/50 | Batch 93/198 | Loss: 1.217873454093933\n",
            "Epoch 46/50 | Batch 94/198 | Loss: 1.450522541999817\n",
            "Epoch 46/50 | Batch 95/198 | Loss: 1.2620822191238403\n",
            "Epoch 46/50 | Batch 96/198 | Loss: 1.2516450881958008\n",
            "Epoch 46/50 | Batch 97/198 | Loss: 1.378243327140808\n",
            "Epoch 46/50 | Batch 98/198 | Loss: 1.1445661783218384\n",
            "Epoch 46/50 | Batch 99/198 | Loss: 1.3900034427642822\n",
            "Epoch 46/50 | Batch 100/198 | Loss: 1.2731223106384277\n",
            "Epoch 46/50 | Batch 101/198 | Loss: 1.360333800315857\n",
            "Epoch 46/50 | Batch 102/198 | Loss: 1.135823130607605\n",
            "Epoch 46/50 | Batch 103/198 | Loss: 1.3434007167816162\n",
            "Epoch 46/50 | Batch 104/198 | Loss: 1.2770533561706543\n",
            "Epoch 46/50 | Batch 105/198 | Loss: 1.4146226644515991\n",
            "Epoch 46/50 | Batch 106/198 | Loss: 1.4358028173446655\n",
            "Epoch 46/50 | Batch 107/198 | Loss: 1.3939061164855957\n",
            "Epoch 46/50 | Batch 108/198 | Loss: 1.4138109683990479\n",
            "Epoch 46/50 | Batch 109/198 | Loss: 1.3808228969573975\n",
            "Epoch 46/50 | Batch 110/198 | Loss: 1.3224061727523804\n",
            "Epoch 46/50 | Batch 111/198 | Loss: 1.5841511487960815\n",
            "Epoch 46/50 | Batch 112/198 | Loss: 1.2304027080535889\n",
            "Epoch 46/50 | Batch 113/198 | Loss: 1.506436824798584\n",
            "Epoch 46/50 | Batch 114/198 | Loss: 1.323972225189209\n",
            "Epoch 46/50 | Batch 115/198 | Loss: 1.21072256565094\n",
            "Epoch 46/50 | Batch 116/198 | Loss: 1.4997341632843018\n",
            "Epoch 46/50 | Batch 117/198 | Loss: 1.4609795808792114\n",
            "Epoch 46/50 | Batch 118/198 | Loss: 1.3340907096862793\n",
            "Epoch 46/50 | Batch 119/198 | Loss: 1.5040593147277832\n",
            "Epoch 46/50 | Batch 120/198 | Loss: 1.4758483171463013\n",
            "Epoch 46/50 | Batch 121/198 | Loss: 1.3661426305770874\n",
            "Epoch 46/50 | Batch 122/198 | Loss: 1.5918169021606445\n",
            "Epoch 46/50 | Batch 123/198 | Loss: 1.3107595443725586\n",
            "Epoch 46/50 | Batch 124/198 | Loss: 1.4462366104125977\n",
            "Epoch 46/50 | Batch 125/198 | Loss: 1.2772647142410278\n",
            "Epoch 46/50 | Batch 126/198 | Loss: 1.472170114517212\n",
            "Epoch 46/50 | Batch 127/198 | Loss: 1.2792019844055176\n",
            "Epoch 46/50 | Batch 128/198 | Loss: 1.4744203090667725\n",
            "Epoch 46/50 | Batch 129/198 | Loss: 1.459112524986267\n",
            "Epoch 46/50 | Batch 130/198 | Loss: 1.2370789051055908\n",
            "Epoch 46/50 | Batch 131/198 | Loss: 1.4676913022994995\n",
            "Epoch 46/50 | Batch 132/198 | Loss: 1.5037765502929688\n",
            "Epoch 46/50 | Batch 133/198 | Loss: 1.3003588914871216\n",
            "Epoch 46/50 | Batch 134/198 | Loss: 1.3686338663101196\n",
            "Epoch 46/50 | Batch 135/198 | Loss: 1.5636556148529053\n",
            "Epoch 46/50 | Batch 136/198 | Loss: 1.5170639753341675\n",
            "Epoch 46/50 | Batch 137/198 | Loss: 1.2920126914978027\n",
            "Epoch 46/50 | Batch 138/198 | Loss: 1.3332886695861816\n",
            "Epoch 46/50 | Batch 139/198 | Loss: 1.5819658041000366\n",
            "Epoch 46/50 | Batch 140/198 | Loss: 1.294553518295288\n",
            "Epoch 46/50 | Batch 141/198 | Loss: 1.2909520864486694\n",
            "Epoch 46/50 | Batch 142/198 | Loss: 1.428816318511963\n",
            "Epoch 46/50 | Batch 143/198 | Loss: 1.3812956809997559\n",
            "Epoch 46/50 | Batch 144/198 | Loss: 1.5581331253051758\n",
            "Epoch 46/50 | Batch 145/198 | Loss: 1.4161773920059204\n",
            "Epoch 46/50 | Batch 146/198 | Loss: 1.424436330795288\n",
            "Epoch 46/50 | Batch 147/198 | Loss: 1.582865834236145\n",
            "Epoch 46/50 | Batch 148/198 | Loss: 1.416548728942871\n",
            "Epoch 46/50 | Batch 149/198 | Loss: 1.3837172985076904\n",
            "Epoch 46/50 | Batch 150/198 | Loss: 1.3784661293029785\n",
            "Epoch 46/50 | Batch 151/198 | Loss: 1.4826936721801758\n",
            "Epoch 46/50 | Batch 152/198 | Loss: 1.195162057876587\n",
            "Epoch 46/50 | Batch 153/198 | Loss: 1.4559130668640137\n",
            "Epoch 46/50 | Batch 154/198 | Loss: 1.324150562286377\n",
            "Epoch 46/50 | Batch 155/198 | Loss: 1.4210649728775024\n",
            "Epoch 46/50 | Batch 156/198 | Loss: 1.2824597358703613\n",
            "Epoch 46/50 | Batch 157/198 | Loss: 1.4796291589736938\n",
            "Epoch 46/50 | Batch 158/198 | Loss: 1.3455440998077393\n",
            "Epoch 46/50 | Batch 159/198 | Loss: 1.3870042562484741\n",
            "Epoch 46/50 | Batch 160/198 | Loss: 1.574350357055664\n",
            "Epoch 46/50 | Batch 161/198 | Loss: 1.5452640056610107\n",
            "Epoch 46/50 | Batch 162/198 | Loss: 1.4061676263809204\n",
            "Epoch 46/50 | Batch 163/198 | Loss: 1.5218273401260376\n",
            "Epoch 46/50 | Batch 164/198 | Loss: 1.355332374572754\n",
            "Epoch 46/50 | Batch 165/198 | Loss: 1.3697601556777954\n",
            "Epoch 46/50 | Batch 166/198 | Loss: 1.375868320465088\n",
            "Epoch 46/50 | Batch 167/198 | Loss: 1.3668519258499146\n",
            "Epoch 46/50 | Batch 168/198 | Loss: 1.35086190700531\n",
            "Epoch 46/50 | Batch 169/198 | Loss: 1.5112102031707764\n",
            "Epoch 46/50 | Batch 170/198 | Loss: 1.372664451599121\n",
            "Epoch 46/50 | Batch 171/198 | Loss: 1.1541972160339355\n",
            "Epoch 46/50 | Batch 172/198 | Loss: 1.3215036392211914\n",
            "Epoch 46/50 | Batch 173/198 | Loss: 1.564889907836914\n",
            "Epoch 46/50 | Batch 174/198 | Loss: 1.3149653673171997\n",
            "Epoch 46/50 | Batch 175/198 | Loss: 1.3072799444198608\n",
            "Epoch 46/50 | Batch 176/198 | Loss: 1.3140558004379272\n",
            "Epoch 46/50 | Batch 177/198 | Loss: 1.3558248281478882\n",
            "Epoch 46/50 | Batch 178/198 | Loss: 1.3237160444259644\n",
            "Epoch 46/50 | Batch 179/198 | Loss: 1.3095428943634033\n",
            "Epoch 46/50 | Batch 180/198 | Loss: 1.26108980178833\n",
            "Epoch 46/50 | Batch 181/198 | Loss: 1.2972996234893799\n",
            "Epoch 46/50 | Batch 182/198 | Loss: 1.2601021528244019\n",
            "Epoch 46/50 | Batch 183/198 | Loss: 1.5486170053482056\n",
            "Epoch 46/50 | Batch 184/198 | Loss: 1.3081144094467163\n",
            "Epoch 46/50 | Batch 185/198 | Loss: 1.5741097927093506\n",
            "Epoch 46/50 | Batch 186/198 | Loss: 1.4102369546890259\n",
            "Epoch 46/50 | Batch 187/198 | Loss: 1.2211427688598633\n",
            "Epoch 46/50 | Batch 188/198 | Loss: 1.2952462434768677\n",
            "Epoch 46/50 | Batch 189/198 | Loss: 1.3454102277755737\n",
            "Epoch 46/50 | Batch 190/198 | Loss: 1.6164608001708984\n",
            "Epoch 46/50 | Batch 191/198 | Loss: 1.331923246383667\n",
            "Epoch 46/50 | Batch 192/198 | Loss: 1.3997142314910889\n",
            "Epoch 46/50 | Batch 193/198 | Loss: 1.2512867450714111\n",
            "Epoch 46/50 | Batch 194/198 | Loss: 1.332992672920227\n",
            "Epoch 46/50 | Batch 195/198 | Loss: 1.4194941520690918\n",
            "Epoch 46/50 | Batch 196/198 | Loss: 1.2717821598052979\n",
            "Epoch 46/50 | Batch 197/198 | Loss: 1.5933833122253418\n",
            "Epoch 46/50 | Batch 198/198 | Loss: 1.2440783977508545\n",
            "Epoch 46/50 | Average Loss: 1.3861371670106444\n",
            "Epoch 47/50 | Batch 1/198 | Loss: 1.336059331893921\n",
            "Epoch 47/50 | Batch 2/198 | Loss: 1.372660517692566\n",
            "Epoch 47/50 | Batch 3/198 | Loss: 1.236080527305603\n",
            "Epoch 47/50 | Batch 4/198 | Loss: 1.4511141777038574\n",
            "Epoch 47/50 | Batch 5/198 | Loss: 1.4674553871154785\n",
            "Epoch 47/50 | Batch 6/198 | Loss: 1.4340107440948486\n",
            "Epoch 47/50 | Batch 7/198 | Loss: 1.1807464361190796\n",
            "Epoch 47/50 | Batch 8/198 | Loss: 1.4683562517166138\n",
            "Epoch 47/50 | Batch 9/198 | Loss: 1.3974560499191284\n",
            "Epoch 47/50 | Batch 10/198 | Loss: 1.3767304420471191\n",
            "Epoch 47/50 | Batch 11/198 | Loss: 1.390820860862732\n",
            "Epoch 47/50 | Batch 12/198 | Loss: 1.3255406618118286\n",
            "Epoch 47/50 | Batch 13/198 | Loss: 1.506736159324646\n",
            "Epoch 47/50 | Batch 14/198 | Loss: 1.3776401281356812\n",
            "Epoch 47/50 | Batch 15/198 | Loss: 1.5677440166473389\n",
            "Epoch 47/50 | Batch 16/198 | Loss: 1.317123532295227\n",
            "Epoch 47/50 | Batch 17/198 | Loss: 1.4613386392593384\n",
            "Epoch 47/50 | Batch 18/198 | Loss: 1.4896210432052612\n",
            "Epoch 47/50 | Batch 19/198 | Loss: 1.3642206192016602\n",
            "Epoch 47/50 | Batch 20/198 | Loss: 1.2855896949768066\n",
            "Epoch 47/50 | Batch 21/198 | Loss: 1.5082166194915771\n",
            "Epoch 47/50 | Batch 22/198 | Loss: 1.3129725456237793\n",
            "Epoch 47/50 | Batch 23/198 | Loss: 1.3781667947769165\n",
            "Epoch 47/50 | Batch 24/198 | Loss: 1.4628350734710693\n",
            "Epoch 47/50 | Batch 25/198 | Loss: 1.3545315265655518\n",
            "Epoch 47/50 | Batch 26/198 | Loss: 1.3165795803070068\n",
            "Epoch 47/50 | Batch 27/198 | Loss: 1.2589812278747559\n",
            "Epoch 47/50 | Batch 28/198 | Loss: 1.309560775756836\n",
            "Epoch 47/50 | Batch 29/198 | Loss: 1.4826922416687012\n",
            "Epoch 47/50 | Batch 30/198 | Loss: 1.2132222652435303\n",
            "Epoch 47/50 | Batch 31/198 | Loss: 1.3146175146102905\n",
            "Epoch 47/50 | Batch 32/198 | Loss: 1.3746012449264526\n",
            "Epoch 47/50 | Batch 33/198 | Loss: 1.1831187009811401\n",
            "Epoch 47/50 | Batch 34/198 | Loss: 1.4946653842926025\n",
            "Epoch 47/50 | Batch 35/198 | Loss: 1.4000155925750732\n",
            "Epoch 47/50 | Batch 36/198 | Loss: 1.4054920673370361\n",
            "Epoch 47/50 | Batch 37/198 | Loss: 1.470777988433838\n",
            "Epoch 47/50 | Batch 38/198 | Loss: 1.2794493436813354\n",
            "Epoch 47/50 | Batch 39/198 | Loss: 1.6065565347671509\n",
            "Epoch 47/50 | Batch 40/198 | Loss: 1.192933440208435\n",
            "Epoch 47/50 | Batch 41/198 | Loss: 1.4952374696731567\n",
            "Epoch 47/50 | Batch 42/198 | Loss: 1.4589569568634033\n",
            "Epoch 47/50 | Batch 43/198 | Loss: 1.2986959218978882\n",
            "Epoch 47/50 | Batch 44/198 | Loss: 1.5149526596069336\n",
            "Epoch 47/50 | Batch 45/198 | Loss: 1.3294965028762817\n",
            "Epoch 47/50 | Batch 46/198 | Loss: 1.3721065521240234\n",
            "Epoch 47/50 | Batch 47/198 | Loss: 1.3355050086975098\n",
            "Epoch 47/50 | Batch 48/198 | Loss: 1.3888224363327026\n",
            "Epoch 47/50 | Batch 49/198 | Loss: 1.550289511680603\n",
            "Epoch 47/50 | Batch 50/198 | Loss: 1.3192545175552368\n",
            "Epoch 47/50 | Batch 51/198 | Loss: 1.4384267330169678\n",
            "Epoch 47/50 | Batch 52/198 | Loss: 1.3385299444198608\n",
            "Epoch 47/50 | Batch 53/198 | Loss: 1.3073124885559082\n",
            "Epoch 47/50 | Batch 54/198 | Loss: 1.3128609657287598\n",
            "Epoch 47/50 | Batch 55/198 | Loss: 1.4828877449035645\n",
            "Epoch 47/50 | Batch 56/198 | Loss: 1.267181158065796\n",
            "Epoch 47/50 | Batch 57/198 | Loss: 1.542921781539917\n",
            "Epoch 47/50 | Batch 58/198 | Loss: 1.399114727973938\n",
            "Epoch 47/50 | Batch 59/198 | Loss: 1.333193063735962\n",
            "Epoch 47/50 | Batch 60/198 | Loss: 1.4655909538269043\n",
            "Epoch 47/50 | Batch 61/198 | Loss: 1.37342369556427\n",
            "Epoch 47/50 | Batch 62/198 | Loss: 1.4750704765319824\n",
            "Epoch 47/50 | Batch 63/198 | Loss: 1.4600330591201782\n",
            "Epoch 47/50 | Batch 64/198 | Loss: 1.3871644735336304\n",
            "Epoch 47/50 | Batch 65/198 | Loss: 1.3092464208602905\n",
            "Epoch 47/50 | Batch 66/198 | Loss: 1.4742554426193237\n",
            "Epoch 47/50 | Batch 67/198 | Loss: 1.2825101613998413\n",
            "Epoch 47/50 | Batch 68/198 | Loss: 1.4558099508285522\n",
            "Epoch 47/50 | Batch 69/198 | Loss: 1.3326444625854492\n",
            "Epoch 47/50 | Batch 70/198 | Loss: 1.2871805429458618\n",
            "Epoch 47/50 | Batch 71/198 | Loss: 1.3145376443862915\n",
            "Epoch 47/50 | Batch 72/198 | Loss: 1.0429866313934326\n",
            "Epoch 47/50 | Batch 73/198 | Loss: 1.445615530014038\n",
            "Epoch 47/50 | Batch 74/198 | Loss: 1.3397727012634277\n",
            "Epoch 47/50 | Batch 75/198 | Loss: 1.3727471828460693\n",
            "Epoch 47/50 | Batch 76/198 | Loss: 1.3729100227355957\n",
            "Epoch 47/50 | Batch 77/198 | Loss: 1.428322434425354\n",
            "Epoch 47/50 | Batch 78/198 | Loss: 1.2459824085235596\n",
            "Epoch 47/50 | Batch 79/198 | Loss: 1.4191497564315796\n",
            "Epoch 47/50 | Batch 80/198 | Loss: 1.46401047706604\n",
            "Epoch 47/50 | Batch 81/198 | Loss: 1.3906139135360718\n",
            "Epoch 47/50 | Batch 82/198 | Loss: 1.2975386381149292\n",
            "Epoch 47/50 | Batch 83/198 | Loss: 1.176067590713501\n",
            "Epoch 47/50 | Batch 84/198 | Loss: 1.282547950744629\n",
            "Epoch 47/50 | Batch 85/198 | Loss: 1.3725422620773315\n",
            "Epoch 47/50 | Batch 86/198 | Loss: 1.1607431173324585\n",
            "Epoch 47/50 | Batch 87/198 | Loss: 1.4709818363189697\n",
            "Epoch 47/50 | Batch 88/198 | Loss: 1.370298981666565\n",
            "Epoch 47/50 | Batch 89/198 | Loss: 1.2934596538543701\n",
            "Epoch 47/50 | Batch 90/198 | Loss: 1.3269414901733398\n",
            "Epoch 47/50 | Batch 91/198 | Loss: 1.5960005521774292\n",
            "Epoch 47/50 | Batch 92/198 | Loss: 1.4598522186279297\n",
            "Epoch 47/50 | Batch 93/198 | Loss: 1.3949451446533203\n",
            "Epoch 47/50 | Batch 94/198 | Loss: 1.300577998161316\n",
            "Epoch 47/50 | Batch 95/198 | Loss: 1.1904890537261963\n",
            "Epoch 47/50 | Batch 96/198 | Loss: 1.257798194885254\n",
            "Epoch 47/50 | Batch 97/198 | Loss: 1.5771883726119995\n",
            "Epoch 47/50 | Batch 98/198 | Loss: 1.2646863460540771\n",
            "Epoch 47/50 | Batch 99/198 | Loss: 1.4150282144546509\n",
            "Epoch 47/50 | Batch 100/198 | Loss: 1.521865725517273\n",
            "Epoch 47/50 | Batch 101/198 | Loss: 1.4235737323760986\n",
            "Epoch 47/50 | Batch 102/198 | Loss: 1.4746668338775635\n",
            "Epoch 47/50 | Batch 103/198 | Loss: 1.4137930870056152\n",
            "Epoch 47/50 | Batch 104/198 | Loss: 1.3854069709777832\n",
            "Epoch 47/50 | Batch 105/198 | Loss: 1.4886958599090576\n",
            "Epoch 47/50 | Batch 106/198 | Loss: 1.1361674070358276\n",
            "Epoch 47/50 | Batch 107/198 | Loss: 1.4554842710494995\n",
            "Epoch 47/50 | Batch 108/198 | Loss: 1.3186962604522705\n",
            "Epoch 47/50 | Batch 109/198 | Loss: 1.379431128501892\n",
            "Epoch 47/50 | Batch 110/198 | Loss: 1.346402883529663\n",
            "Epoch 47/50 | Batch 111/198 | Loss: 1.3293304443359375\n",
            "Epoch 47/50 | Batch 112/198 | Loss: 1.1009018421173096\n",
            "Epoch 47/50 | Batch 113/198 | Loss: 1.2993559837341309\n",
            "Epoch 47/50 | Batch 114/198 | Loss: 1.4370335340499878\n",
            "Epoch 47/50 | Batch 115/198 | Loss: 1.3824225664138794\n",
            "Epoch 47/50 | Batch 116/198 | Loss: 1.412614107131958\n",
            "Epoch 47/50 | Batch 117/198 | Loss: 1.2686190605163574\n",
            "Epoch 47/50 | Batch 118/198 | Loss: 1.5368422269821167\n",
            "Epoch 47/50 | Batch 119/198 | Loss: 1.5271143913269043\n",
            "Epoch 47/50 | Batch 120/198 | Loss: 1.4858728647232056\n",
            "Epoch 47/50 | Batch 121/198 | Loss: 1.3521370887756348\n",
            "Epoch 47/50 | Batch 122/198 | Loss: 1.4126633405685425\n",
            "Epoch 47/50 | Batch 123/198 | Loss: 1.4127553701400757\n",
            "Epoch 47/50 | Batch 124/198 | Loss: 1.3480573892593384\n",
            "Epoch 47/50 | Batch 125/198 | Loss: 1.394202709197998\n",
            "Epoch 47/50 | Batch 126/198 | Loss: 1.36869478225708\n",
            "Epoch 47/50 | Batch 127/198 | Loss: 1.4320337772369385\n",
            "Epoch 47/50 | Batch 128/198 | Loss: 1.5190725326538086\n",
            "Epoch 47/50 | Batch 129/198 | Loss: 1.302910327911377\n",
            "Epoch 47/50 | Batch 130/198 | Loss: 1.3393549919128418\n",
            "Epoch 47/50 | Batch 131/198 | Loss: 1.413276195526123\n",
            "Epoch 47/50 | Batch 132/198 | Loss: 1.2442656755447388\n",
            "Epoch 47/50 | Batch 133/198 | Loss: 1.346147060394287\n",
            "Epoch 47/50 | Batch 134/198 | Loss: 1.5768364667892456\n",
            "Epoch 47/50 | Batch 135/198 | Loss: 1.4412364959716797\n",
            "Epoch 47/50 | Batch 136/198 | Loss: 1.4077579975128174\n",
            "Epoch 47/50 | Batch 137/198 | Loss: 1.3953677415847778\n",
            "Epoch 47/50 | Batch 138/198 | Loss: 1.6004457473754883\n",
            "Epoch 47/50 | Batch 139/198 | Loss: 1.427107334136963\n",
            "Epoch 47/50 | Batch 140/198 | Loss: 1.5553196668624878\n",
            "Epoch 47/50 | Batch 141/198 | Loss: 1.233325481414795\n",
            "Epoch 47/50 | Batch 142/198 | Loss: 1.294713020324707\n",
            "Epoch 47/50 | Batch 143/198 | Loss: 1.4179744720458984\n",
            "Epoch 47/50 | Batch 144/198 | Loss: 1.3105484247207642\n",
            "Epoch 47/50 | Batch 145/198 | Loss: 1.3465371131896973\n",
            "Epoch 47/50 | Batch 146/198 | Loss: 1.3426640033721924\n",
            "Epoch 47/50 | Batch 147/198 | Loss: 1.3494958877563477\n",
            "Epoch 47/50 | Batch 148/198 | Loss: 1.562132477760315\n",
            "Epoch 47/50 | Batch 149/198 | Loss: 1.292023777961731\n",
            "Epoch 47/50 | Batch 150/198 | Loss: 1.448182225227356\n",
            "Epoch 47/50 | Batch 151/198 | Loss: 1.4625253677368164\n",
            "Epoch 47/50 | Batch 152/198 | Loss: 1.4854830503463745\n",
            "Epoch 47/50 | Batch 153/198 | Loss: 1.2676866054534912\n",
            "Epoch 47/50 | Batch 154/198 | Loss: 1.3207528591156006\n",
            "Epoch 47/50 | Batch 155/198 | Loss: 1.2236175537109375\n",
            "Epoch 47/50 | Batch 156/198 | Loss: 1.197737455368042\n",
            "Epoch 47/50 | Batch 157/198 | Loss: 1.3718386888504028\n",
            "Epoch 47/50 | Batch 158/198 | Loss: 1.4027003049850464\n",
            "Epoch 47/50 | Batch 159/198 | Loss: 1.4635651111602783\n",
            "Epoch 47/50 | Batch 160/198 | Loss: 1.7108350992202759\n",
            "Epoch 47/50 | Batch 161/198 | Loss: 1.4529770612716675\n",
            "Epoch 47/50 | Batch 162/198 | Loss: 1.4102559089660645\n",
            "Epoch 47/50 | Batch 163/198 | Loss: 1.4839646816253662\n",
            "Epoch 47/50 | Batch 164/198 | Loss: 1.3001534938812256\n",
            "Epoch 47/50 | Batch 165/198 | Loss: 1.4350284337997437\n",
            "Epoch 47/50 | Batch 166/198 | Loss: 1.374729037284851\n",
            "Epoch 47/50 | Batch 167/198 | Loss: 1.5353763103485107\n",
            "Epoch 47/50 | Batch 168/198 | Loss: 1.3676189184188843\n",
            "Epoch 47/50 | Batch 169/198 | Loss: 1.5060919523239136\n",
            "Epoch 47/50 | Batch 170/198 | Loss: 1.2408626079559326\n",
            "Epoch 47/50 | Batch 171/198 | Loss: 1.3478708267211914\n",
            "Epoch 47/50 | Batch 172/198 | Loss: 1.3826634883880615\n",
            "Epoch 47/50 | Batch 173/198 | Loss: 1.5337406396865845\n",
            "Epoch 47/50 | Batch 174/198 | Loss: 1.266015887260437\n",
            "Epoch 47/50 | Batch 175/198 | Loss: 1.3633973598480225\n",
            "Epoch 47/50 | Batch 176/198 | Loss: 1.5581263303756714\n",
            "Epoch 47/50 | Batch 177/198 | Loss: 1.4006067514419556\n",
            "Epoch 47/50 | Batch 178/198 | Loss: 1.5216127634048462\n",
            "Epoch 47/50 | Batch 179/198 | Loss: 1.3660236597061157\n",
            "Epoch 47/50 | Batch 180/198 | Loss: 1.3803234100341797\n",
            "Epoch 47/50 | Batch 181/198 | Loss: 1.4562352895736694\n",
            "Epoch 47/50 | Batch 182/198 | Loss: 1.3087728023529053\n",
            "Epoch 47/50 | Batch 183/198 | Loss: 1.3747670650482178\n",
            "Epoch 47/50 | Batch 184/198 | Loss: 1.5705991983413696\n",
            "Epoch 47/50 | Batch 185/198 | Loss: 1.4500927925109863\n",
            "Epoch 47/50 | Batch 186/198 | Loss: 1.4162565469741821\n",
            "Epoch 47/50 | Batch 187/198 | Loss: 1.262838363647461\n",
            "Epoch 47/50 | Batch 188/198 | Loss: 1.2655404806137085\n",
            "Epoch 47/50 | Batch 189/198 | Loss: 1.4625767469406128\n",
            "Epoch 47/50 | Batch 190/198 | Loss: 1.2114710807800293\n",
            "Epoch 47/50 | Batch 191/198 | Loss: 1.1951597929000854\n",
            "Epoch 47/50 | Batch 192/198 | Loss: 1.1483789682388306\n",
            "Epoch 47/50 | Batch 193/198 | Loss: 1.3432164192199707\n",
            "Epoch 47/50 | Batch 194/198 | Loss: 1.3125462532043457\n",
            "Epoch 47/50 | Batch 195/198 | Loss: 1.493675708770752\n",
            "Epoch 47/50 | Batch 196/198 | Loss: 1.217352032661438\n",
            "Epoch 47/50 | Batch 197/198 | Loss: 1.426456093788147\n",
            "Epoch 47/50 | Batch 198/198 | Loss: 1.3532365560531616\n",
            "Epoch 47/50 | Average Loss: 1.3806814563394798\n",
            "Epoch 48/50 | Batch 1/198 | Loss: 1.4956411123275757\n",
            "Epoch 48/50 | Batch 2/198 | Loss: 1.3036410808563232\n",
            "Epoch 48/50 | Batch 3/198 | Loss: 1.5456631183624268\n",
            "Epoch 48/50 | Batch 4/198 | Loss: 1.4850317239761353\n",
            "Epoch 48/50 | Batch 5/198 | Loss: 1.3928837776184082\n",
            "Epoch 48/50 | Batch 6/198 | Loss: 1.5621308088302612\n",
            "Epoch 48/50 | Batch 7/198 | Loss: 1.3325607776641846\n",
            "Epoch 48/50 | Batch 8/198 | Loss: 1.3983062505722046\n",
            "Epoch 48/50 | Batch 9/198 | Loss: 1.3105809688568115\n",
            "Epoch 48/50 | Batch 10/198 | Loss: 1.4942755699157715\n",
            "Epoch 48/50 | Batch 11/198 | Loss: 1.4802128076553345\n",
            "Epoch 48/50 | Batch 12/198 | Loss: 1.4882481098175049\n",
            "Epoch 48/50 | Batch 13/198 | Loss: 1.5970399379730225\n",
            "Epoch 48/50 | Batch 14/198 | Loss: 1.223716139793396\n",
            "Epoch 48/50 | Batch 15/198 | Loss: 1.2691835165023804\n",
            "Epoch 48/50 | Batch 16/198 | Loss: 1.3938746452331543\n",
            "Epoch 48/50 | Batch 17/198 | Loss: 1.2296948432922363\n",
            "Epoch 48/50 | Batch 18/198 | Loss: 1.3790264129638672\n",
            "Epoch 48/50 | Batch 19/198 | Loss: 1.383612036705017\n",
            "Epoch 48/50 | Batch 20/198 | Loss: 1.4262449741363525\n",
            "Epoch 48/50 | Batch 21/198 | Loss: 1.6055371761322021\n",
            "Epoch 48/50 | Batch 22/198 | Loss: 1.3214402198791504\n",
            "Epoch 48/50 | Batch 23/198 | Loss: 1.5515880584716797\n",
            "Epoch 48/50 | Batch 24/198 | Loss: 1.3634251356124878\n",
            "Epoch 48/50 | Batch 25/198 | Loss: 1.464516282081604\n",
            "Epoch 48/50 | Batch 26/198 | Loss: 1.4357447624206543\n",
            "Epoch 48/50 | Batch 27/198 | Loss: 1.300217628479004\n",
            "Epoch 48/50 | Batch 28/198 | Loss: 1.315329909324646\n",
            "Epoch 48/50 | Batch 29/198 | Loss: 1.5309721231460571\n",
            "Epoch 48/50 | Batch 30/198 | Loss: 1.4827725887298584\n",
            "Epoch 48/50 | Batch 31/198 | Loss: 1.4187636375427246\n",
            "Epoch 48/50 | Batch 32/198 | Loss: 1.2655757665634155\n",
            "Epoch 48/50 | Batch 33/198 | Loss: 1.2210969924926758\n",
            "Epoch 48/50 | Batch 34/198 | Loss: 1.2173823118209839\n",
            "Epoch 48/50 | Batch 35/198 | Loss: 1.4409472942352295\n",
            "Epoch 48/50 | Batch 36/198 | Loss: 1.3450613021850586\n",
            "Epoch 48/50 | Batch 37/198 | Loss: 1.4724434614181519\n",
            "Epoch 48/50 | Batch 38/198 | Loss: 1.2890688180923462\n",
            "Epoch 48/50 | Batch 39/198 | Loss: 1.343011498451233\n",
            "Epoch 48/50 | Batch 40/198 | Loss: 1.4129222631454468\n",
            "Epoch 48/50 | Batch 41/198 | Loss: 1.5675745010375977\n",
            "Epoch 48/50 | Batch 42/198 | Loss: 1.5448755025863647\n",
            "Epoch 48/50 | Batch 43/198 | Loss: 1.3059102296829224\n",
            "Epoch 48/50 | Batch 44/198 | Loss: 1.5441935062408447\n",
            "Epoch 48/50 | Batch 45/198 | Loss: 1.4338501691818237\n",
            "Epoch 48/50 | Batch 46/198 | Loss: 1.4069056510925293\n",
            "Epoch 48/50 | Batch 47/198 | Loss: 1.3383164405822754\n",
            "Epoch 48/50 | Batch 48/198 | Loss: 1.3800503015518188\n",
            "Epoch 48/50 | Batch 49/198 | Loss: 1.3432775735855103\n",
            "Epoch 48/50 | Batch 50/198 | Loss: 1.1811188459396362\n",
            "Epoch 48/50 | Batch 51/198 | Loss: 1.3096563816070557\n",
            "Epoch 48/50 | Batch 52/198 | Loss: 1.2272443771362305\n",
            "Epoch 48/50 | Batch 53/198 | Loss: 1.2706416845321655\n",
            "Epoch 48/50 | Batch 54/198 | Loss: 1.380521535873413\n",
            "Epoch 48/50 | Batch 55/198 | Loss: 1.3924016952514648\n",
            "Epoch 48/50 | Batch 56/198 | Loss: 1.4414881467819214\n",
            "Epoch 48/50 | Batch 57/198 | Loss: 1.3224483728408813\n",
            "Epoch 48/50 | Batch 58/198 | Loss: 1.4592372179031372\n",
            "Epoch 48/50 | Batch 59/198 | Loss: 1.4494792222976685\n",
            "Epoch 48/50 | Batch 60/198 | Loss: 1.4238271713256836\n",
            "Epoch 48/50 | Batch 61/198 | Loss: 1.3061800003051758\n",
            "Epoch 48/50 | Batch 62/198 | Loss: 1.4297512769699097\n",
            "Epoch 48/50 | Batch 63/198 | Loss: 1.496852159500122\n",
            "Epoch 48/50 | Batch 64/198 | Loss: 1.328962802886963\n",
            "Epoch 48/50 | Batch 65/198 | Loss: 1.2796070575714111\n",
            "Epoch 48/50 | Batch 66/198 | Loss: 1.3336520195007324\n",
            "Epoch 48/50 | Batch 67/198 | Loss: 1.1173754930496216\n",
            "Epoch 48/50 | Batch 68/198 | Loss: 1.5523505210876465\n",
            "Epoch 48/50 | Batch 69/198 | Loss: 1.4612765312194824\n",
            "Epoch 48/50 | Batch 70/198 | Loss: 1.238904356956482\n",
            "Epoch 48/50 | Batch 71/198 | Loss: 1.463092565536499\n",
            "Epoch 48/50 | Batch 72/198 | Loss: 1.4028986692428589\n",
            "Epoch 48/50 | Batch 73/198 | Loss: 1.3662139177322388\n",
            "Epoch 48/50 | Batch 74/198 | Loss: 1.4381927251815796\n",
            "Epoch 48/50 | Batch 75/198 | Loss: 1.5837862491607666\n",
            "Epoch 48/50 | Batch 76/198 | Loss: 1.374783992767334\n",
            "Epoch 48/50 | Batch 77/198 | Loss: 1.314511775970459\n",
            "Epoch 48/50 | Batch 78/198 | Loss: 1.523638367652893\n",
            "Epoch 48/50 | Batch 79/198 | Loss: 1.4670815467834473\n",
            "Epoch 48/50 | Batch 80/198 | Loss: 1.3046075105667114\n",
            "Epoch 48/50 | Batch 81/198 | Loss: 1.3781613111495972\n",
            "Epoch 48/50 | Batch 82/198 | Loss: 1.2744463682174683\n",
            "Epoch 48/50 | Batch 83/198 | Loss: 1.370937705039978\n",
            "Epoch 48/50 | Batch 84/198 | Loss: 1.224973440170288\n",
            "Epoch 48/50 | Batch 85/198 | Loss: 1.3365339040756226\n",
            "Epoch 48/50 | Batch 86/198 | Loss: 1.2728415727615356\n",
            "Epoch 48/50 | Batch 87/198 | Loss: 1.4414348602294922\n",
            "Epoch 48/50 | Batch 88/198 | Loss: 1.2397902011871338\n",
            "Epoch 48/50 | Batch 89/198 | Loss: 1.2614731788635254\n",
            "Epoch 48/50 | Batch 90/198 | Loss: 1.455505132675171\n",
            "Epoch 48/50 | Batch 91/198 | Loss: 1.4174879789352417\n",
            "Epoch 48/50 | Batch 92/198 | Loss: 1.2820792198181152\n",
            "Epoch 48/50 | Batch 93/198 | Loss: 1.4406206607818604\n",
            "Epoch 48/50 | Batch 94/198 | Loss: 1.2295949459075928\n",
            "Epoch 48/50 | Batch 95/198 | Loss: 1.2479760646820068\n",
            "Epoch 48/50 | Batch 96/198 | Loss: 1.3705127239227295\n",
            "Epoch 48/50 | Batch 97/198 | Loss: 1.384535312652588\n",
            "Epoch 48/50 | Batch 98/198 | Loss: 1.29475736618042\n",
            "Epoch 48/50 | Batch 99/198 | Loss: 1.3715283870697021\n",
            "Epoch 48/50 | Batch 100/198 | Loss: 1.349625587463379\n",
            "Epoch 48/50 | Batch 101/198 | Loss: 1.3924580812454224\n",
            "Epoch 48/50 | Batch 102/198 | Loss: 1.5080584287643433\n",
            "Epoch 48/50 | Batch 103/198 | Loss: 1.4230090379714966\n",
            "Epoch 48/50 | Batch 104/198 | Loss: 1.3980281352996826\n",
            "Epoch 48/50 | Batch 105/198 | Loss: 1.2187016010284424\n",
            "Epoch 48/50 | Batch 106/198 | Loss: 1.351764440536499\n",
            "Epoch 48/50 | Batch 107/198 | Loss: 1.267980694770813\n",
            "Epoch 48/50 | Batch 108/198 | Loss: 1.4311013221740723\n",
            "Epoch 48/50 | Batch 109/198 | Loss: 1.1635513305664062\n",
            "Epoch 48/50 | Batch 110/198 | Loss: 1.4430954456329346\n",
            "Epoch 48/50 | Batch 111/198 | Loss: 1.1958866119384766\n",
            "Epoch 48/50 | Batch 112/198 | Loss: 1.4456639289855957\n",
            "Epoch 48/50 | Batch 113/198 | Loss: 1.2648730278015137\n",
            "Epoch 48/50 | Batch 114/198 | Loss: 1.3278093338012695\n",
            "Epoch 48/50 | Batch 115/198 | Loss: 1.397053837776184\n",
            "Epoch 48/50 | Batch 116/198 | Loss: 1.325442910194397\n",
            "Epoch 48/50 | Batch 117/198 | Loss: 1.2225309610366821\n",
            "Epoch 48/50 | Batch 118/198 | Loss: 1.2900687456130981\n",
            "Epoch 48/50 | Batch 119/198 | Loss: 1.2281938791275024\n",
            "Epoch 48/50 | Batch 120/198 | Loss: 1.2678881883621216\n",
            "Epoch 48/50 | Batch 121/198 | Loss: 1.4478411674499512\n",
            "Epoch 48/50 | Batch 122/198 | Loss: 1.4821852445602417\n",
            "Epoch 48/50 | Batch 123/198 | Loss: 1.4088664054870605\n",
            "Epoch 48/50 | Batch 124/198 | Loss: 1.299481749534607\n",
            "Epoch 48/50 | Batch 125/198 | Loss: 1.230247974395752\n",
            "Epoch 48/50 | Batch 126/198 | Loss: 1.33043372631073\n",
            "Epoch 48/50 | Batch 127/198 | Loss: 1.4283198118209839\n",
            "Epoch 48/50 | Batch 128/198 | Loss: 1.2334250211715698\n",
            "Epoch 48/50 | Batch 129/198 | Loss: 1.344176173210144\n",
            "Epoch 48/50 | Batch 130/198 | Loss: 1.50984525680542\n",
            "Epoch 48/50 | Batch 131/198 | Loss: 1.522649884223938\n",
            "Epoch 48/50 | Batch 132/198 | Loss: 1.1775823831558228\n",
            "Epoch 48/50 | Batch 133/198 | Loss: 1.4553951025009155\n",
            "Epoch 48/50 | Batch 134/198 | Loss: 1.3495941162109375\n",
            "Epoch 48/50 | Batch 135/198 | Loss: 1.4881260395050049\n",
            "Epoch 48/50 | Batch 136/198 | Loss: 1.362241506576538\n",
            "Epoch 48/50 | Batch 137/198 | Loss: 1.248301386833191\n",
            "Epoch 48/50 | Batch 138/198 | Loss: 1.291226863861084\n",
            "Epoch 48/50 | Batch 139/198 | Loss: 1.325358510017395\n",
            "Epoch 48/50 | Batch 140/198 | Loss: 1.2507151365280151\n",
            "Epoch 48/50 | Batch 141/198 | Loss: 1.547302007675171\n",
            "Epoch 48/50 | Batch 142/198 | Loss: 1.3082636594772339\n",
            "Epoch 48/50 | Batch 143/198 | Loss: 1.5248879194259644\n",
            "Epoch 48/50 | Batch 144/198 | Loss: 1.4157840013504028\n",
            "Epoch 48/50 | Batch 145/198 | Loss: 1.3493432998657227\n",
            "Epoch 48/50 | Batch 146/198 | Loss: 1.2663918733596802\n",
            "Epoch 48/50 | Batch 147/198 | Loss: 1.3521358966827393\n",
            "Epoch 48/50 | Batch 148/198 | Loss: 1.3515323400497437\n",
            "Epoch 48/50 | Batch 149/198 | Loss: 1.2397570610046387\n",
            "Epoch 48/50 | Batch 150/198 | Loss: 1.382200002670288\n",
            "Epoch 48/50 | Batch 151/198 | Loss: 1.4044780731201172\n",
            "Epoch 48/50 | Batch 152/198 | Loss: 1.2486556768417358\n",
            "Epoch 48/50 | Batch 153/198 | Loss: 1.5886932611465454\n",
            "Epoch 48/50 | Batch 154/198 | Loss: 1.4769530296325684\n",
            "Epoch 48/50 | Batch 155/198 | Loss: 1.579513430595398\n",
            "Epoch 48/50 | Batch 156/198 | Loss: 1.4439148902893066\n",
            "Epoch 48/50 | Batch 157/198 | Loss: 1.4038728475570679\n",
            "Epoch 48/50 | Batch 158/198 | Loss: 1.3962243795394897\n",
            "Epoch 48/50 | Batch 159/198 | Loss: 1.3205188512802124\n",
            "Epoch 48/50 | Batch 160/198 | Loss: 1.4657506942749023\n",
            "Epoch 48/50 | Batch 161/198 | Loss: 1.4498881101608276\n",
            "Epoch 48/50 | Batch 162/198 | Loss: 1.3515121936798096\n",
            "Epoch 48/50 | Batch 163/198 | Loss: 1.407578468322754\n",
            "Epoch 48/50 | Batch 164/198 | Loss: 1.4145426750183105\n",
            "Epoch 48/50 | Batch 165/198 | Loss: 1.3994637727737427\n",
            "Epoch 48/50 | Batch 166/198 | Loss: 1.4823201894760132\n",
            "Epoch 48/50 | Batch 167/198 | Loss: 1.1039178371429443\n",
            "Epoch 48/50 | Batch 168/198 | Loss: 1.4781275987625122\n",
            "Epoch 48/50 | Batch 169/198 | Loss: 1.3147953748703003\n",
            "Epoch 48/50 | Batch 170/198 | Loss: 1.4641739130020142\n",
            "Epoch 48/50 | Batch 171/198 | Loss: 1.238710880279541\n",
            "Epoch 48/50 | Batch 172/198 | Loss: 1.4485691785812378\n",
            "Epoch 48/50 | Batch 173/198 | Loss: 1.3937016725540161\n",
            "Epoch 48/50 | Batch 174/198 | Loss: 1.3970110416412354\n",
            "Epoch 48/50 | Batch 175/198 | Loss: 1.2877765893936157\n",
            "Epoch 48/50 | Batch 176/198 | Loss: 1.4521713256835938\n",
            "Epoch 48/50 | Batch 177/198 | Loss: 1.3484933376312256\n",
            "Epoch 48/50 | Batch 178/198 | Loss: 1.3347423076629639\n",
            "Epoch 48/50 | Batch 179/198 | Loss: 1.525019645690918\n",
            "Epoch 48/50 | Batch 180/198 | Loss: 1.488533854484558\n",
            "Epoch 48/50 | Batch 181/198 | Loss: 1.2593802213668823\n",
            "Epoch 48/50 | Batch 182/198 | Loss: 1.385030746459961\n",
            "Epoch 48/50 | Batch 183/198 | Loss: 1.2767138481140137\n",
            "Epoch 48/50 | Batch 184/198 | Loss: 1.3865993022918701\n",
            "Epoch 48/50 | Batch 185/198 | Loss: 1.387587308883667\n",
            "Epoch 48/50 | Batch 186/198 | Loss: 1.3667734861373901\n",
            "Epoch 48/50 | Batch 187/198 | Loss: 1.5385732650756836\n",
            "Epoch 48/50 | Batch 188/198 | Loss: 1.3642163276672363\n",
            "Epoch 48/50 | Batch 189/198 | Loss: 1.3929253816604614\n",
            "Epoch 48/50 | Batch 190/198 | Loss: 1.4233624935150146\n",
            "Epoch 48/50 | Batch 191/198 | Loss: 1.4436384439468384\n",
            "Epoch 48/50 | Batch 192/198 | Loss: 1.174601674079895\n",
            "Epoch 48/50 | Batch 193/198 | Loss: 1.3729232549667358\n",
            "Epoch 48/50 | Batch 194/198 | Loss: 1.2704817056655884\n",
            "Epoch 48/50 | Batch 195/198 | Loss: 1.613101840019226\n",
            "Epoch 48/50 | Batch 196/198 | Loss: 1.3233387470245361\n",
            "Epoch 48/50 | Batch 197/198 | Loss: 1.4350204467773438\n",
            "Epoch 48/50 | Batch 198/198 | Loss: 1.2132784128189087\n",
            "Epoch 48/50 | Average Loss: 1.375580218705264\n",
            "Epoch 49/50 | Batch 1/198 | Loss: 1.3372493982315063\n",
            "Epoch 49/50 | Batch 2/198 | Loss: 1.4562724828720093\n",
            "Epoch 49/50 | Batch 3/198 | Loss: 1.3798683881759644\n",
            "Epoch 49/50 | Batch 4/198 | Loss: 1.6021021604537964\n",
            "Epoch 49/50 | Batch 5/198 | Loss: 1.4642142057418823\n",
            "Epoch 49/50 | Batch 6/198 | Loss: 1.5225727558135986\n",
            "Epoch 49/50 | Batch 7/198 | Loss: 1.454437017440796\n",
            "Epoch 49/50 | Batch 8/198 | Loss: 1.2601580619812012\n",
            "Epoch 49/50 | Batch 9/198 | Loss: 1.2735589742660522\n",
            "Epoch 49/50 | Batch 10/198 | Loss: 1.390064001083374\n",
            "Epoch 49/50 | Batch 11/198 | Loss: 1.3561869859695435\n",
            "Epoch 49/50 | Batch 12/198 | Loss: 1.2497221231460571\n",
            "Epoch 49/50 | Batch 13/198 | Loss: 1.4763225317001343\n",
            "Epoch 49/50 | Batch 14/198 | Loss: 1.4714670181274414\n",
            "Epoch 49/50 | Batch 15/198 | Loss: 1.383730173110962\n",
            "Epoch 49/50 | Batch 16/198 | Loss: 1.4234659671783447\n",
            "Epoch 49/50 | Batch 17/198 | Loss: 1.1381771564483643\n",
            "Epoch 49/50 | Batch 18/198 | Loss: 1.36722993850708\n",
            "Epoch 49/50 | Batch 19/198 | Loss: 1.6134099960327148\n",
            "Epoch 49/50 | Batch 20/198 | Loss: 1.4547302722930908\n",
            "Epoch 49/50 | Batch 21/198 | Loss: 1.2525595426559448\n",
            "Epoch 49/50 | Batch 22/198 | Loss: 1.4130949974060059\n",
            "Epoch 49/50 | Batch 23/198 | Loss: 1.4447076320648193\n",
            "Epoch 49/50 | Batch 24/198 | Loss: 1.30068039894104\n",
            "Epoch 49/50 | Batch 25/198 | Loss: 1.3101897239685059\n",
            "Epoch 49/50 | Batch 26/198 | Loss: 1.306178331375122\n",
            "Epoch 49/50 | Batch 27/198 | Loss: 1.255497694015503\n",
            "Epoch 49/50 | Batch 28/198 | Loss: 1.2553247213363647\n",
            "Epoch 49/50 | Batch 29/198 | Loss: 1.313765048980713\n",
            "Epoch 49/50 | Batch 30/198 | Loss: 1.3850690126419067\n",
            "Epoch 49/50 | Batch 31/198 | Loss: 1.4455586671829224\n",
            "Epoch 49/50 | Batch 32/198 | Loss: 1.3033474683761597\n",
            "Epoch 49/50 | Batch 33/198 | Loss: 1.21414315700531\n",
            "Epoch 49/50 | Batch 34/198 | Loss: 1.2596312761306763\n",
            "Epoch 49/50 | Batch 35/198 | Loss: 1.357621431350708\n",
            "Epoch 49/50 | Batch 36/198 | Loss: 1.3712798357009888\n",
            "Epoch 49/50 | Batch 37/198 | Loss: 1.3529092073440552\n",
            "Epoch 49/50 | Batch 38/198 | Loss: 1.4097306728363037\n",
            "Epoch 49/50 | Batch 39/198 | Loss: 1.3303954601287842\n",
            "Epoch 49/50 | Batch 40/198 | Loss: 1.5059987306594849\n",
            "Epoch 49/50 | Batch 41/198 | Loss: 1.3417729139328003\n",
            "Epoch 49/50 | Batch 42/198 | Loss: 1.4809999465942383\n",
            "Epoch 49/50 | Batch 43/198 | Loss: 1.3513338565826416\n",
            "Epoch 49/50 | Batch 44/198 | Loss: 1.4225133657455444\n",
            "Epoch 49/50 | Batch 45/198 | Loss: 1.441172480583191\n",
            "Epoch 49/50 | Batch 46/198 | Loss: 1.3917489051818848\n",
            "Epoch 49/50 | Batch 47/198 | Loss: 1.1056387424468994\n",
            "Epoch 49/50 | Batch 48/198 | Loss: 1.4411689043045044\n",
            "Epoch 49/50 | Batch 49/198 | Loss: 1.3845303058624268\n",
            "Epoch 49/50 | Batch 50/198 | Loss: 1.6586941480636597\n",
            "Epoch 49/50 | Batch 51/198 | Loss: 1.3942698240280151\n",
            "Epoch 49/50 | Batch 52/198 | Loss: 1.2517457008361816\n",
            "Epoch 49/50 | Batch 53/198 | Loss: 1.4257676601409912\n",
            "Epoch 49/50 | Batch 54/198 | Loss: 1.2287694215774536\n",
            "Epoch 49/50 | Batch 55/198 | Loss: 1.3928844928741455\n",
            "Epoch 49/50 | Batch 56/198 | Loss: 1.3607856035232544\n",
            "Epoch 49/50 | Batch 57/198 | Loss: 1.1394089460372925\n",
            "Epoch 49/50 | Batch 58/198 | Loss: 1.3067305088043213\n",
            "Epoch 49/50 | Batch 59/198 | Loss: 1.2776310443878174\n",
            "Epoch 49/50 | Batch 60/198 | Loss: 1.359959363937378\n",
            "Epoch 49/50 | Batch 61/198 | Loss: 1.4408767223358154\n",
            "Epoch 49/50 | Batch 62/198 | Loss: 1.3434183597564697\n",
            "Epoch 49/50 | Batch 63/198 | Loss: 1.4062879085540771\n",
            "Epoch 49/50 | Batch 64/198 | Loss: 1.3188248872756958\n",
            "Epoch 49/50 | Batch 65/198 | Loss: 1.348483681678772\n",
            "Epoch 49/50 | Batch 66/198 | Loss: 1.4765592813491821\n",
            "Epoch 49/50 | Batch 67/198 | Loss: 1.599092960357666\n",
            "Epoch 49/50 | Batch 68/198 | Loss: 1.20006263256073\n",
            "Epoch 49/50 | Batch 69/198 | Loss: 1.3063052892684937\n",
            "Epoch 49/50 | Batch 70/198 | Loss: 1.4020417928695679\n",
            "Epoch 49/50 | Batch 71/198 | Loss: 1.4799795150756836\n",
            "Epoch 49/50 | Batch 72/198 | Loss: 1.1690562963485718\n",
            "Epoch 49/50 | Batch 73/198 | Loss: 1.3030335903167725\n",
            "Epoch 49/50 | Batch 74/198 | Loss: 1.3054150342941284\n",
            "Epoch 49/50 | Batch 75/198 | Loss: 1.4076122045516968\n",
            "Epoch 49/50 | Batch 76/198 | Loss: 1.3474960327148438\n",
            "Epoch 49/50 | Batch 77/198 | Loss: 1.4453860521316528\n",
            "Epoch 49/50 | Batch 78/198 | Loss: 1.3954256772994995\n",
            "Epoch 49/50 | Batch 79/198 | Loss: 1.694493293762207\n",
            "Epoch 49/50 | Batch 80/198 | Loss: 1.1442017555236816\n",
            "Epoch 49/50 | Batch 81/198 | Loss: 1.3608795404434204\n",
            "Epoch 49/50 | Batch 82/198 | Loss: 1.408943772315979\n",
            "Epoch 49/50 | Batch 83/198 | Loss: 1.3752307891845703\n",
            "Epoch 49/50 | Batch 84/198 | Loss: 1.4567691087722778\n",
            "Epoch 49/50 | Batch 85/198 | Loss: 1.2585448026657104\n",
            "Epoch 49/50 | Batch 86/198 | Loss: 1.4931780099868774\n",
            "Epoch 49/50 | Batch 87/198 | Loss: 1.4132510423660278\n",
            "Epoch 49/50 | Batch 88/198 | Loss: 1.323825478553772\n",
            "Epoch 49/50 | Batch 89/198 | Loss: 1.4141814708709717\n",
            "Epoch 49/50 | Batch 90/198 | Loss: 1.3171521425247192\n",
            "Epoch 49/50 | Batch 91/198 | Loss: 1.307921290397644\n",
            "Epoch 49/50 | Batch 92/198 | Loss: 1.403205394744873\n",
            "Epoch 49/50 | Batch 93/198 | Loss: 1.3735438585281372\n",
            "Epoch 49/50 | Batch 94/198 | Loss: 1.3009928464889526\n",
            "Epoch 49/50 | Batch 95/198 | Loss: 1.388195514678955\n",
            "Epoch 49/50 | Batch 96/198 | Loss: 1.3495478630065918\n",
            "Epoch 49/50 | Batch 97/198 | Loss: 1.161314845085144\n",
            "Epoch 49/50 | Batch 98/198 | Loss: 1.0981518030166626\n",
            "Epoch 49/50 | Batch 99/198 | Loss: 1.383576512336731\n",
            "Epoch 49/50 | Batch 100/198 | Loss: 1.408969759941101\n",
            "Epoch 49/50 | Batch 101/198 | Loss: 1.3253099918365479\n",
            "Epoch 49/50 | Batch 102/198 | Loss: 1.2647737264633179\n",
            "Epoch 49/50 | Batch 103/198 | Loss: 1.391152262687683\n",
            "Epoch 49/50 | Batch 104/198 | Loss: 1.4768773317337036\n",
            "Epoch 49/50 | Batch 105/198 | Loss: 1.4799960851669312\n",
            "Epoch 49/50 | Batch 106/198 | Loss: 1.4932138919830322\n",
            "Epoch 49/50 | Batch 107/198 | Loss: 1.3396307229995728\n",
            "Epoch 49/50 | Batch 108/198 | Loss: 1.488173007965088\n",
            "Epoch 49/50 | Batch 109/198 | Loss: 1.4253880977630615\n",
            "Epoch 49/50 | Batch 110/198 | Loss: 1.328572392463684\n",
            "Epoch 49/50 | Batch 111/198 | Loss: 1.4112530946731567\n",
            "Epoch 49/50 | Batch 112/198 | Loss: 1.363879919052124\n",
            "Epoch 49/50 | Batch 113/198 | Loss: 1.4777607917785645\n",
            "Epoch 49/50 | Batch 114/198 | Loss: 1.1938375234603882\n",
            "Epoch 49/50 | Batch 115/198 | Loss: 1.365337610244751\n",
            "Epoch 49/50 | Batch 116/198 | Loss: 1.3052760362625122\n",
            "Epoch 49/50 | Batch 117/198 | Loss: 1.239099383354187\n",
            "Epoch 49/50 | Batch 118/198 | Loss: 1.2576828002929688\n",
            "Epoch 49/50 | Batch 119/198 | Loss: 1.239524245262146\n",
            "Epoch 49/50 | Batch 120/198 | Loss: 1.4217801094055176\n",
            "Epoch 49/50 | Batch 121/198 | Loss: 1.4087414741516113\n",
            "Epoch 49/50 | Batch 122/198 | Loss: 1.409429907798767\n",
            "Epoch 49/50 | Batch 123/198 | Loss: 1.4128804206848145\n",
            "Epoch 49/50 | Batch 124/198 | Loss: 1.3723844289779663\n",
            "Epoch 49/50 | Batch 125/198 | Loss: 1.2823514938354492\n",
            "Epoch 49/50 | Batch 126/198 | Loss: 1.491798996925354\n",
            "Epoch 49/50 | Batch 127/198 | Loss: 1.5010722875595093\n",
            "Epoch 49/50 | Batch 128/198 | Loss: 1.5205402374267578\n",
            "Epoch 49/50 | Batch 129/198 | Loss: 1.3773218393325806\n",
            "Epoch 49/50 | Batch 130/198 | Loss: 1.4305344820022583\n",
            "Epoch 49/50 | Batch 131/198 | Loss: 1.2084283828735352\n",
            "Epoch 49/50 | Batch 132/198 | Loss: 1.1956465244293213\n",
            "Epoch 49/50 | Batch 133/198 | Loss: 1.3351640701293945\n",
            "Epoch 49/50 | Batch 134/198 | Loss: 1.3581130504608154\n",
            "Epoch 49/50 | Batch 135/198 | Loss: 1.3032034635543823\n",
            "Epoch 49/50 | Batch 136/198 | Loss: 1.2985528707504272\n",
            "Epoch 49/50 | Batch 137/198 | Loss: 1.3055037260055542\n",
            "Epoch 49/50 | Batch 138/198 | Loss: 1.6178594827651978\n",
            "Epoch 49/50 | Batch 139/198 | Loss: 1.465592622756958\n",
            "Epoch 49/50 | Batch 140/198 | Loss: 1.4995671510696411\n",
            "Epoch 49/50 | Batch 141/198 | Loss: 1.4634793996810913\n",
            "Epoch 49/50 | Batch 142/198 | Loss: 1.389979600906372\n",
            "Epoch 49/50 | Batch 143/198 | Loss: 1.219058632850647\n",
            "Epoch 49/50 | Batch 144/198 | Loss: 1.3726210594177246\n",
            "Epoch 49/50 | Batch 145/198 | Loss: 1.3802824020385742\n",
            "Epoch 49/50 | Batch 146/198 | Loss: 1.197148323059082\n",
            "Epoch 49/50 | Batch 147/198 | Loss: 1.0252833366394043\n",
            "Epoch 49/50 | Batch 148/198 | Loss: 1.2528542280197144\n",
            "Epoch 49/50 | Batch 149/198 | Loss: 1.129663348197937\n",
            "Epoch 49/50 | Batch 150/198 | Loss: 1.412803292274475\n",
            "Epoch 49/50 | Batch 151/198 | Loss: 1.3519999980926514\n",
            "Epoch 49/50 | Batch 152/198 | Loss: 1.3764971494674683\n",
            "Epoch 49/50 | Batch 153/198 | Loss: 1.3235397338867188\n",
            "Epoch 49/50 | Batch 154/198 | Loss: 1.4713953733444214\n",
            "Epoch 49/50 | Batch 155/198 | Loss: 1.3905376195907593\n",
            "Epoch 49/50 | Batch 156/198 | Loss: 1.1967288255691528\n",
            "Epoch 49/50 | Batch 157/198 | Loss: 1.1324992179870605\n",
            "Epoch 49/50 | Batch 158/198 | Loss: 1.5092716217041016\n",
            "Epoch 49/50 | Batch 159/198 | Loss: 1.4411975145339966\n",
            "Epoch 49/50 | Batch 160/198 | Loss: 1.458289623260498\n",
            "Epoch 49/50 | Batch 161/198 | Loss: 1.5284602642059326\n",
            "Epoch 49/50 | Batch 162/198 | Loss: 1.4725335836410522\n",
            "Epoch 49/50 | Batch 163/198 | Loss: 1.252924919128418\n",
            "Epoch 49/50 | Batch 164/198 | Loss: 1.3714717626571655\n",
            "Epoch 49/50 | Batch 165/198 | Loss: 1.2698605060577393\n",
            "Epoch 49/50 | Batch 166/198 | Loss: 1.4043341875076294\n",
            "Epoch 49/50 | Batch 167/198 | Loss: 1.4074163436889648\n",
            "Epoch 49/50 | Batch 168/198 | Loss: 1.4326719045639038\n",
            "Epoch 49/50 | Batch 169/198 | Loss: 1.4066237211227417\n",
            "Epoch 49/50 | Batch 170/198 | Loss: 1.3759434223175049\n",
            "Epoch 49/50 | Batch 171/198 | Loss: 1.1974611282348633\n",
            "Epoch 49/50 | Batch 172/198 | Loss: 1.4210705757141113\n",
            "Epoch 49/50 | Batch 173/198 | Loss: 1.35563063621521\n",
            "Epoch 49/50 | Batch 174/198 | Loss: 1.5417851209640503\n",
            "Epoch 49/50 | Batch 175/198 | Loss: 1.5081034898757935\n",
            "Epoch 49/50 | Batch 176/198 | Loss: 1.435765027999878\n",
            "Epoch 49/50 | Batch 177/198 | Loss: 1.479038953781128\n",
            "Epoch 49/50 | Batch 178/198 | Loss: 1.4009349346160889\n",
            "Epoch 49/50 | Batch 179/198 | Loss: 1.2834335565567017\n",
            "Epoch 49/50 | Batch 180/198 | Loss: 1.337871789932251\n",
            "Epoch 49/50 | Batch 181/198 | Loss: 1.5028431415557861\n",
            "Epoch 49/50 | Batch 182/198 | Loss: 1.563067078590393\n",
            "Epoch 49/50 | Batch 183/198 | Loss: 1.3904362916946411\n",
            "Epoch 49/50 | Batch 184/198 | Loss: 1.178417444229126\n",
            "Epoch 49/50 | Batch 185/198 | Loss: 1.5377106666564941\n",
            "Epoch 49/50 | Batch 186/198 | Loss: 1.230899691581726\n",
            "Epoch 49/50 | Batch 187/198 | Loss: 1.4492952823638916\n",
            "Epoch 49/50 | Batch 188/198 | Loss: 1.4452584981918335\n",
            "Epoch 49/50 | Batch 189/198 | Loss: 1.3061360120773315\n",
            "Epoch 49/50 | Batch 190/198 | Loss: 1.4141641855239868\n",
            "Epoch 49/50 | Batch 191/198 | Loss: 1.375\n",
            "Epoch 49/50 | Batch 192/198 | Loss: 1.2053775787353516\n",
            "Epoch 49/50 | Batch 193/198 | Loss: 1.6484793424606323\n",
            "Epoch 49/50 | Batch 194/198 | Loss: 1.406075358390808\n",
            "Epoch 49/50 | Batch 195/198 | Loss: 1.5958431959152222\n",
            "Epoch 49/50 | Batch 196/198 | Loss: 1.347404956817627\n",
            "Epoch 49/50 | Batch 197/198 | Loss: 1.425546646118164\n",
            "Epoch 49/50 | Batch 198/198 | Loss: 1.472902536392212\n",
            "Epoch 49/50 | Average Loss: 1.3712530683989477\n",
            "Epoch 50/50 | Batch 1/198 | Loss: 1.4787731170654297\n",
            "Epoch 50/50 | Batch 2/198 | Loss: 1.2140142917633057\n",
            "Epoch 50/50 | Batch 3/198 | Loss: 1.4011050462722778\n",
            "Epoch 50/50 | Batch 4/198 | Loss: 1.1898176670074463\n",
            "Epoch 50/50 | Batch 5/198 | Loss: 1.7361180782318115\n",
            "Epoch 50/50 | Batch 6/198 | Loss: 1.4191988706588745\n",
            "Epoch 50/50 | Batch 7/198 | Loss: 1.3999170064926147\n",
            "Epoch 50/50 | Batch 8/198 | Loss: 1.2867697477340698\n",
            "Epoch 50/50 | Batch 9/198 | Loss: 1.34439218044281\n",
            "Epoch 50/50 | Batch 10/198 | Loss: 1.4221546649932861\n",
            "Epoch 50/50 | Batch 11/198 | Loss: 1.3366330862045288\n",
            "Epoch 50/50 | Batch 12/198 | Loss: 1.0988606214523315\n",
            "Epoch 50/50 | Batch 13/198 | Loss: 1.3790024518966675\n",
            "Epoch 50/50 | Batch 14/198 | Loss: 1.5026346445083618\n",
            "Epoch 50/50 | Batch 15/198 | Loss: 1.352247714996338\n",
            "Epoch 50/50 | Batch 16/198 | Loss: 1.3798245191574097\n",
            "Epoch 50/50 | Batch 17/198 | Loss: 1.4775317907333374\n",
            "Epoch 50/50 | Batch 18/198 | Loss: 1.3087962865829468\n",
            "Epoch 50/50 | Batch 19/198 | Loss: 1.4047863483428955\n",
            "Epoch 50/50 | Batch 20/198 | Loss: 1.4565329551696777\n",
            "Epoch 50/50 | Batch 21/198 | Loss: 1.3247977495193481\n",
            "Epoch 50/50 | Batch 22/198 | Loss: 1.2878912687301636\n",
            "Epoch 50/50 | Batch 23/198 | Loss: 1.3748186826705933\n",
            "Epoch 50/50 | Batch 24/198 | Loss: 1.3891363143920898\n",
            "Epoch 50/50 | Batch 25/198 | Loss: 1.5157567262649536\n",
            "Epoch 50/50 | Batch 26/198 | Loss: 1.444214105606079\n",
            "Epoch 50/50 | Batch 27/198 | Loss: 1.4765211343765259\n",
            "Epoch 50/50 | Batch 28/198 | Loss: 1.438374638557434\n",
            "Epoch 50/50 | Batch 29/198 | Loss: 1.3668880462646484\n",
            "Epoch 50/50 | Batch 30/198 | Loss: 1.187824010848999\n",
            "Epoch 50/50 | Batch 31/198 | Loss: 1.4021430015563965\n",
            "Epoch 50/50 | Batch 32/198 | Loss: 1.3073194026947021\n",
            "Epoch 50/50 | Batch 33/198 | Loss: 1.1519503593444824\n",
            "Epoch 50/50 | Batch 34/198 | Loss: 1.557234525680542\n",
            "Epoch 50/50 | Batch 35/198 | Loss: 1.2775427103042603\n",
            "Epoch 50/50 | Batch 36/198 | Loss: 1.3950221538543701\n",
            "Epoch 50/50 | Batch 37/198 | Loss: 1.32086181640625\n",
            "Epoch 50/50 | Batch 38/198 | Loss: 1.2543667554855347\n",
            "Epoch 50/50 | Batch 39/198 | Loss: 1.2685805559158325\n",
            "Epoch 50/50 | Batch 40/198 | Loss: 1.3141483068466187\n",
            "Epoch 50/50 | Batch 41/198 | Loss: 1.1720350980758667\n",
            "Epoch 50/50 | Batch 42/198 | Loss: 1.4197584390640259\n",
            "Epoch 50/50 | Batch 43/198 | Loss: 1.4400285482406616\n",
            "Epoch 50/50 | Batch 44/198 | Loss: 1.261184573173523\n",
            "Epoch 50/50 | Batch 45/198 | Loss: 1.3332723379135132\n",
            "Epoch 50/50 | Batch 46/198 | Loss: 1.416113018989563\n",
            "Epoch 50/50 | Batch 47/198 | Loss: 1.450567603111267\n",
            "Epoch 50/50 | Batch 48/198 | Loss: 1.573915719985962\n",
            "Epoch 50/50 | Batch 49/198 | Loss: 1.3597469329833984\n",
            "Epoch 50/50 | Batch 50/198 | Loss: 1.355190396308899\n",
            "Epoch 50/50 | Batch 51/198 | Loss: 1.2560006380081177\n",
            "Epoch 50/50 | Batch 52/198 | Loss: 1.388472080230713\n",
            "Epoch 50/50 | Batch 53/198 | Loss: 1.276795744895935\n",
            "Epoch 50/50 | Batch 54/198 | Loss: 1.4211580753326416\n",
            "Epoch 50/50 | Batch 55/198 | Loss: 1.5128422975540161\n",
            "Epoch 50/50 | Batch 56/198 | Loss: 1.2917500734329224\n",
            "Epoch 50/50 | Batch 57/198 | Loss: 1.4488462209701538\n",
            "Epoch 50/50 | Batch 58/198 | Loss: 1.3166335821151733\n",
            "Epoch 50/50 | Batch 59/198 | Loss: 1.4325922727584839\n",
            "Epoch 50/50 | Batch 60/198 | Loss: 1.210949420928955\n",
            "Epoch 50/50 | Batch 61/198 | Loss: 1.4046635627746582\n",
            "Epoch 50/50 | Batch 62/198 | Loss: 1.379319667816162\n",
            "Epoch 50/50 | Batch 63/198 | Loss: 1.5335346460342407\n",
            "Epoch 50/50 | Batch 64/198 | Loss: 1.2319055795669556\n",
            "Epoch 50/50 | Batch 65/198 | Loss: 1.2685655355453491\n",
            "Epoch 50/50 | Batch 66/198 | Loss: 1.2550876140594482\n",
            "Epoch 50/50 | Batch 67/198 | Loss: 1.4441927671432495\n",
            "Epoch 50/50 | Batch 68/198 | Loss: 1.5097142457962036\n",
            "Epoch 50/50 | Batch 69/198 | Loss: 1.2708871364593506\n",
            "Epoch 50/50 | Batch 70/198 | Loss: 1.3012255430221558\n",
            "Epoch 50/50 | Batch 71/198 | Loss: 1.3838388919830322\n",
            "Epoch 50/50 | Batch 72/198 | Loss: 1.3347954750061035\n",
            "Epoch 50/50 | Batch 73/198 | Loss: 1.0740938186645508\n",
            "Epoch 50/50 | Batch 74/198 | Loss: 1.495604395866394\n",
            "Epoch 50/50 | Batch 75/198 | Loss: 1.523066520690918\n",
            "Epoch 50/50 | Batch 76/198 | Loss: 1.3916317224502563\n",
            "Epoch 50/50 | Batch 77/198 | Loss: 1.3378461599349976\n",
            "Epoch 50/50 | Batch 78/198 | Loss: 1.2177714109420776\n",
            "Epoch 50/50 | Batch 79/198 | Loss: 1.372069239616394\n",
            "Epoch 50/50 | Batch 80/198 | Loss: 1.316019058227539\n",
            "Epoch 50/50 | Batch 81/198 | Loss: 1.1342214345932007\n",
            "Epoch 50/50 | Batch 82/198 | Loss: 1.170674204826355\n",
            "Epoch 50/50 | Batch 83/198 | Loss: 1.5487505197525024\n",
            "Epoch 50/50 | Batch 84/198 | Loss: 1.3246166706085205\n",
            "Epoch 50/50 | Batch 85/198 | Loss: 1.4871959686279297\n",
            "Epoch 50/50 | Batch 86/198 | Loss: 1.096966028213501\n",
            "Epoch 50/50 | Batch 87/198 | Loss: 1.5715183019638062\n",
            "Epoch 50/50 | Batch 88/198 | Loss: 1.430955410003662\n",
            "Epoch 50/50 | Batch 89/198 | Loss: 1.4328184127807617\n",
            "Epoch 50/50 | Batch 90/198 | Loss: 1.266977071762085\n",
            "Epoch 50/50 | Batch 91/198 | Loss: 1.5092638731002808\n",
            "Epoch 50/50 | Batch 92/198 | Loss: 1.3986390829086304\n",
            "Epoch 50/50 | Batch 93/198 | Loss: 1.4837952852249146\n",
            "Epoch 50/50 | Batch 94/198 | Loss: 1.1464773416519165\n",
            "Epoch 50/50 | Batch 95/198 | Loss: 1.2420458793640137\n",
            "Epoch 50/50 | Batch 96/198 | Loss: 1.3067209720611572\n",
            "Epoch 50/50 | Batch 97/198 | Loss: 1.4639005661010742\n",
            "Epoch 50/50 | Batch 98/198 | Loss: 1.3409911394119263\n",
            "Epoch 50/50 | Batch 99/198 | Loss: 1.2799737453460693\n",
            "Epoch 50/50 | Batch 100/198 | Loss: 1.4226887226104736\n",
            "Epoch 50/50 | Batch 101/198 | Loss: 1.3511613607406616\n",
            "Epoch 50/50 | Batch 102/198 | Loss: 1.4909464120864868\n",
            "Epoch 50/50 | Batch 103/198 | Loss: 1.3025070428848267\n",
            "Epoch 50/50 | Batch 104/198 | Loss: 1.3434525728225708\n",
            "Epoch 50/50 | Batch 105/198 | Loss: 1.2731236219406128\n",
            "Epoch 50/50 | Batch 106/198 | Loss: 1.3818483352661133\n",
            "Epoch 50/50 | Batch 107/198 | Loss: 1.4230612516403198\n",
            "Epoch 50/50 | Batch 108/198 | Loss: 1.3869868516921997\n",
            "Epoch 50/50 | Batch 109/198 | Loss: 1.2361129522323608\n",
            "Epoch 50/50 | Batch 110/198 | Loss: 1.261988878250122\n",
            "Epoch 50/50 | Batch 111/198 | Loss: 1.2183090448379517\n",
            "Epoch 50/50 | Batch 112/198 | Loss: 1.5806523561477661\n",
            "Epoch 50/50 | Batch 113/198 | Loss: 1.1152079105377197\n",
            "Epoch 50/50 | Batch 114/198 | Loss: 1.337025761604309\n",
            "Epoch 50/50 | Batch 115/198 | Loss: 1.3241307735443115\n",
            "Epoch 50/50 | Batch 116/198 | Loss: 1.1950315237045288\n",
            "Epoch 50/50 | Batch 117/198 | Loss: 1.4284952878952026\n",
            "Epoch 50/50 | Batch 118/198 | Loss: 1.2433340549468994\n",
            "Epoch 50/50 | Batch 119/198 | Loss: 1.3333759307861328\n",
            "Epoch 50/50 | Batch 120/198 | Loss: 1.46646249294281\n",
            "Epoch 50/50 | Batch 121/198 | Loss: 1.307417392730713\n",
            "Epoch 50/50 | Batch 122/198 | Loss: 1.3492558002471924\n",
            "Epoch 50/50 | Batch 123/198 | Loss: 1.40346097946167\n",
            "Epoch 50/50 | Batch 124/198 | Loss: 1.3339565992355347\n",
            "Epoch 50/50 | Batch 125/198 | Loss: 1.282037377357483\n",
            "Epoch 50/50 | Batch 126/198 | Loss: 1.4299075603485107\n",
            "Epoch 50/50 | Batch 127/198 | Loss: 1.5030544996261597\n",
            "Epoch 50/50 | Batch 128/198 | Loss: 1.4433934688568115\n",
            "Epoch 50/50 | Batch 129/198 | Loss: 1.4205400943756104\n",
            "Epoch 50/50 | Batch 130/198 | Loss: 1.3017926216125488\n",
            "Epoch 50/50 | Batch 131/198 | Loss: 1.549959421157837\n",
            "Epoch 50/50 | Batch 132/198 | Loss: 1.280983567237854\n",
            "Epoch 50/50 | Batch 133/198 | Loss: 1.4958573579788208\n",
            "Epoch 50/50 | Batch 134/198 | Loss: 1.46461021900177\n",
            "Epoch 50/50 | Batch 135/198 | Loss: 1.3528283834457397\n",
            "Epoch 50/50 | Batch 136/198 | Loss: 1.2978360652923584\n",
            "Epoch 50/50 | Batch 137/198 | Loss: 1.236122965812683\n",
            "Epoch 50/50 | Batch 138/198 | Loss: 1.2271370887756348\n",
            "Epoch 50/50 | Batch 139/198 | Loss: 1.2460535764694214\n",
            "Epoch 50/50 | Batch 140/198 | Loss: 1.569810390472412\n",
            "Epoch 50/50 | Batch 141/198 | Loss: 1.2599389553070068\n",
            "Epoch 50/50 | Batch 142/198 | Loss: 1.5050365924835205\n",
            "Epoch 50/50 | Batch 143/198 | Loss: 1.397039771080017\n",
            "Epoch 50/50 | Batch 144/198 | Loss: 1.3515748977661133\n",
            "Epoch 50/50 | Batch 145/198 | Loss: 1.4384368658065796\n",
            "Epoch 50/50 | Batch 146/198 | Loss: 1.4127956628799438\n",
            "Epoch 50/50 | Batch 147/198 | Loss: 1.3139666318893433\n",
            "Epoch 50/50 | Batch 148/198 | Loss: 1.471205472946167\n",
            "Epoch 50/50 | Batch 149/198 | Loss: 1.3063902854919434\n",
            "Epoch 50/50 | Batch 150/198 | Loss: 1.278785228729248\n",
            "Epoch 50/50 | Batch 151/198 | Loss: 1.3277662992477417\n",
            "Epoch 50/50 | Batch 152/198 | Loss: 1.3183153867721558\n",
            "Epoch 50/50 | Batch 153/198 | Loss: 1.480715036392212\n",
            "Epoch 50/50 | Batch 154/198 | Loss: 1.4231749773025513\n",
            "Epoch 50/50 | Batch 155/198 | Loss: 1.4151320457458496\n",
            "Epoch 50/50 | Batch 156/198 | Loss: 1.4712082147598267\n",
            "Epoch 50/50 | Batch 157/198 | Loss: 1.2625792026519775\n",
            "Epoch 50/50 | Batch 158/198 | Loss: 1.4467458724975586\n",
            "Epoch 50/50 | Batch 159/198 | Loss: 1.5785107612609863\n",
            "Epoch 50/50 | Batch 160/198 | Loss: 1.4921962022781372\n",
            "Epoch 50/50 | Batch 161/198 | Loss: 1.1100335121154785\n",
            "Epoch 50/50 | Batch 162/198 | Loss: 1.4042842388153076\n",
            "Epoch 50/50 | Batch 163/198 | Loss: 1.4571489095687866\n",
            "Epoch 50/50 | Batch 164/198 | Loss: 1.480031132698059\n",
            "Epoch 50/50 | Batch 165/198 | Loss: 1.1472928524017334\n",
            "Epoch 50/50 | Batch 166/198 | Loss: 1.5993307828903198\n",
            "Epoch 50/50 | Batch 167/198 | Loss: 1.3138902187347412\n",
            "Epoch 50/50 | Batch 168/198 | Loss: 1.4224790334701538\n",
            "Epoch 50/50 | Batch 169/198 | Loss: 1.4969955682754517\n",
            "Epoch 50/50 | Batch 170/198 | Loss: 1.383750319480896\n",
            "Epoch 50/50 | Batch 171/198 | Loss: 1.3486623764038086\n",
            "Epoch 50/50 | Batch 172/198 | Loss: 1.4162198305130005\n",
            "Epoch 50/50 | Batch 173/198 | Loss: 1.5406019687652588\n",
            "Epoch 50/50 | Batch 174/198 | Loss: 1.3923486471176147\n",
            "Epoch 50/50 | Batch 175/198 | Loss: 1.480581521987915\n",
            "Epoch 50/50 | Batch 176/198 | Loss: 1.2622461318969727\n",
            "Epoch 50/50 | Batch 177/198 | Loss: 1.4177019596099854\n",
            "Epoch 50/50 | Batch 178/198 | Loss: 1.1009609699249268\n",
            "Epoch 50/50 | Batch 179/198 | Loss: 1.582209825515747\n",
            "Epoch 50/50 | Batch 180/198 | Loss: 1.428186297416687\n",
            "Epoch 50/50 | Batch 181/198 | Loss: 1.550302505493164\n",
            "Epoch 50/50 | Batch 182/198 | Loss: 1.2721375226974487\n",
            "Epoch 50/50 | Batch 183/198 | Loss: 1.3967416286468506\n",
            "Epoch 50/50 | Batch 184/198 | Loss: 1.534045934677124\n",
            "Epoch 50/50 | Batch 185/198 | Loss: 1.2679308652877808\n",
            "Epoch 50/50 | Batch 186/198 | Loss: 1.2526112794876099\n",
            "Epoch 50/50 | Batch 187/198 | Loss: 1.4264322519302368\n",
            "Epoch 50/50 | Batch 188/198 | Loss: 1.3490906953811646\n",
            "Epoch 50/50 | Batch 189/198 | Loss: 1.4276071786880493\n",
            "Epoch 50/50 | Batch 190/198 | Loss: 1.3276418447494507\n",
            "Epoch 50/50 | Batch 191/198 | Loss: 1.2737040519714355\n",
            "Epoch 50/50 | Batch 192/198 | Loss: 1.3200552463531494\n",
            "Epoch 50/50 | Batch 193/198 | Loss: 1.3819143772125244\n",
            "Epoch 50/50 | Batch 194/198 | Loss: 1.3581329584121704\n",
            "Epoch 50/50 | Batch 195/198 | Loss: 1.2360368967056274\n",
            "Epoch 50/50 | Batch 196/198 | Loss: 1.4285976886749268\n",
            "Epoch 50/50 | Batch 197/198 | Loss: 1.3065602779388428\n",
            "Epoch 50/50 | Batch 198/198 | Loss: 1.5885415077209473\n",
            "Epoch 50/50 | Average Loss: 1.3672281064168372\n",
            "No complete batches remaining after filtering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Books/trained_model'\n",
        "\n",
        "# Save the trained model\n",
        "model.save_pretrained(path)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(\"path\")\n",
        "\n",
        "# Save the model configuration\n",
        "model.config.save_pretrained(path)\n",
        "\n",
        "# Save other relevant components\n",
        "torch.save(optimizer.state_dict(),path + \"/optimizer.pth\")\n",
        "torch.save(scheduler.state_dict(), path+\"/scheduler.pth\")"
      ],
      "metadata": {
        "id": "hphyqgTn7DIn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(average_loss_p)):\n",
        "    loss = str(average_loss_p[i])\n",
        "    print(\"For Epoch {} Avg loss is {}\".format(i + 1, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKoQZRn_G3B2",
        "outputId": "d9c9b15c-6bb3-425d-b072-831961b66ffa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Epoch 1 Avg loss is 4.4664851619739725\n",
            "For Epoch 2 Avg loss is 4.280565104099235\n",
            "For Epoch 3 Avg loss is 4.23955447264392\n",
            "For Epoch 4 Avg loss is 4.198595897115842\n",
            "For Epoch 5 Avg loss is 4.14556430445777\n",
            "For Epoch 6 Avg loss is 4.075488082086197\n",
            "For Epoch 7 Avg loss is 3.984121063742975\n",
            "For Epoch 8 Avg loss is 3.870536218989979\n",
            "For Epoch 9 Avg loss is 3.735497242272502\n",
            "For Epoch 10 Avg loss is 3.584352909916579\n",
            "For Epoch 11 Avg loss is 3.422223590841197\n",
            "For Epoch 12 Avg loss is 3.25636736672334\n",
            "For Epoch 13 Avg loss is 3.0883133893061165\n",
            "For Epoch 14 Avg loss is 2.9247889928143436\n",
            "For Epoch 15 Avg loss is 2.766388942496945\n",
            "For Epoch 16 Avg loss is 2.6202012408863413\n",
            "For Epoch 17 Avg loss is 2.4884209109075144\n",
            "For Epoch 18 Avg loss is 2.366399875794998\n",
            "For Epoch 19 Avg loss is 2.259764075279236\n",
            "For Epoch 20 Avg loss is 2.1642836675499426\n",
            "For Epoch 21 Avg loss is 2.078123009566105\n",
            "For Epoch 22 Avg loss is 2.001676698525747\n",
            "For Epoch 23 Avg loss is 1.9332367747721046\n",
            "For Epoch 24 Avg loss is 1.8722340753584197\n",
            "For Epoch 25 Avg loss is 1.8180645872848202\n",
            "For Epoch 26 Avg loss is 1.7694416937201913\n",
            "For Epoch 27 Avg loss is 1.726009730738823\n",
            "For Epoch 28 Avg loss is 1.686755693320072\n",
            "For Epoch 29 Avg loss is 1.651652622102487\n",
            "For Epoch 30 Avg loss is 1.6201749961785596\n",
            "For Epoch 31 Avg loss is 1.5917201596077042\n",
            "For Epoch 32 Avg loss is 1.5663394374076767\n",
            "For Epoch 33 Avg loss is 1.5434154506885644\n",
            "For Epoch 34 Avg loss is 1.5226157301604146\n",
            "For Epoch 35 Avg loss is 1.5039457185099823\n",
            "For Epoch 36 Avg loss is 1.4872599629440693\n",
            "For Epoch 37 Avg loss is 1.4721241569278216\n",
            "For Epoch 38 Avg loss is 1.4581654083849205\n",
            "For Epoch 39 Avg loss is 1.4458704705190177\n",
            "For Epoch 40 Avg loss is 1.4345482858744534\n",
            "For Epoch 41 Avg loss is 1.4240638806362345\n",
            "For Epoch 42 Avg loss is 1.4151631611766238\n",
            "For Epoch 43 Avg loss is 1.406626450895059\n",
            "For Epoch 44 Avg loss is 1.3990410156924316\n",
            "For Epoch 45 Avg loss is 1.3923793519386138\n",
            "For Epoch 46 Avg loss is 1.3861371670106444\n",
            "For Epoch 47 Avg loss is 1.3806814563394798\n",
            "For Epoch 48 Avg loss is 1.375580218705264\n",
            "For Epoch 49 Avg loss is 1.3712530683989477\n",
            "For Epoch 50 Avg loss is 1.3672281064168372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained('trained_model')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('trained_model')\n",
        "\n",
        "# Set the device for inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Set the maximum length of generated text\n",
        "max_length = 50\n",
        "\n",
        "# Set the prompt or input text\n",
        "prompt = \"thanks\"\n",
        "\n",
        "# Tokenize the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0gBOvFFYeq0",
        "outputId": "ec0b420c-6e3d-469e-98a0-3f8a927c2558"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: thanks. you always end stories\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained('trained_model')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('trained_model')\n",
        "\n",
        "# Set the device for inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Set the maximum length of generated text\n",
        "max_length = 50\n",
        "\n",
        "# Set the prompt or input text\n",
        "prompt = \"chat with me\"\n",
        "\n",
        "# Tokenize the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQZK56vsdM_K",
        "outputId": "30075b9f-b695-4280-d9b6-03cf2addc678"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: chat with me for doing!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained('trained_model')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('trained_model')\n",
        "\n",
        "# Set the device for inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Set the maximum length of generated text\n",
        "max_length = 50\n",
        "\n",
        "# Set the prompt or input text\n",
        "prompt = \"do you have any sentiments to find\"\n",
        "\n",
        "# Tokenize the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbxuAJ4ifskv",
        "outputId": "c29bffae-20f4-447a-c2e4-64eaa6162c3a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: do you have any sentiments to find!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained('trained_model')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('trained_model')\n",
        "\n",
        "# Set the device for inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Set the maximum length of generated text\n",
        "max_length = 50\n",
        "\n",
        "# Set the prompt or input text\n",
        "prompt = \"describe a story for me on any new topic\"\n",
        "\n",
        "# Tokenize the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iLZR5P6gPpk",
        "outputId": "1138e4fa-93ba-47d9-e302-05e665ddd363"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: describe a story for me on any new topic you could too to the the the the a the the gets is no to no that the no on be no to the do to women doctor on what to to to wear to! for to draw a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UkNpn0RugC8u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}