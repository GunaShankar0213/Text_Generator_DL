{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1mCMamCp6UxGTFe2XhE6CGAyBgJRwVugP","authorship_tag":"ABX9TyMXqRgcFBYa+3/Hbo0hpFIF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6d6e83f5d4244d1ea9b4e728c1fe3e93":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9aa35d6a1354177a725ebfd31732219","IPY_MODEL_cb42622a31994a44a9fe8fb58d49e341","IPY_MODEL_c3666ab274ee4ab0a5fac96dd7cfc9ff"],"layout":"IPY_MODEL_03d0f47abac24a61b17f06fc49ef1514"}},"a9aa35d6a1354177a725ebfd31732219":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69a1be0689d84bdba4e1eca7d03d3e69","placeholder":"​","style":"IPY_MODEL_303aa9f24de9482fad1a098c831ca782","value":"Downloading (…)olve/main/vocab.json: 100%"}},"cb42622a31994a44a9fe8fb58d49e341":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d4b4fda204e42f19bddf4c4bf3ff9d4","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c2a6607f0d446e3a2b819bc8f4f0b92","value":1042301}},"c3666ab274ee4ab0a5fac96dd7cfc9ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f24cc56cfd04d04a12e4e01d6f91daf","placeholder":"​","style":"IPY_MODEL_54beb22e63a746cd94b16a831fb73645","value":" 1.04M/1.04M [00:00&lt;00:00, 4.23MB/s]"}},"03d0f47abac24a61b17f06fc49ef1514":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69a1be0689d84bdba4e1eca7d03d3e69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"303aa9f24de9482fad1a098c831ca782":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d4b4fda204e42f19bddf4c4bf3ff9d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c2a6607f0d446e3a2b819bc8f4f0b92":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f24cc56cfd04d04a12e4e01d6f91daf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54beb22e63a746cd94b16a831fb73645":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cec20d92922c4f7bb578c61e297bae20":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2102f85d22746a1a0f475612f1b7c8f","IPY_MODEL_179594ef3be143ec9c733149c767c575","IPY_MODEL_50cdb847b1594cf09856e4b8fd10fcec"],"layout":"IPY_MODEL_050126ae5fc04f468db1a144cc7ca4ee"}},"d2102f85d22746a1a0f475612f1b7c8f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19493ca687794b9cbedaa73c427b5e04","placeholder":"​","style":"IPY_MODEL_e5b53d32f24747e2bde69af8e15f45af","value":"Downloading (…)olve/main/merges.txt: 100%"}},"179594ef3be143ec9c733149c767c575":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8af5cd86e40a481184c91e4990eb136d","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac3ef4c6b3ba4a10aaa52b3860b67ba2","value":456318}},"50cdb847b1594cf09856e4b8fd10fcec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffc651f9fbb44d94a4aa662913c70067","placeholder":"​","style":"IPY_MODEL_15628173639d450a8fef83999a17f7c9","value":" 456k/456k [00:00&lt;00:00, 3.92MB/s]"}},"050126ae5fc04f468db1a144cc7ca4ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19493ca687794b9cbedaa73c427b5e04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5b53d32f24747e2bde69af8e15f45af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8af5cd86e40a481184c91e4990eb136d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac3ef4c6b3ba4a10aaa52b3860b67ba2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ffc651f9fbb44d94a4aa662913c70067":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15628173639d450a8fef83999a17f7c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cad8c19276f043fe98aba52b1eb0e2b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aeb77da0a51c45f6be02d8c35c8ea25e","IPY_MODEL_111178d5e5004b20997c706b4ec96e99","IPY_MODEL_e17d14a172e245f7af72b7e1ba7bf2c4"],"layout":"IPY_MODEL_96c55f530ff540a28ee267ce3ec11952"}},"aeb77da0a51c45f6be02d8c35c8ea25e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_513c3a2db0fc4642b227dd22bcd16cad","placeholder":"​","style":"IPY_MODEL_78bda2ab4a924fc6b68350138eebeb21","value":"Downloading (…)lve/main/config.json: 100%"}},"111178d5e5004b20997c706b4ec96e99":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ae65e6147ec4fe192fa90de9abc7805","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6df37f7c0bae40b5a7c28e92b89e8728","value":665}},"e17d14a172e245f7af72b7e1ba7bf2c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23cf125cae4b4a2d96bd10f78d74639c","placeholder":"​","style":"IPY_MODEL_4e9826b4200146e793515f55b625f49f","value":" 665/665 [00:00&lt;00:00, 29.9kB/s]"}},"96c55f530ff540a28ee267ce3ec11952":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"513c3a2db0fc4642b227dd22bcd16cad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78bda2ab4a924fc6b68350138eebeb21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ae65e6147ec4fe192fa90de9abc7805":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6df37f7c0bae40b5a7c28e92b89e8728":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23cf125cae4b4a2d96bd10f78d74639c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e9826b4200146e793515f55b625f49f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a251bb72302349e4864337696224dcec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1660c19448c942c48151e72018ef9783","IPY_MODEL_28c5b52b747043daad53111f632cbcd0","IPY_MODEL_f4988424023f4323bcb645d24330966b"],"layout":"IPY_MODEL_499c4696ae984f64912a589645653907"}},"1660c19448c942c48151e72018ef9783":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50c78587418f4051b5b62c76003a241d","placeholder":"​","style":"IPY_MODEL_f720e11c79794577a10fa1b18819b504","value":"Downloading model.safetensors: 100%"}},"28c5b52b747043daad53111f632cbcd0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ef6c1f86af04b828f584d64390e8bec","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2f3a0b4ec08454fbed005ac7175cc4e","value":548105171}},"f4988424023f4323bcb645d24330966b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab999414debe49179159404342fd5c5b","placeholder":"​","style":"IPY_MODEL_517ed5d791354749b67fde8c0ce3ab44","value":" 548M/548M [00:02&lt;00:00, 218MB/s]"}},"499c4696ae984f64912a589645653907":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50c78587418f4051b5b62c76003a241d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f720e11c79794577a10fa1b18819b504":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ef6c1f86af04b828f584d64390e8bec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2f3a0b4ec08454fbed005ac7175cc4e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab999414debe49179159404342fd5c5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"517ed5d791354749b67fde8c0ce3ab44":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0254a67712a94200a73a1a9a661224d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c1f113de4db456caf9e12eb8fd58c96","IPY_MODEL_4a5541281d95454294ed3de8a160ed88","IPY_MODEL_aa0b6b3e8b7c4574a0f4b1cae94ef62c"],"layout":"IPY_MODEL_b1c0c3fa6289403f8fd559905e1debc0"}},"7c1f113de4db456caf9e12eb8fd58c96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_881f522c24334c7d9986e4ccad6c3c69","placeholder":"​","style":"IPY_MODEL_4915389ec0ae4164a22be41bfb19d004","value":"Downloading (…)neration_config.json: 100%"}},"4a5541281d95454294ed3de8a160ed88":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57820b4c2276440b90bb60e2dee094d0","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_472348e4ee1b4a0780024222e00eafe1","value":124}},"aa0b6b3e8b7c4574a0f4b1cae94ef62c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_885d707ebf734b1b8bc1045a1dda1065","placeholder":"​","style":"IPY_MODEL_4f2d46663d0541cfa7aab99c2e7e6b5c","value":" 124/124 [00:00&lt;00:00, 4.37kB/s]"}},"b1c0c3fa6289403f8fd559905e1debc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"881f522c24334c7d9986e4ccad6c3c69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4915389ec0ae4164a22be41bfb19d004":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57820b4c2276440b90bb60e2dee094d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"472348e4ee1b4a0780024222e00eafe1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"885d707ebf734b1b8bc1045a1dda1065":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f2d46663d0541cfa7aab99c2e7e6b5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Xl4v7JZS2-YW","executionInfo":{"status":"ok","timestamp":1688034119410,"user_tz":-330,"elapsed":1555,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRSbllbH361U","executionInfo":{"status":"ok","timestamp":1688034133415,"user_tz":-330,"elapsed":14010,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"397dbd26-f2ed-403c-e879-465f09c62505"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["q_a_data = pd.read_csv('/content/drive/MyDrive/Books/Ai Writting/Conversation/final_Chat_Data_GBC.csv')\n","q_a_data = q_a_data.drop(\"sentiment\",axis=1)\n","persona_data = pd.read_csv('/content/drive/MyDrive/Books/Ai Writting/Conversation/personality.csv')\n","persona_data = persona_data.drop(\"Unnamed: 0\",axis = 1)"],"metadata":{"id":"Z6_Q8NxB38OH","executionInfo":{"status":"ok","timestamp":1688034134485,"user_tz":-330,"elapsed":1074,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["q_a_data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"Ba3lk-ON4fri","executionInfo":{"status":"ok","timestamp":1688034134486,"user_tz":-330,"elapsed":7,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"fe9a648b-ee5f-4919-e0c5-235236107f87"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                              question  \\\n","0               hi, how are you doing?   \n","1        i'm fine. how about yourself?   \n","2  i'm pretty good. thanks for asking.   \n","3    no problem. so how have you been?   \n","4     i've been great. what about you?   \n","\n","                                     answer  \n","0             i'm fine. how about yourself?  \n","1       i'm pretty good. thanks for asking.  \n","2         no problem. so how have you been?  \n","3          i've been great. what about you?  \n","4  i've been good. i'm in school right now.  "],"text/html":["\n","  <div id=\"df-716ace9e-4fee-4dfc-9767-0fdc57e02104\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hi, how are you doing?</td>\n","      <td>i'm fine. how about yourself?</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i'm fine. how about yourself?</td>\n","      <td>i'm pretty good. thanks for asking.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>i'm pretty good. thanks for asking.</td>\n","      <td>no problem. so how have you been?</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>no problem. so how have you been?</td>\n","      <td>i've been great. what about you?</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i've been great. what about you?</td>\n","      <td>i've been good. i'm in school right now.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-716ace9e-4fee-4dfc-9767-0fdc57e02104')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-716ace9e-4fee-4dfc-9767-0fdc57e02104 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-716ace9e-4fee-4dfc-9767-0fdc57e02104');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["\n","persona_data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"1DmaWQCo4iA5","executionInfo":{"status":"ok","timestamp":1688034135318,"user_tz":-330,"elapsed":836,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"6909114c-a013-42ed-b5ed-db5a2c86e79c"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             Persona  \\\n","0   i like to remodel homes. i like to go hunting...   \n","1   my mom is my best friend. i have four sisters...   \n","2   i had a gig at local theater last night. i wo...   \n","3   i am very athletic. i wear contacts. i have b...   \n","4   i am primarily a meat eater. i am a guitar pl...   \n","\n","                                                chat  \n","0  hi , how are you doing ? i am getting ready to...  \n","1  hi , how are you doing today ?\\ni am spending ...  \n","2  we all live in a yellow submarine , a yellow s...  \n","3  hi ! i work as a gourmet cook .\\ni do not like...  \n","4  how are you doing today\\nwhat do you do for ca...  "],"text/html":["\n","  <div id=\"df-1278b7a2-373e-4f25-9bb4-3b154ee56ae0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Persona</th>\n","      <th>chat</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i like to remodel homes. i like to go hunting...</td>\n","      <td>hi , how are you doing ? i am getting ready to...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>my mom is my best friend. i have four sisters...</td>\n","      <td>hi , how are you doing today ?\\ni am spending ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>i had a gig at local theater last night. i wo...</td>\n","      <td>we all live in a yellow submarine , a yellow s...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i am very athletic. i wear contacts. i have b...</td>\n","      <td>hi ! i work as a gourmet cook .\\ni do not like...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i am primarily a meat eater. i am a guitar pl...</td>\n","      <td>how are you doing today\\nwhat do you do for ca...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1278b7a2-373e-4f25-9bb4-3b154ee56ae0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1278b7a2-373e-4f25-9bb4-3b154ee56ae0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1278b7a2-373e-4f25-9bb4-3b154ee56ae0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["data1 = q_a_data\n","data2 = persona_data"],"metadata":{"id":"PyavIm0E6WU9","executionInfo":{"status":"ok","timestamp":1688034135318,"user_tz":-330,"elapsed":12,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["data2.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tqYs9bTc7ckT","executionInfo":{"status":"ok","timestamp":1688034135319,"user_tz":-330,"elapsed":12,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"8b5bc506-5217-4291-bc66-28d714aba012"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Persona    0\n","chat       0\n","dtype: int64"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Select relevant columns from the datasets\n","data1_selected = data1[[\"question\", \"answer\"]]\n","data2_selected = data2[[\"Persona\", \"chat\"]]\n","\n","# Join question and persona, answer and chat into single columns\n","data1_selected[\"text\"] = data1_selected[\"question\"]\n","data1_selected.loc[len(data1_selected)-1, \"text\"] += \" \" + data2_selected[\"Persona\"].values[0]\n","data1_selected[\"response\"] = data1_selected[\"answer\"]\n","data1_selected.loc[len(data1_selected)-1, \"response\"] += \" \" + data2_selected[\"chat\"].values[0]\n","\n","# Drop unnecessary columns\n","data1_selected = data1_selected[[\"text\", \"response\"]]\n","\n","# Concatenate the datasets\n","combined_data = pd.concat([data1_selected, data2_selected.iloc[1:]])\n","\n","# Reset the index of the combined dataset\n","combined_data.reset_index(drop=True, inplace=True)\n","\n","# Replace NaN values with empty string\n","combined_data.fillna(\"\", inplace=True)\n","\n","combined_data['User'] = combined_data['text'] + combined_data['Persona']\n","combined_data['Reply'] = combined_data['response'] + combined_data['chat']"],"metadata":{"id":"TjnD4xsY4ktG","executionInfo":{"status":"ok","timestamp":1688034135320,"user_tz":-330,"elapsed":10,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["cols_drop = [\"text\" ,\"response\", \"Persona\",\"chat\"]\n","combined_data = combined_data.drop(cols_drop,axis=1)\n","combined_data.tail()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"B8SzE8U36aqm","executionInfo":{"status":"ok","timestamp":1688034135807,"user_tz":-330,"elapsed":13,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"1c6afc5a-ddb4-4940-805a-e5e118597968"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                    User  \\\n","12658   my favorite food is fried chicken. i have a t...   \n","12659   i love fast food. i am a stay at home mom. i ...   \n","12660   my family and i go camping every month. i am ...   \n","12661   i have red hair. i work at a retail store. i ...   \n","12662   i m applying for publishing jobs. the only au...   \n","\n","                                                   Reply  \n","12658  good evening , i have been having a terrible t...  \n","12659  hi there how is work\\ni do not work these days...  \n","12660  hello , i have a daughter who is in kindergart...  \n","12661  hi . how are you doing ?\\ni am doing well . ju...  \n","12662  hello ! how are you today ?\\nhello i am good ....  "],"text/html":["\n","  <div id=\"df-e59c96be-ffc8-47ac-a2b6-0725969f094f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User</th>\n","      <th>Reply</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>12658</th>\n","      <td>my favorite food is fried chicken. i have a t...</td>\n","      <td>good evening , i have been having a terrible t...</td>\n","    </tr>\n","    <tr>\n","      <th>12659</th>\n","      <td>i love fast food. i am a stay at home mom. i ...</td>\n","      <td>hi there how is work\\ni do not work these days...</td>\n","    </tr>\n","    <tr>\n","      <th>12660</th>\n","      <td>my family and i go camping every month. i am ...</td>\n","      <td>hello , i have a daughter who is in kindergart...</td>\n","    </tr>\n","    <tr>\n","      <th>12661</th>\n","      <td>i have red hair. i work at a retail store. i ...</td>\n","      <td>hi . how are you doing ?\\ni am doing well . ju...</td>\n","    </tr>\n","    <tr>\n","      <th>12662</th>\n","      <td>i m applying for publishing jobs. the only au...</td>\n","      <td>hello ! how are you today ?\\nhello i am good ....</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e59c96be-ffc8-47ac-a2b6-0725969f094f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e59c96be-ffc8-47ac-a2b6-0725969f094f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e59c96be-ffc8-47ac-a2b6-0725969f094f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["combined_data.to_csv('Final_OUT_Conv_Data.csv')"],"metadata":{"id":"SczQFQuv6tXZ","executionInfo":{"status":"ok","timestamp":1688034138108,"user_tz":-330,"elapsed":604,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["#Pre Processing Dataset"],"metadata":{"id":"a7avCoYaCeUD"}},{"cell_type":"markdown","source":["**Text Cleaning:** Remove any unnecessary characters, symbols, or special characters that do not contribute to the meaning of the text. This includes removing HTML tags, URLs, or any noise specific to your dataset.\n","\n","**Lowercasing:** Convert the text to lowercase to ensure consistency and avoid treating words with different casing as different entities.\n","\n","**Tokenization:** Split the text into individual tokens, which can be words or subword units. Tokenization helps the model understand the structure of the text and build its vocabulary.\n","\n","**Special Tokens:** Add special tokens to denote the beginning and end of sentences or prompts, as well as special tokens for padding or masking if necessary.\n","\n","**Sequence Length Limit:** Set a maximum sequence length for the input data. If a sequence exceeds this limit, it needs to be truncated or split into smaller segments to fit within the model's memory constraints.\n","\n","**Handling Long Texts:** For long texts that cannot fit into a single sequence, you can use techniques like sliding window or recursive encoding to process the text in smaller chunks.\n","\n","**Data Encoding:** Map the tokens to their corresponding integer IDs according to the model's vocabulary. This step transforms the text data into a numerical representation that can be processed by the model.\n","\n","**Data Formatting:** Organize the encoded data into input-output pairs suitable for the model. For example, for autoregressive models like GPT, the input sequence can be followed by the target sequence shifted by one position.\n","\n","**Masking:** Apply attention masks or padding masks to indicate which tokens are valid input and which should be ignored during training. This helps the model focus on relevant tokens and handle varying sequence lengths.\n","\n","**Data Batching:** Group the input-output pairs into batches for more efficient training. Batching allows parallel processing and improves training speed."],"metadata":{"id":"xQH1SfxmCiYj"}},{"cell_type":"code","source":["!pip install transformers tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnCp4rR2C6bl","executionInfo":{"status":"ok","timestamp":1687970559781,"user_tz":-330,"elapsed":13417,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"5367d79f-28ad-4efc-f453-6023366dc894"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.6.3)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"]}]},{"cell_type":"code","source":["import re\n","import torch\n","from transformers import GPT2Tokenizer"],"metadata":{"id":"HxcEKC327Wg0","executionInfo":{"status":"ok","timestamp":1687970564508,"user_tz":-330,"elapsed":4732,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["data = combined_data"],"metadata":{"id":"tN-j7qsnEpMg","executionInfo":{"status":"ok","timestamp":1688034204737,"user_tz":-330,"elapsed":898,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["##Cleaning Text\n","\n","\n","1. Removing leading and trailing spaces using the strip method.\n","\n","2. Removing HTML tags using a regular expression pattern (<.*?>) and the re.sub function.\n","\n","3. Removing URLs using a regular expression pattern (http\\S+|www\\S+|https\\S+) and the re.sub function.\n","\n","4. Removing special characters or symbols using a regular expression pattern ([^a-zA-Z0-9\\s]) and the re.sub function.\n","\n","5. Normalizing whitespace using a regular expression pattern (\\s+) and the re.sub function.\n","\n","6. Replacing multiple consecutive punctuation marks (e.g., \"!!!\" or \"??\") with a single one using a regular expression pattern (([!?.]){2,}) and the re.sub function.\n","\n","7. Replacing numbers with a special token (e.g., \"NUM\") using a regular expression pattern (\\d+) and the re.sub function.\n","\n","8. Lowering the text to small letters\n","\n","\n","\n","\n"],"metadata":{"id":"nmSw75WmDuGZ"}},{"cell_type":"code","source":["import re\n","\n","def clean_text(text):\n","    # Remove leading/trailing spaces\n","    text = text.strip()\n","    # Remove HTML tags\n","    text = re.sub(r\"<.*?>\", \"\", text)\n","    # Remove URLs\n","    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n","    # Remove special characters or symbols\n","    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n","    # Normalize whitespace\n","    text = re.sub(r\"\\s+\", \" \", text)\n","    # Replace multiple consecutive punctuation marks with a single one\n","    # text = re.sub(r\"([!?.]){2,}\", r\"\\1\", text)\n","    text = re.sub(r\"([!?.]){2,}\", r\"\", text)\n","    # Replace numbers with a special token\n","    text = re.sub(r\"\\d+\", \"NUM\", text)\n","\n","    return text\n","\n","# Apply text cleaning to \"User\" and \"Reply\" columns\n","data[\"User\"] = data[\"User\"].apply(clean_text)\n","data[\"Reply\"] = data[\"Reply\"].apply(clean_text)"],"metadata":{"id":"qYJtIlYDDtzJ","executionInfo":{"status":"ok","timestamp":1687970566029,"user_tz":-330,"elapsed":1537,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["##Tokenization"],"metadata":{"id":"rBKFufhGE_6n"}},{"cell_type":"code","source":["# Initialize the GPT-2 tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","data[\"User_tokens\"] = data[\"User\"].apply(tokenizer.tokenize)\n","data[\"Reply_tokens\"] = data[\"Reply\"].apply(tokenizer.tokenize)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["6d6e83f5d4244d1ea9b4e728c1fe3e93","a9aa35d6a1354177a725ebfd31732219","cb42622a31994a44a9fe8fb58d49e341","c3666ab274ee4ab0a5fac96dd7cfc9ff","03d0f47abac24a61b17f06fc49ef1514","69a1be0689d84bdba4e1eca7d03d3e69","303aa9f24de9482fad1a098c831ca782","7d4b4fda204e42f19bddf4c4bf3ff9d4","4c2a6607f0d446e3a2b819bc8f4f0b92","4f24cc56cfd04d04a12e4e01d6f91daf","54beb22e63a746cd94b16a831fb73645","cec20d92922c4f7bb578c61e297bae20","d2102f85d22746a1a0f475612f1b7c8f","179594ef3be143ec9c733149c767c575","50cdb847b1594cf09856e4b8fd10fcec","050126ae5fc04f468db1a144cc7ca4ee","19493ca687794b9cbedaa73c427b5e04","e5b53d32f24747e2bde69af8e15f45af","8af5cd86e40a481184c91e4990eb136d","ac3ef4c6b3ba4a10aaa52b3860b67ba2","ffc651f9fbb44d94a4aa662913c70067","15628173639d450a8fef83999a17f7c9","cad8c19276f043fe98aba52b1eb0e2b0","aeb77da0a51c45f6be02d8c35c8ea25e","111178d5e5004b20997c706b4ec96e99","e17d14a172e245f7af72b7e1ba7bf2c4","96c55f530ff540a28ee267ce3ec11952","513c3a2db0fc4642b227dd22bcd16cad","78bda2ab4a924fc6b68350138eebeb21","6ae65e6147ec4fe192fa90de9abc7805","6df37f7c0bae40b5a7c28e92b89e8728","23cf125cae4b4a2d96bd10f78d74639c","4e9826b4200146e793515f55b625f49f"]},"id":"DSdCG5IPDZCR","executionInfo":{"status":"ok","timestamp":1687970584313,"user_tz":-330,"elapsed":18291,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"069bdf94-3610-45d0-a6f1-26e97d13d5d4"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d6e83f5d4244d1ea9b4e728c1fe3e93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cec20d92922c4f7bb578c61e297bae20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cad8c19276f043fe98aba52b1eb0e2b0"}},"metadata":{}}]},{"cell_type":"code","source":["# Step 4: Special Tokens\n","special_tokens = {\n","    \"bos_token\": \"<bos>\",\n","    \"eos_token\": \"<eos>\",\n","    \"pad_token\": \"<pad>\",\n","    \"mask_token\": \"<mask>\"\n","}\n","tokenizer.add_special_tokens(special_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J5h_4Us-DZOc","executionInfo":{"status":"ok","timestamp":1687970584314,"user_tz":-330,"elapsed":15,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"3684aed0-55e7-4bc8-cd14-a6dbd01f59c5"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# Step 6: Data Encoding\n","data[\"User_ids\"] = data[\"User_tokens\"].apply(tokenizer.convert_tokens_to_ids)\n","data[\"Reply_ids\"] = data[\"Reply_tokens\"].apply(tokenizer.convert_tokens_to_ids)"],"metadata":{"id":"JdfXwpgDFQYA","executionInfo":{"status":"ok","timestamp":1687970588999,"user_tz":-330,"elapsed":4699,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Step 7: Data Formatting\n","formatted_data = [(user_ids, reply_ids) for user_ids, reply_ids in zip(data[\"User_ids\"], data[\"Reply_ids\"])]"],"metadata":{"id":"2_fnIh1RF562","executionInfo":{"status":"ok","timestamp":1687970589684,"user_tz":-330,"elapsed":702,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Step 8: Masking\n","attention_mask = [[1] * len(user_ids) for user_ids in data[\"User_ids\"]]"],"metadata":{"id":"ByxOxMC_F8xa","executionInfo":{"status":"ok","timestamp":1687970589685,"user_tz":-330,"elapsed":5,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Step 9: Data Batching\n","batch_size = 32  # Set the desired batch size"],"metadata":{"id":"hMoUTNMxGAZf","executionInfo":{"status":"ok","timestamp":1687970589686,"user_tz":-330,"elapsed":6,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Pad the sequences to a fixed length\n","padded_input_ids = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids, _ in formatted_data], batch_first=True)\n","padded_attention_mask = torch.nn.utils.rnn.pad_sequence([torch.tensor(mask) for _, mask in zip(data[\"User_ids\"], attention_mask)], batch_first=True)"],"metadata":{"id":"7bUm2ZlSGXUL","executionInfo":{"status":"ok","timestamp":1687970590401,"user_tz":-330,"elapsed":720,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Convert the padded sequences and attention masks into PyTorch tensors\n","input_ids = torch.tensor(padded_input_ids)\n","attention_mask = torch.tensor(padded_attention_mask)\n","\n","# Create data batches\n","data_batches = torch.utils.data.TensorDataset(input_ids, attention_mask)\n","data_loader = torch.utils.data.DataLoader(data_batches, batch_size=batch_size, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GGRAQYRsGEum","executionInfo":{"status":"ok","timestamp":1687970590402,"user_tz":-330,"elapsed":12,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"fda458b2-0286-42d4-a9bd-e2f6ba1466aa"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-22-c5607bf1755d>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(padded_input_ids)\n","<ipython-input-22-c5607bf1755d>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  attention_mask = torch.tensor(padded_attention_mask)\n"]}]},{"cell_type":"markdown","source":["Summary Code"],"metadata":{"id":"cJnUOdViHiFL"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from transformers import GPT2Tokenizer\n","from torch.nn.utils.rnn import pad_sequence\n","\n","\n","\n","# Initialize the GPT-2 tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# Step 1: Text Cleaning\n","# Step 2: Lowercasing\n","# Step 3: Tokenization\n","# Step 4: Special Tokens\n","# Step 5: Sequence Length Limit\n","# Step 6: Data Encoding\n","formatted_data = []\n","attention_mask = []\n","\n","for user_text, reply_text in zip(data[\"User\"], data[\"Reply\"]):\n","    # Text Cleaning\n","    user_text = clean_text(user_text)\n","    reply_text = clean_text(reply_text)\n","\n","    # Lowercasing\n","    user_text = user_text.lower()\n","    reply_text = reply_text.lower()\n","\n","    # Tokenization\n","    user_tokens = tokenizer.tokenize(user_text)\n","    reply_tokens = tokenizer.tokenize(reply_text)\n","\n","    # Special Tokens\n","    user_tokens = [tokenizer.bos_token] + user_tokens + [tokenizer.eos_token]\n","    reply_tokens = [tokenizer.bos_token] + reply_tokens + [tokenizer.eos_token]\n","\n","    # Sequence Length Limit\n","    max_sequence_length = 128  # Set the maximum sequence length\n","    user_tokens = user_tokens[:max_sequence_length]\n","    reply_tokens = reply_tokens[:max_sequence_length]\n","\n","    # Data Encoding\n","    user_ids = tokenizer.convert_tokens_to_ids(user_tokens)\n","    reply_ids = tokenizer.convert_tokens_to_ids(reply_tokens)\n","\n","    formatted_data.append((user_ids, reply_ids))\n","    attention_mask.append([1] * len(user_ids))\n","\n","# # Step 7: Data Formatting\n","# input_ids = pad_sequence([torch.tensor(user_ids) for user_ids, _ in formatted_data], batch_first=True)\n","# target_ids = pad_sequence([torch.tensor(reply_ids) for _, reply_ids in formatted_data], batch_first=True)\n","# attention_mask = pad_sequence([torch.tensor(mask) for mask in attention_mask], batch_first=True)\n","\n","# Step 7: Data Formatting\n","input_ids = pad_sequence([torch.tensor(user_ids) for user_ids, _ in formatted_data], batch_first=True, padding_value=0.0)  # Set padding value to 0.0\n","target_ids = pad_sequence([torch.tensor(reply_ids) for _, reply_ids in formatted_data], batch_first=True, padding_value=0.0)  # Set padding value to 0.0\n","attention_mask = pad_sequence([torch.tensor(mask) for mask in attention_mask], batch_first=True)\n","\n","\n","# Step 8: Masking\n","attention_mask = attention_mask.bool()\n","\n","# Step 9: Data Batching\n","batch_size = 32  # Set the desired batch size\n","\n","# Create data batches\n","data_batches = torch.utils.data.TensorDataset(input_ids, attention_mask, target_ids)\n","data_loader = torch.utils.data.DataLoader(data_batches, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"Z5oQ7b-lDZiu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model Building"],"metadata":{"id":"Zqzz9LPQIFVR"}},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel"],"metadata":{"id":"VCAvfTeFINbJ","executionInfo":{"status":"ok","timestamp":1687970590402,"user_tz":-330,"elapsed":11,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Load the pre-trained GPT-2 model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","# Set the device for training\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# Set the model to the desired device\n","model = model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["a251bb72302349e4864337696224dcec","1660c19448c942c48151e72018ef9783","28c5b52b747043daad53111f632cbcd0","f4988424023f4323bcb645d24330966b","499c4696ae984f64912a589645653907","50c78587418f4051b5b62c76003a241d","f720e11c79794577a10fa1b18819b504","8ef6c1f86af04b828f584d64390e8bec","e2f3a0b4ec08454fbed005ac7175cc4e","ab999414debe49179159404342fd5c5b","517ed5d791354749b67fde8c0ce3ab44","0254a67712a94200a73a1a9a661224d2","7c1f113de4db456caf9e12eb8fd58c96","4a5541281d95454294ed3de8a160ed88","aa0b6b3e8b7c4574a0f4b1cae94ef62c","b1c0c3fa6289403f8fd559905e1debc0","881f522c24334c7d9986e4ccad6c3c69","4915389ec0ae4164a22be41bfb19d004","57820b4c2276440b90bb60e2dee094d0","472348e4ee1b4a0780024222e00eafe1","885d707ebf734b1b8bc1045a1dda1065","4f2d46663d0541cfa7aab99c2e7e6b5c"]},"id":"IAnb_29vHUwA","executionInfo":{"status":"ok","timestamp":1687970602964,"user_tz":-330,"elapsed":12572,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"f5ed94a3-b1de-49ec-cccd-c26e12d244fa"},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a251bb72302349e4864337696224dcec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0254a67712a94200a73a1a9a661224d2"}},"metadata":{}}]},{"cell_type":"code","source":["# Set hyperparameters\n","learning_rate = 1e-6\n","num_epochs = 3"],"metadata":{"id":"xgoV3lbbITnz","executionInfo":{"status":"ok","timestamp":1687970602965,"user_tz":-330,"elapsed":24,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Set optimizer and learning rate scheduler\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"],"metadata":{"id":"9xp5w_JlIXzs","executionInfo":{"status":"ok","timestamp":1687970602965,"user_tz":-330,"elapsed":22,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","# Training loop\n","for epoch in range(num_epochs):\n","    total_loss = 0.0\n","    total_batches = 0\n","\n","    for batch_idx, (input_ids, attention_mask, target_ids) in enumerate(data_loader):\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        target_ids = target_ids.to(device)\n","\n","        # Clear gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target_ids)\n","        loss = outputs.loss\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update weights\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        total_batches += 1\n","\n","    # Calculate average loss for the epoch\n","    average_loss = total_loss / total_batches\n","\n","    # Update learning rate\n","    scheduler.step()\n","\n","    # Print progress\n","    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {average_loss}\")\n","\n","# Save the trained model\n","model.save_pretrained(\"trained_model\")\n","tokenizer.save_pretrained(\"trained_model\")\n","```\n","\n"],"metadata":{"id":"ljN36k9KlVwJ"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","from torch.nn.utils.rnn import pad_sequence\n","\n","average_loss_p = []\n","\n","# Initialize the GPT-2 tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# Step 1: Text Cleaning\n","# Step 2: Lowercasing\n","# Step 3: Tokenization\n","# Step 4: Special Tokens\n","# Step 5: Sequence Length Limit\n","# Step 6: Data Encoding\n","formatted_data = []\n","attention_mask = []\n","\n","for user_text, reply_text in zip(data[\"User\"], data[\"Reply\"]):\n","    # Text Cleaning\n","    user_text = clean_text(user_text)\n","    reply_text = clean_text(reply_text)\n","\n","    # Lowercasing\n","    user_text = user_text.lower()\n","    reply_text = reply_text.lower()\n","\n","    # Tokenization\n","    user_tokens = tokenizer.tokenize(user_text)\n","    reply_tokens = tokenizer.tokenize(reply_text)\n","\n","    # Special Tokens\n","    user_tokens = [tokenizer.bos_token] + user_tokens + [tokenizer.eos_token]\n","    reply_tokens = [tokenizer.bos_token] + reply_tokens + [tokenizer.eos_token]\n","\n","    # Sequence Length Limit\n","    max_sequence_length = 60 # Set the maximum sequence length\n","    user_tokens = user_tokens[:max_sequence_length]\n","    reply_tokens = reply_tokens[:max_sequence_length]\n","\n","    # Data Encoding\n","    user_ids = tokenizer.convert_tokens_to_ids(user_tokens)\n","    reply_ids = tokenizer.convert_tokens_to_ids(reply_tokens)\n","\n","    formatted_data.append((user_ids, reply_ids))\n","    attention_mask.append([1] * len(user_ids))\n","\n","# Step 7: Data Formatting\n","input_ids = pad_sequence([torch.tensor(user_ids) for user_ids, _ in formatted_data], batch_first=True, padding_value=0.0)  # Set padding value to 0.0\n","target_ids = pad_sequence([torch.tensor(reply_ids) for _, reply_ids in formatted_data], batch_first=True, padding_value=0.0)  # Set padding value to 0.0\n","attention_mask = pad_sequence([torch.tensor(mask) for mask in attention_mask], batch_first=True)\n","\n","# Step 8: Masking\n","attention_mask = attention_mask.bool()\n","\n","# Filter out incomplete batches\n","complete_batches = [idx for idx, (input_batch, target_batch) in enumerate(zip(input_ids, target_ids)) if input_batch.size(0) == target_batch.size(0)]\n","input_ids = input_ids[complete_batches]\n","target_ids = target_ids[complete_batches]\n","attention_mask = attention_mask[complete_batches]\n","\n","# Step 9: Data Batching\n","batch_size = 64  # Set the desired batch size\n","\n","if len(input_ids) > 0:\n","    # Create data batches\n","    data_batches = torch.utils.data.TensorDataset(input_ids, attention_mask, target_ids)\n","    data_loader = torch.utils.data.DataLoader(data_batches, batch_size=batch_size, shuffle=True)\n","\n","    # Load the pre-trained GPT-2 model and tokenizer\n","    model = GPT2LMHeadModel.from_pretrained('gpt2')\n","    # Set the device for training\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # Set the model to the desired device\n","    model = model.to(device)\n","    # Set hyperparameters\n","    learning_rate = 1e-4\n","    num_epochs = 150\n","    # Set optimizer and learning rate scheduler\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","\n","   # Training loop\n","for epoch in range(num_epochs):\n","    total_loss = 0.0\n","    total_batches = 0\n","\n","    for batch_idx, (input_ids, attention_mask, target_ids) in enumerate(data_loader):\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        target_ids = target_ids.to(device)\n","\n","        # Clear gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target_ids)\n","        loss = outputs.loss\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update weights\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        total_batches += 1\n","\n","        # Print progress\n","        print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx+1}/{len(data_loader)} | Loss: {loss.item()}\")\n","\n","    # Calculate average loss for the epoch\n","    average_loss = total_loss / total_batches\n","    average_loss_p.append(average_loss)\n","    # Update learning rate\n","    scheduler.step()\n","\n","    # Print progress\n","    print(f\"Epoch {epoch+1}/{num_epochs} | Average Loss: {average_loss}\")\n","\n","\n","    # Save the trained model\n","    model.save_pretrained(\"trained_model\")\n","    tokenizer.save_pretrained(\"trained_model\")\n","else:\n","    print(\"No complete batches remaining after filtering.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VPu7kQhIXkH","outputId":"d8dbf664-d375-4e49-dbe5-5691f0a612d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch 10/150 | Batch 109/198 | Loss: 3.6907105445861816\n","Epoch 10/150 | Batch 110/198 | Loss: 3.5099165439605713\n","Epoch 10/150 | Batch 111/198 | Loss: 4.141637802124023\n","Epoch 10/150 | Batch 112/198 | Loss: 3.4878971576690674\n","Epoch 10/150 | Batch 113/198 | Loss: 3.7158408164978027\n","Epoch 10/150 | Batch 114/198 | Loss: 3.8053669929504395\n","Epoch 10/150 | Batch 115/198 | Loss: 3.6551880836486816\n","Epoch 10/150 | Batch 116/198 | Loss: 3.4025776386260986\n","Epoch 10/150 | Batch 117/198 | Loss: 3.5225138664245605\n","Epoch 10/150 | Batch 118/198 | Loss: 3.7576112747192383\n","Epoch 10/150 | Batch 119/198 | Loss: 3.705986738204956\n","Epoch 10/150 | Batch 120/198 | Loss: 3.6353344917297363\n","Epoch 10/150 | Batch 121/198 | Loss: 3.598421096801758\n","Epoch 10/150 | Batch 122/198 | Loss: 3.848510980606079\n","Epoch 10/150 | Batch 123/198 | Loss: 3.8709256649017334\n","Epoch 10/150 | Batch 124/198 | Loss: 4.4435505867004395\n","Epoch 10/150 | Batch 125/198 | Loss: 3.179626941680908\n","Epoch 10/150 | Batch 126/198 | Loss: 3.6547694206237793\n","Epoch 10/150 | Batch 127/198 | Loss: 3.75616192817688\n","Epoch 10/150 | Batch 128/198 | Loss: 3.0786685943603516\n","Epoch 10/150 | Batch 129/198 | Loss: 3.559619665145874\n","Epoch 10/150 | Batch 130/198 | Loss: 3.6105079650878906\n","Epoch 10/150 | Batch 131/198 | Loss: 3.6456379890441895\n","Epoch 10/150 | Batch 132/198 | Loss: 4.018016815185547\n","Epoch 10/150 | Batch 133/198 | Loss: 3.3893697261810303\n","Epoch 10/150 | Batch 134/198 | Loss: 3.6214466094970703\n","Epoch 10/150 | Batch 135/198 | Loss: 3.3488035202026367\n","Epoch 10/150 | Batch 136/198 | Loss: 3.8857810497283936\n","Epoch 10/150 | Batch 137/198 | Loss: 3.183306932449341\n","Epoch 10/150 | Batch 138/198 | Loss: 3.6154987812042236\n","Epoch 10/150 | Batch 139/198 | Loss: 4.267247200012207\n","Epoch 10/150 | Batch 140/198 | Loss: 3.683966875076294\n","Epoch 10/150 | Batch 141/198 | Loss: 3.3052425384521484\n","Epoch 10/150 | Batch 142/198 | Loss: 3.63095760345459\n","Epoch 10/150 | Batch 143/198 | Loss: 3.515230178833008\n","Epoch 10/150 | Batch 144/198 | Loss: 3.3062188625335693\n","Epoch 10/150 | Batch 145/198 | Loss: 3.479254961013794\n","Epoch 10/150 | Batch 146/198 | Loss: 3.184279203414917\n","Epoch 10/150 | Batch 147/198 | Loss: 3.152167320251465\n","Epoch 10/150 | Batch 148/198 | Loss: 3.3357126712799072\n","Epoch 10/150 | Batch 149/198 | Loss: 3.7240054607391357\n","Epoch 10/150 | Batch 150/198 | Loss: 3.58699107170105\n","Epoch 10/150 | Batch 151/198 | Loss: 3.9071149826049805\n","Epoch 10/150 | Batch 152/198 | Loss: 3.4416637420654297\n","Epoch 10/150 | Batch 153/198 | Loss: 3.454669952392578\n","Epoch 10/150 | Batch 154/198 | Loss: 3.456571102142334\n","Epoch 10/150 | Batch 155/198 | Loss: 3.6811487674713135\n","Epoch 10/150 | Batch 156/198 | Loss: 3.5820324420928955\n","Epoch 10/150 | Batch 157/198 | Loss: 3.5801849365234375\n","Epoch 10/150 | Batch 158/198 | Loss: 4.098784923553467\n","Epoch 10/150 | Batch 159/198 | Loss: 3.77473521232605\n","Epoch 10/150 | Batch 160/198 | Loss: 3.4364404678344727\n","Epoch 10/150 | Batch 161/198 | Loss: 3.68953800201416\n","Epoch 10/150 | Batch 162/198 | Loss: 3.828422784805298\n","Epoch 10/150 | Batch 163/198 | Loss: 3.6452457904815674\n","Epoch 10/150 | Batch 164/198 | Loss: 3.4395742416381836\n","Epoch 10/150 | Batch 165/198 | Loss: 3.482581853866577\n","Epoch 10/150 | Batch 166/198 | Loss: 3.6376404762268066\n","Epoch 10/150 | Batch 167/198 | Loss: 3.910994291305542\n","Epoch 10/150 | Batch 168/198 | Loss: 3.514094829559326\n","Epoch 10/150 | Batch 169/198 | Loss: 3.8681938648223877\n","Epoch 10/150 | Batch 170/198 | Loss: 3.898599624633789\n","Epoch 10/150 | Batch 171/198 | Loss: 3.303738832473755\n","Epoch 10/150 | Batch 172/198 | Loss: 3.6802914142608643\n","Epoch 10/150 | Batch 173/198 | Loss: 3.585380792617798\n","Epoch 10/150 | Batch 174/198 | Loss: 3.2772715091705322\n","Epoch 10/150 | Batch 175/198 | Loss: 3.820723533630371\n","Epoch 10/150 | Batch 176/198 | Loss: 3.5130743980407715\n","Epoch 10/150 | Batch 177/198 | Loss: 3.6433041095733643\n","Epoch 10/150 | Batch 178/198 | Loss: 3.8846559524536133\n","Epoch 10/150 | Batch 179/198 | Loss: 3.9143919944763184\n","Epoch 10/150 | Batch 180/198 | Loss: 3.49709415435791\n","Epoch 10/150 | Batch 181/198 | Loss: 3.9391305446624756\n","Epoch 10/150 | Batch 182/198 | Loss: 4.217367172241211\n","Epoch 10/150 | Batch 183/198 | Loss: 3.4791676998138428\n","Epoch 10/150 | Batch 184/198 | Loss: 3.647260904312134\n","Epoch 10/150 | Batch 185/198 | Loss: 3.7652626037597656\n","Epoch 10/150 | Batch 186/198 | Loss: 3.748898983001709\n","Epoch 10/150 | Batch 187/198 | Loss: 3.295985221862793\n","Epoch 10/150 | Batch 188/198 | Loss: 3.877617597579956\n","Epoch 10/150 | Batch 189/198 | Loss: 3.795499801635742\n","Epoch 10/150 | Batch 190/198 | Loss: 3.166971445083618\n","Epoch 10/150 | Batch 191/198 | Loss: 3.612036943435669\n","Epoch 10/150 | Batch 192/198 | Loss: 3.3207173347473145\n","Epoch 10/150 | Batch 193/198 | Loss: 3.4104461669921875\n","Epoch 10/150 | Batch 194/198 | Loss: 3.580315589904785\n","Epoch 10/150 | Batch 195/198 | Loss: 3.4936747550964355\n","Epoch 10/150 | Batch 196/198 | Loss: 3.0028276443481445\n","Epoch 10/150 | Batch 197/198 | Loss: 3.628126859664917\n","Epoch 10/150 | Batch 198/198 | Loss: 3.6943187713623047\n","Epoch 10/150 | Average Loss: 3.626378574756661\n","Epoch 11/150 | Batch 1/198 | Loss: 3.58565616607666\n","Epoch 11/150 | Batch 2/198 | Loss: 3.444594144821167\n","Epoch 11/150 | Batch 3/198 | Loss: 3.544557809829712\n","Epoch 11/150 | Batch 4/198 | Loss: 3.2797346115112305\n","Epoch 11/150 | Batch 5/198 | Loss: 3.328652858734131\n","Epoch 11/150 | Batch 6/198 | Loss: 3.3945586681365967\n","Epoch 11/150 | Batch 7/198 | Loss: 3.2959718704223633\n","Epoch 11/150 | Batch 8/198 | Loss: 3.555917501449585\n","Epoch 11/150 | Batch 9/198 | Loss: 3.3412158489227295\n","Epoch 11/150 | Batch 10/198 | Loss: 3.848785877227783\n","Epoch 11/150 | Batch 11/198 | Loss: 3.0704078674316406\n","Epoch 11/150 | Batch 12/198 | Loss: 3.7173736095428467\n","Epoch 11/150 | Batch 13/198 | Loss: 3.495898723602295\n","Epoch 11/150 | Batch 14/198 | Loss: 3.8769140243530273\n","Epoch 11/150 | Batch 15/198 | Loss: 3.671816825866699\n","Epoch 11/150 | Batch 16/198 | Loss: 3.4518232345581055\n","Epoch 11/150 | Batch 17/198 | Loss: 3.7855072021484375\n","Epoch 11/150 | Batch 18/198 | Loss: 3.1991875171661377\n","Epoch 11/150 | Batch 19/198 | Loss: 3.19960880279541\n","Epoch 11/150 | Batch 20/198 | Loss: 3.379924774169922\n","Epoch 11/150 | Batch 21/198 | Loss: 3.5606889724731445\n","Epoch 11/150 | Batch 22/198 | Loss: 3.531912326812744\n","Epoch 11/150 | Batch 23/198 | Loss: 3.7413525581359863\n","Epoch 11/150 | Batch 24/198 | Loss: 3.3777029514312744\n","Epoch 11/150 | Batch 25/198 | Loss: 3.458430290222168\n","Epoch 11/150 | Batch 26/198 | Loss: 3.078519105911255\n","Epoch 11/150 | Batch 27/198 | Loss: 3.4112842082977295\n","Epoch 11/150 | Batch 28/198 | Loss: 3.260751962661743\n","Epoch 11/150 | Batch 29/198 | Loss: 3.277165412902832\n","Epoch 11/150 | Batch 30/198 | Loss: 3.7110958099365234\n","Epoch 11/150 | Batch 31/198 | Loss: 3.3096771240234375\n","Epoch 11/150 | Batch 32/198 | Loss: 3.6595358848571777\n","Epoch 11/150 | Batch 33/198 | Loss: 3.4562699794769287\n","Epoch 11/150 | Batch 34/198 | Loss: 3.65667724609375\n","Epoch 11/150 | Batch 35/198 | Loss: 3.005758762359619\n","Epoch 11/150 | Batch 36/198 | Loss: 3.0742223262786865\n","Epoch 11/150 | Batch 37/198 | Loss: 3.6060268878936768\n","Epoch 11/150 | Batch 38/198 | Loss: 3.3742406368255615\n","Epoch 11/150 | Batch 39/198 | Loss: 3.3050954341888428\n","Epoch 11/150 | Batch 40/198 | Loss: 3.113381862640381\n","Epoch 11/150 | Batch 41/198 | Loss: 4.0165629386901855\n","Epoch 11/150 | Batch 42/198 | Loss: 3.235508441925049\n","Epoch 11/150 | Batch 43/198 | Loss: 2.6731696128845215\n","Epoch 11/150 | Batch 44/198 | Loss: 3.450148820877075\n","Epoch 11/150 | Batch 45/198 | Loss: 3.490609884262085\n","Epoch 11/150 | Batch 46/198 | Loss: 3.6244940757751465\n","Epoch 11/150 | Batch 47/198 | Loss: 3.3817267417907715\n","Epoch 11/150 | Batch 48/198 | Loss: 3.8814852237701416\n","Epoch 11/150 | Batch 49/198 | Loss: 3.518433094024658\n","Epoch 11/150 | Batch 50/198 | Loss: 3.569254159927368\n","Epoch 11/150 | Batch 51/198 | Loss: 3.843956708908081\n","Epoch 11/150 | Batch 52/198 | Loss: 3.132542610168457\n","Epoch 11/150 | Batch 53/198 | Loss: 3.64371657371521\n","Epoch 11/150 | Batch 54/198 | Loss: 3.778393030166626\n","Epoch 11/150 | Batch 55/198 | Loss: 3.574960231781006\n","Epoch 11/150 | Batch 56/198 | Loss: 2.8676905632019043\n","Epoch 11/150 | Batch 57/198 | Loss: 3.532090187072754\n","Epoch 11/150 | Batch 58/198 | Loss: 3.6602284908294678\n","Epoch 11/150 | Batch 59/198 | Loss: 3.5044617652893066\n","Epoch 11/150 | Batch 60/198 | Loss: 3.4061849117279053\n","Epoch 11/150 | Batch 61/198 | Loss: 3.6769258975982666\n","Epoch 11/150 | Batch 62/198 | Loss: 3.291537284851074\n","Epoch 11/150 | Batch 63/198 | Loss: 3.5808165073394775\n","Epoch 11/150 | Batch 64/198 | Loss: 3.3634331226348877\n","Epoch 11/150 | Batch 65/198 | Loss: 3.1182737350463867\n","Epoch 11/150 | Batch 66/198 | Loss: 3.5218076705932617\n","Epoch 11/150 | Batch 67/198 | Loss: 3.3524343967437744\n","Epoch 11/150 | Batch 68/198 | Loss: 3.2273659706115723\n","Epoch 11/150 | Batch 69/198 | Loss: 3.660388708114624\n","Epoch 11/150 | Batch 70/198 | Loss: 3.636873483657837\n","Epoch 11/150 | Batch 71/198 | Loss: 3.556689739227295\n","Epoch 11/150 | Batch 72/198 | Loss: 3.4498884677886963\n","Epoch 11/150 | Batch 73/198 | Loss: 3.1991665363311768\n","Epoch 11/150 | Batch 74/198 | Loss: 3.2368662357330322\n","Epoch 11/150 | Batch 75/198 | Loss: 3.41532039642334\n","Epoch 11/150 | Batch 76/198 | Loss: 3.323153018951416\n","Epoch 11/150 | Batch 77/198 | Loss: 3.259814977645874\n","Epoch 11/150 | Batch 78/198 | Loss: 3.5378546714782715\n","Epoch 11/150 | Batch 79/198 | Loss: 3.391606569290161\n","Epoch 11/150 | Batch 80/198 | Loss: 3.5222697257995605\n","Epoch 11/150 | Batch 81/198 | Loss: 3.775599956512451\n","Epoch 11/150 | Batch 82/198 | Loss: 3.045421600341797\n","Epoch 11/150 | Batch 83/198 | Loss: 3.506273031234741\n","Epoch 11/150 | Batch 84/198 | Loss: 2.907585620880127\n","Epoch 11/150 | Batch 85/198 | Loss: 4.149435997009277\n","Epoch 11/150 | Batch 86/198 | Loss: 3.596332311630249\n","Epoch 11/150 | Batch 87/198 | Loss: 2.9007368087768555\n","Epoch 11/150 | Batch 88/198 | Loss: 3.5147814750671387\n","Epoch 11/150 | Batch 89/198 | Loss: 4.165897846221924\n","Epoch 11/150 | Batch 90/198 | Loss: 3.2916433811187744\n","Epoch 11/150 | Batch 91/198 | Loss: 3.5018649101257324\n","Epoch 11/150 | Batch 92/198 | Loss: 3.2410922050476074\n","Epoch 11/150 | Batch 93/198 | Loss: 3.6304280757904053\n","Epoch 11/150 | Batch 94/198 | Loss: 3.789609432220459\n","Epoch 11/150 | Batch 95/198 | Loss: 3.4508049488067627\n","Epoch 11/150 | Batch 96/198 | Loss: 3.9482929706573486\n","Epoch 11/150 | Batch 97/198 | Loss: 3.5518429279327393\n","Epoch 11/150 | Batch 98/198 | Loss: 3.536137819290161\n","Epoch 11/150 | Batch 99/198 | Loss: 3.528074264526367\n","Epoch 11/150 | Batch 100/198 | Loss: 3.3254754543304443\n","Epoch 11/150 | Batch 101/198 | Loss: 3.4745383262634277\n","Epoch 11/150 | Batch 102/198 | Loss: 3.391970157623291\n","Epoch 11/150 | Batch 103/198 | Loss: 3.5365846157073975\n","Epoch 11/150 | Batch 104/198 | Loss: 3.5999531745910645\n","Epoch 11/150 | Batch 105/198 | Loss: 3.4944331645965576\n","Epoch 11/150 | Batch 106/198 | Loss: 4.0543293952941895\n","Epoch 11/150 | Batch 107/198 | Loss: 3.7476725578308105\n","Epoch 11/150 | Batch 108/198 | Loss: 3.3828465938568115\n","Epoch 11/150 | Batch 109/198 | Loss: 3.8756978511810303\n","Epoch 11/150 | Batch 110/198 | Loss: 3.436741828918457\n","Epoch 11/150 | Batch 111/198 | Loss: 3.4647057056427\n","Epoch 11/150 | Batch 112/198 | Loss: 3.1991140842437744\n","Epoch 11/150 | Batch 113/198 | Loss: 3.74554705619812\n","Epoch 11/150 | Batch 114/198 | Loss: 3.1726529598236084\n","Epoch 11/150 | Batch 115/198 | Loss: 3.454984664916992\n","Epoch 11/150 | Batch 116/198 | Loss: 3.0544111728668213\n","Epoch 11/150 | Batch 117/198 | Loss: 3.5022966861724854\n","Epoch 11/150 | Batch 118/198 | Loss: 3.34838604927063\n","Epoch 11/150 | Batch 119/198 | Loss: 4.052080154418945\n","Epoch 11/150 | Batch 120/198 | Loss: 3.430863380432129\n","Epoch 11/150 | Batch 121/198 | Loss: 3.85613751411438\n","Epoch 11/150 | Batch 122/198 | Loss: 3.8793911933898926\n","Epoch 11/150 | Batch 123/198 | Loss: 3.9270520210266113\n","Epoch 11/150 | Batch 124/198 | Loss: 3.3633744716644287\n","Epoch 11/150 | Batch 125/198 | Loss: 3.876807689666748\n","Epoch 11/150 | Batch 126/198 | Loss: 3.7235379219055176\n","Epoch 11/150 | Batch 127/198 | Loss: 3.475647211074829\n","Epoch 11/150 | Batch 128/198 | Loss: 3.1050302982330322\n","Epoch 11/150 | Batch 129/198 | Loss: 3.8259525299072266\n","Epoch 11/150 | Batch 130/198 | Loss: 3.7835850715637207\n","Epoch 11/150 | Batch 131/198 | Loss: 3.1703391075134277\n","Epoch 11/150 | Batch 132/198 | Loss: 3.3620426654815674\n","Epoch 11/150 | Batch 133/198 | Loss: 3.286748170852661\n","Epoch 11/150 | Batch 134/198 | Loss: 3.0667812824249268\n","Epoch 11/150 | Batch 135/198 | Loss: 3.327578544616699\n","Epoch 11/150 | Batch 136/198 | Loss: 3.524536609649658\n","Epoch 11/150 | Batch 137/198 | Loss: 3.2181107997894287\n","Epoch 11/150 | Batch 138/198 | Loss: 3.6612813472747803\n","Epoch 11/150 | Batch 139/198 | Loss: 3.7626943588256836\n","Epoch 11/150 | Batch 140/198 | Loss: 4.006302833557129\n","Epoch 11/150 | Batch 141/198 | Loss: 3.450605630874634\n","Epoch 11/150 | Batch 142/198 | Loss: 3.5288479328155518\n","Epoch 11/150 | Batch 143/198 | Loss: 3.5820260047912598\n","Epoch 11/150 | Batch 144/198 | Loss: 3.589702844619751\n","Epoch 11/150 | Batch 145/198 | Loss: 3.4715218544006348\n","Epoch 11/150 | Batch 146/198 | Loss: 3.401092529296875\n","Epoch 11/150 | Batch 147/198 | Loss: 3.233248472213745\n","Epoch 11/150 | Batch 148/198 | Loss: 4.061110019683838\n","Epoch 11/150 | Batch 149/198 | Loss: 3.4052894115448\n","Epoch 11/150 | Batch 150/198 | Loss: 3.367288112640381\n","Epoch 11/150 | Batch 151/198 | Loss: 3.490327835083008\n","Epoch 11/150 | Batch 152/198 | Loss: 3.0684540271759033\n","Epoch 11/150 | Batch 153/198 | Loss: 3.203711748123169\n","Epoch 11/150 | Batch 154/198 | Loss: 2.956310749053955\n","Epoch 11/150 | Batch 155/198 | Loss: 3.312255859375\n","Epoch 11/150 | Batch 156/198 | Loss: 3.4951586723327637\n","Epoch 11/150 | Batch 157/198 | Loss: 3.5169477462768555\n","Epoch 11/150 | Batch 158/198 | Loss: 3.3557376861572266\n","Epoch 11/150 | Batch 159/198 | Loss: 3.071847915649414\n","Epoch 11/150 | Batch 160/198 | Loss: 3.4580469131469727\n","Epoch 11/150 | Batch 161/198 | Loss: 3.1478445529937744\n","Epoch 11/150 | Batch 162/198 | Loss: 3.8900206089019775\n","Epoch 11/150 | Batch 163/198 | Loss: 3.3575689792633057\n","Epoch 11/150 | Batch 164/198 | Loss: 3.380446672439575\n","Epoch 11/150 | Batch 165/198 | Loss: 2.9895055294036865\n","Epoch 11/150 | Batch 166/198 | Loss: 3.9219865798950195\n","Epoch 11/150 | Batch 167/198 | Loss: 3.3399546146392822\n","Epoch 11/150 | Batch 168/198 | Loss: 4.020340442657471\n","Epoch 11/150 | Batch 169/198 | Loss: 3.265451669692993\n","Epoch 11/150 | Batch 170/198 | Loss: 3.3969573974609375\n","Epoch 11/150 | Batch 171/198 | Loss: 3.8691563606262207\n","Epoch 11/150 | Batch 172/198 | Loss: 3.367640495300293\n","Epoch 11/150 | Batch 173/198 | Loss: 3.517416477203369\n","Epoch 11/150 | Batch 174/198 | Loss: 3.7004554271698\n","Epoch 11/150 | Batch 175/198 | Loss: 3.4255213737487793\n","Epoch 11/150 | Batch 176/198 | Loss: 3.413867235183716\n","Epoch 11/150 | Batch 177/198 | Loss: 3.1348087787628174\n","Epoch 11/150 | Batch 178/198 | Loss: 3.492493152618408\n","Epoch 11/150 | Batch 179/198 | Loss: 3.384105920791626\n","Epoch 11/150 | Batch 180/198 | Loss: 3.5690953731536865\n","Epoch 11/150 | Batch 181/198 | Loss: 3.9481358528137207\n","Epoch 11/150 | Batch 182/198 | Loss: 3.3718972206115723\n","Epoch 11/150 | Batch 183/198 | Loss: 3.9638140201568604\n","Epoch 11/150 | Batch 184/198 | Loss: 3.593390703201294\n","Epoch 11/150 | Batch 185/198 | Loss: 3.318783760070801\n","Epoch 11/150 | Batch 186/198 | Loss: 3.1827385425567627\n","Epoch 11/150 | Batch 187/198 | Loss: 3.342385768890381\n","Epoch 11/150 | Batch 188/198 | Loss: 3.448636054992676\n","Epoch 11/150 | Batch 189/198 | Loss: 2.977621555328369\n","Epoch 11/150 | Batch 190/198 | Loss: 3.379459857940674\n","Epoch 11/150 | Batch 191/198 | Loss: 3.428266763687134\n","Epoch 11/150 | Batch 192/198 | Loss: 3.563793182373047\n","Epoch 11/150 | Batch 193/198 | Loss: 3.3367865085601807\n","Epoch 11/150 | Batch 194/198 | Loss: 3.465186834335327\n","Epoch 11/150 | Batch 195/198 | Loss: 3.479597806930542\n","Epoch 11/150 | Batch 196/198 | Loss: 3.3635079860687256\n","Epoch 11/150 | Batch 197/198 | Loss: 3.456007242202759\n","Epoch 11/150 | Batch 198/198 | Loss: 3.6699323654174805\n","Epoch 11/150 | Average Loss: 3.471819125040613\n","Epoch 12/150 | Batch 1/198 | Loss: 3.5115561485290527\n","Epoch 12/150 | Batch 2/198 | Loss: 3.682239532470703\n","Epoch 12/150 | Batch 3/198 | Loss: 3.005276918411255\n","Epoch 12/150 | Batch 4/198 | Loss: 3.5753486156463623\n","Epoch 12/150 | Batch 5/198 | Loss: 3.7423861026763916\n","Epoch 12/150 | Batch 6/198 | Loss: 3.5963902473449707\n","Epoch 12/150 | Batch 7/198 | Loss: 3.3570432662963867\n","Epoch 12/150 | Batch 8/198 | Loss: 3.219813823699951\n","Epoch 12/150 | Batch 9/198 | Loss: 3.238215923309326\n","Epoch 12/150 | Batch 10/198 | Loss: 3.0403079986572266\n","Epoch 12/150 | Batch 11/198 | Loss: 3.425961494445801\n","Epoch 12/150 | Batch 12/198 | Loss: 3.476329803466797\n","Epoch 12/150 | Batch 13/198 | Loss: 3.297362804412842\n","Epoch 12/150 | Batch 14/198 | Loss: 3.439560890197754\n","Epoch 12/150 | Batch 15/198 | Loss: 3.2758665084838867\n","Epoch 12/150 | Batch 16/198 | Loss: 3.2971765995025635\n","Epoch 12/150 | Batch 17/198 | Loss: 3.1573374271392822\n","Epoch 12/150 | Batch 18/198 | Loss: 3.2359344959259033\n","Epoch 12/150 | Batch 19/198 | Loss: 3.489175796508789\n","Epoch 12/150 | Batch 20/198 | Loss: 3.1033425331115723\n","Epoch 12/150 | Batch 21/198 | Loss: 3.352617025375366\n","Epoch 12/150 | Batch 22/198 | Loss: 3.47682785987854\n","Epoch 12/150 | Batch 23/198 | Loss: 3.529386281967163\n","Epoch 12/150 | Batch 24/198 | Loss: 3.491838216781616\n","Epoch 12/150 | Batch 25/198 | Loss: 3.5381171703338623\n","Epoch 12/150 | Batch 26/198 | Loss: 3.010615348815918\n","Epoch 12/150 | Batch 27/198 | Loss: 3.4293980598449707\n","Epoch 12/150 | Batch 28/198 | Loss: 3.4030954837799072\n","Epoch 12/150 | Batch 29/198 | Loss: 2.995858669281006\n","Epoch 12/150 | Batch 30/198 | Loss: 3.015897512435913\n","Epoch 12/150 | Batch 31/198 | Loss: 3.4375858306884766\n","Epoch 12/150 | Batch 32/198 | Loss: 3.0135374069213867\n","Epoch 12/150 | Batch 33/198 | Loss: 3.517631769180298\n","Epoch 12/150 | Batch 34/198 | Loss: 3.4446189403533936\n","Epoch 12/150 | Batch 35/198 | Loss: 3.074465274810791\n","Epoch 12/150 | Batch 36/198 | Loss: 2.819232702255249\n","Epoch 12/150 | Batch 37/198 | Loss: 3.4123692512512207\n","Epoch 12/150 | Batch 38/198 | Loss: 3.026871919631958\n","Epoch 12/150 | Batch 39/198 | Loss: 3.317932605743408\n","Epoch 12/150 | Batch 40/198 | Loss: 3.0191023349761963\n","Epoch 12/150 | Batch 41/198 | Loss: 3.1836326122283936\n","Epoch 12/150 | Batch 42/198 | Loss: 3.707848310470581\n","Epoch 12/150 | Batch 43/198 | Loss: 3.141770601272583\n","Epoch 12/150 | Batch 44/198 | Loss: 3.118777275085449\n","Epoch 12/150 | Batch 45/198 | Loss: 3.5775845050811768\n","Epoch 12/150 | Batch 46/198 | Loss: 3.009880781173706\n","Epoch 12/150 | Batch 47/198 | Loss: 3.1783447265625\n","Epoch 12/150 | Batch 48/198 | Loss: 3.10904598236084\n","Epoch 12/150 | Batch 49/198 | Loss: 3.3998847007751465\n","Epoch 12/150 | Batch 50/198 | Loss: 3.559089422225952\n","Epoch 12/150 | Batch 51/198 | Loss: 2.943722724914551\n","Epoch 12/150 | Batch 52/198 | Loss: 3.048391580581665\n","Epoch 12/150 | Batch 53/198 | Loss: 3.180455446243286\n","Epoch 12/150 | Batch 54/198 | Loss: 2.941056489944458\n","Epoch 12/150 | Batch 55/198 | Loss: 3.4340484142303467\n","Epoch 12/150 | Batch 56/198 | Loss: 3.051537275314331\n","Epoch 12/150 | Batch 57/198 | Loss: 3.771106719970703\n","Epoch 12/150 | Batch 58/198 | Loss: 3.7486839294433594\n","Epoch 12/150 | Batch 59/198 | Loss: 3.1777803897857666\n","Epoch 12/150 | Batch 60/198 | Loss: 3.2343783378601074\n","Epoch 12/150 | Batch 61/198 | Loss: 3.183581829071045\n","Epoch 12/150 | Batch 62/198 | Loss: 3.1363818645477295\n","Epoch 12/150 | Batch 63/198 | Loss: 3.9507861137390137\n","Epoch 12/150 | Batch 64/198 | Loss: 3.1024723052978516\n","Epoch 12/150 | Batch 65/198 | Loss: 3.520355463027954\n","Epoch 12/150 | Batch 66/198 | Loss: 3.3056392669677734\n","Epoch 12/150 | Batch 67/198 | Loss: 3.8847920894622803\n","Epoch 12/150 | Batch 68/198 | Loss: 3.0867362022399902\n","Epoch 12/150 | Batch 69/198 | Loss: 3.1773200035095215\n","Epoch 12/150 | Batch 70/198 | Loss: 3.2826907634735107\n","Epoch 12/150 | Batch 71/198 | Loss: 3.3525631427764893\n","Epoch 12/150 | Batch 72/198 | Loss: 3.401934862136841\n","Epoch 12/150 | Batch 73/198 | Loss: 3.187084913253784\n","Epoch 12/150 | Batch 74/198 | Loss: 3.356214761734009\n","Epoch 12/150 | Batch 75/198 | Loss: 3.32669734954834\n","Epoch 12/150 | Batch 76/198 | Loss: 3.0221610069274902\n","Epoch 12/150 | Batch 77/198 | Loss: 3.4088196754455566\n","Epoch 12/150 | Batch 78/198 | Loss: 2.3843536376953125\n","Epoch 12/150 | Batch 79/198 | Loss: 3.4555654525756836\n","Epoch 12/150 | Batch 80/198 | Loss: 3.628225088119507\n","Epoch 12/150 | Batch 81/198 | Loss: 3.431138277053833\n","Epoch 12/150 | Batch 82/198 | Loss: 3.067737579345703\n","Epoch 12/150 | Batch 83/198 | Loss: 3.4657087326049805\n","Epoch 12/150 | Batch 84/198 | Loss: 2.912297487258911\n","Epoch 12/150 | Batch 85/198 | Loss: 3.2860937118530273\n","Epoch 12/150 | Batch 86/198 | Loss: 3.341817617416382\n","Epoch 12/150 | Batch 87/198 | Loss: 3.2160823345184326\n","Epoch 12/150 | Batch 88/198 | Loss: 3.1511573791503906\n","Epoch 12/150 | Batch 89/198 | Loss: 3.386261463165283\n","Epoch 12/150 | Batch 90/198 | Loss: 3.1584103107452393\n","Epoch 12/150 | Batch 91/198 | Loss: 3.775933027267456\n","Epoch 12/150 | Batch 92/198 | Loss: 3.3471364974975586\n","Epoch 12/150 | Batch 93/198 | Loss: 3.619364023208618\n","Epoch 12/150 | Batch 94/198 | Loss: 3.140212297439575\n","Epoch 12/150 | Batch 95/198 | Loss: 3.1954498291015625\n","Epoch 12/150 | Batch 96/198 | Loss: 3.6370115280151367\n","Epoch 12/150 | Batch 97/198 | Loss: 2.895099639892578\n","Epoch 12/150 | Batch 98/198 | Loss: 3.227468252182007\n","Epoch 12/150 | Batch 99/198 | Loss: 3.1811108589172363\n","Epoch 12/150 | Batch 100/198 | Loss: 3.424163341522217\n","Epoch 12/150 | Batch 101/198 | Loss: 3.6198368072509766\n","Epoch 12/150 | Batch 102/198 | Loss: 3.072323799133301\n","Epoch 12/150 | Batch 103/198 | Loss: 3.4746737480163574\n","Epoch 12/150 | Batch 104/198 | Loss: 3.696140766143799\n","Epoch 12/150 | Batch 105/198 | Loss: 3.4646475315093994\n","Epoch 12/150 | Batch 106/198 | Loss: 3.0921125411987305\n","Epoch 12/150 | Batch 107/198 | Loss: 3.3114752769470215\n","Epoch 12/150 | Batch 108/198 | Loss: 3.4829680919647217\n","Epoch 12/150 | Batch 109/198 | Loss: 3.3725340366363525\n","Epoch 12/150 | Batch 110/198 | Loss: 3.400921583175659\n","Epoch 12/150 | Batch 111/198 | Loss: 3.3652684688568115\n","Epoch 12/150 | Batch 112/198 | Loss: 3.14872670173645\n","Epoch 12/150 | Batch 113/198 | Loss: 3.387932300567627\n","Epoch 12/150 | Batch 114/198 | Loss: 3.6119017601013184\n","Epoch 12/150 | Batch 115/198 | Loss: 3.277468681335449\n","Epoch 12/150 | Batch 116/198 | Loss: 3.2186810970306396\n","Epoch 12/150 | Batch 117/198 | Loss: 3.3387861251831055\n","Epoch 12/150 | Batch 118/198 | Loss: 3.4439566135406494\n","Epoch 12/150 | Batch 119/198 | Loss: 3.1970741748809814\n","Epoch 12/150 | Batch 120/198 | Loss: 3.1695730686187744\n","Epoch 12/150 | Batch 121/198 | Loss: 3.602976083755493\n","Epoch 12/150 | Batch 122/198 | Loss: 3.5517354011535645\n","Epoch 12/150 | Batch 123/198 | Loss: 2.72198224067688\n","Epoch 12/150 | Batch 124/198 | Loss: 3.421243667602539\n","Epoch 12/150 | Batch 125/198 | Loss: 3.286409854888916\n","Epoch 12/150 | Batch 126/198 | Loss: 3.7591707706451416\n","Epoch 12/150 | Batch 127/198 | Loss: 3.429410696029663\n","Epoch 12/150 | Batch 128/198 | Loss: 3.09952449798584\n","Epoch 12/150 | Batch 129/198 | Loss: 3.2721107006073\n","Epoch 12/150 | Batch 130/198 | Loss: 3.3342857360839844\n","Epoch 12/150 | Batch 131/198 | Loss: 3.2335426807403564\n","Epoch 12/150 | Batch 132/198 | Loss: 3.6359357833862305\n","Epoch 12/150 | Batch 133/198 | Loss: 3.735416889190674\n","Epoch 12/150 | Batch 134/198 | Loss: 3.105992317199707\n","Epoch 12/150 | Batch 135/198 | Loss: 3.427152633666992\n","Epoch 12/150 | Batch 136/198 | Loss: 2.9942145347595215\n","Epoch 12/150 | Batch 137/198 | Loss: 3.1985769271850586\n","Epoch 12/150 | Batch 138/198 | Loss: 3.556094169616699\n","Epoch 12/150 | Batch 139/198 | Loss: 3.548499822616577\n","Epoch 12/150 | Batch 140/198 | Loss: 3.434760332107544\n","Epoch 12/150 | Batch 141/198 | Loss: 2.8271431922912598\n","Epoch 12/150 | Batch 142/198 | Loss: 3.5314629077911377\n","Epoch 12/150 | Batch 143/198 | Loss: 3.709373950958252\n","Epoch 12/150 | Batch 144/198 | Loss: 3.1786770820617676\n","Epoch 12/150 | Batch 145/198 | Loss: 3.4033286571502686\n","Epoch 12/150 | Batch 146/198 | Loss: 3.083359956741333\n","Epoch 12/150 | Batch 147/198 | Loss: 3.0412745475769043\n","Epoch 12/150 | Batch 148/198 | Loss: 3.3594729900360107\n","Epoch 12/150 | Batch 149/198 | Loss: 3.582486867904663\n","Epoch 12/150 | Batch 150/198 | Loss: 3.7026865482330322\n","Epoch 12/150 | Batch 151/198 | Loss: 3.381976366043091\n","Epoch 12/150 | Batch 152/198 | Loss: 3.0479705333709717\n","Epoch 12/150 | Batch 153/198 | Loss: 2.998093366622925\n","Epoch 12/150 | Batch 154/198 | Loss: 3.3026084899902344\n","Epoch 12/150 | Batch 155/198 | Loss: 3.6346724033355713\n","Epoch 12/150 | Batch 156/198 | Loss: 3.0171499252319336\n","Epoch 12/150 | Batch 157/198 | Loss: 3.515172243118286\n","Epoch 12/150 | Batch 158/198 | Loss: 3.4631991386413574\n","Epoch 12/150 | Batch 159/198 | Loss: 3.3604576587677\n","Epoch 12/150 | Batch 160/198 | Loss: 3.4990155696868896\n","Epoch 12/150 | Batch 161/198 | Loss: 3.274887800216675\n","Epoch 12/150 | Batch 162/198 | Loss: 3.1412904262542725\n","Epoch 12/150 | Batch 163/198 | Loss: 2.872184991836548\n","Epoch 12/150 | Batch 164/198 | Loss: 3.387579917907715\n","Epoch 12/150 | Batch 165/198 | Loss: 3.7124269008636475\n","Epoch 12/150 | Batch 166/198 | Loss: 3.6585330963134766\n","Epoch 12/150 | Batch 167/198 | Loss: 2.8917815685272217\n","Epoch 12/150 | Batch 168/198 | Loss: 3.5091373920440674\n","Epoch 12/150 | Batch 169/198 | Loss: 3.1477503776550293\n","Epoch 12/150 | Batch 170/198 | Loss: 3.094151496887207\n","Epoch 12/150 | Batch 171/198 | Loss: 3.1373958587646484\n","Epoch 12/150 | Batch 172/198 | Loss: 3.369506359100342\n","Epoch 12/150 | Batch 173/198 | Loss: 3.283884286880493\n","Epoch 12/150 | Batch 174/198 | Loss: 3.2309868335723877\n","Epoch 12/150 | Batch 175/198 | Loss: 3.2394187450408936\n","Epoch 12/150 | Batch 176/198 | Loss: 2.969236373901367\n","Epoch 12/150 | Batch 177/198 | Loss: 2.9625000953674316\n","Epoch 12/150 | Batch 178/198 | Loss: 3.51196026802063\n","Epoch 12/150 | Batch 179/198 | Loss: 3.398481607437134\n","Epoch 12/150 | Batch 180/198 | Loss: 3.562422752380371\n","Epoch 12/150 | Batch 181/198 | Loss: 3.7102925777435303\n","Epoch 12/150 | Batch 182/198 | Loss: 3.5197997093200684\n","Epoch 12/150 | Batch 183/198 | Loss: 3.103090286254883\n","Epoch 12/150 | Batch 184/198 | Loss: 3.659231185913086\n","Epoch 12/150 | Batch 185/198 | Loss: 3.0357766151428223\n","Epoch 12/150 | Batch 186/198 | Loss: 3.346299409866333\n","Epoch 12/150 | Batch 187/198 | Loss: 3.123656749725342\n","Epoch 12/150 | Batch 188/198 | Loss: 3.2385191917419434\n","Epoch 12/150 | Batch 189/198 | Loss: 3.298858880996704\n","Epoch 12/150 | Batch 190/198 | Loss: 3.1273140907287598\n","Epoch 12/150 | Batch 191/198 | Loss: 3.2145376205444336\n","Epoch 12/150 | Batch 192/198 | Loss: 3.141390800476074\n","Epoch 12/150 | Batch 193/198 | Loss: 3.522528886795044\n","Epoch 12/150 | Batch 194/198 | Loss: 3.1456105709075928\n","Epoch 12/150 | Batch 195/198 | Loss: 3.2829220294952393\n","Epoch 12/150 | Batch 196/198 | Loss: 2.9856879711151123\n","Epoch 12/150 | Batch 197/198 | Loss: 3.426013469696045\n","Epoch 12/150 | Batch 198/198 | Loss: 3.486304998397827\n","Epoch 12/150 | Average Loss: 3.312543688398419\n","Epoch 13/150 | Batch 1/198 | Loss: 3.497446298599243\n","Epoch 13/150 | Batch 2/198 | Loss: 2.9928743839263916\n","Epoch 13/150 | Batch 3/198 | Loss: 3.031402826309204\n","Epoch 13/150 | Batch 4/198 | Loss: 2.868476629257202\n","Epoch 13/150 | Batch 5/198 | Loss: 2.9984161853790283\n","Epoch 13/150 | Batch 6/198 | Loss: 3.1822657585144043\n","Epoch 13/150 | Batch 7/198 | Loss: 3.302978992462158\n","Epoch 13/150 | Batch 8/198 | Loss: 3.2361366748809814\n","Epoch 13/150 | Batch 9/198 | Loss: 3.125931978225708\n","Epoch 13/150 | Batch 10/198 | Loss: 3.025310516357422\n","Epoch 13/150 | Batch 11/198 | Loss: 3.180938243865967\n","Epoch 13/150 | Batch 12/198 | Loss: 3.1093711853027344\n","Epoch 13/150 | Batch 13/198 | Loss: 3.169358015060425\n","Epoch 13/150 | Batch 14/198 | Loss: 3.1677677631378174\n","Epoch 13/150 | Batch 15/198 | Loss: 3.061241626739502\n","Epoch 13/150 | Batch 16/198 | Loss: 3.0373048782348633\n","Epoch 13/150 | Batch 17/198 | Loss: 3.2094953060150146\n","Epoch 13/150 | Batch 18/198 | Loss: 3.070129156112671\n","Epoch 13/150 | Batch 19/198 | Loss: 3.035292625427246\n","Epoch 13/150 | Batch 20/198 | Loss: 3.277527093887329\n","Epoch 13/150 | Batch 21/198 | Loss: 2.4158575534820557\n","Epoch 13/150 | Batch 22/198 | Loss: 3.2140443325042725\n","Epoch 13/150 | Batch 23/198 | Loss: 3.4894678592681885\n","Epoch 13/150 | Batch 24/198 | Loss: 3.1836118698120117\n","Epoch 13/150 | Batch 25/198 | Loss: 3.4618613719940186\n","Epoch 13/150 | Batch 26/198 | Loss: 3.270385503768921\n","Epoch 13/150 | Batch 27/198 | Loss: 3.246633529663086\n","Epoch 13/150 | Batch 28/198 | Loss: 3.0722575187683105\n","Epoch 13/150 | Batch 29/198 | Loss: 3.114362955093384\n","Epoch 13/150 | Batch 30/198 | Loss: 3.050086259841919\n","Epoch 13/150 | Batch 31/198 | Loss: 3.1626973152160645\n","Epoch 13/150 | Batch 32/198 | Loss: 2.8291141986846924\n","Epoch 13/150 | Batch 33/198 | Loss: 3.099665641784668\n","Epoch 13/150 | Batch 34/198 | Loss: 3.3729193210601807\n","Epoch 13/150 | Batch 35/198 | Loss: 3.0422327518463135\n","Epoch 13/150 | Batch 36/198 | Loss: 2.883671998977661\n","Epoch 13/150 | Batch 37/198 | Loss: 3.280764579772949\n","Epoch 13/150 | Batch 38/198 | Loss: 3.3613197803497314\n","Epoch 13/150 | Batch 39/198 | Loss: 3.3807475566864014\n","Epoch 13/150 | Batch 40/198 | Loss: 3.062518358230591\n","Epoch 13/150 | Batch 41/198 | Loss: 3.433675527572632\n","Epoch 13/150 | Batch 42/198 | Loss: 2.8189737796783447\n","Epoch 13/150 | Batch 43/198 | Loss: 3.522592544555664\n","Epoch 13/150 | Batch 44/198 | Loss: 2.733649969100952\n","Epoch 13/150 | Batch 45/198 | Loss: 3.079932689666748\n","Epoch 13/150 | Batch 46/198 | Loss: 2.8662376403808594\n","Epoch 13/150 | Batch 47/198 | Loss: 3.137080192565918\n","Epoch 13/150 | Batch 48/198 | Loss: 3.4210927486419678\n","Epoch 13/150 | Batch 49/198 | Loss: 3.0846645832061768\n","Epoch 13/150 | Batch 50/198 | Loss: 3.3253307342529297\n","Epoch 13/150 | Batch 51/198 | Loss: 3.0911450386047363\n","Epoch 13/150 | Batch 52/198 | Loss: 2.7386224269866943\n","Epoch 13/150 | Batch 53/198 | Loss: 3.649951457977295\n","Epoch 13/150 | Batch 54/198 | Loss: 3.0404715538024902\n","Epoch 13/150 | Batch 55/198 | Loss: 3.2071404457092285\n","Epoch 13/150 | Batch 56/198 | Loss: 3.0906858444213867\n","Epoch 13/150 | Batch 57/198 | Loss: 3.2133309841156006\n","Epoch 13/150 | Batch 58/198 | Loss: 2.948756694793701\n","Epoch 13/150 | Batch 59/198 | Loss: 3.0551209449768066\n","Epoch 13/150 | Batch 60/198 | Loss: 2.9209952354431152\n","Epoch 13/150 | Batch 61/198 | Loss: 3.0989928245544434\n","Epoch 13/150 | Batch 62/198 | Loss: 3.2461328506469727\n","Epoch 13/150 | Batch 63/198 | Loss: 2.873366594314575\n","Epoch 13/150 | Batch 64/198 | Loss: 3.3327600955963135\n","Epoch 13/150 | Batch 65/198 | Loss: 3.0127205848693848\n","Epoch 13/150 | Batch 66/198 | Loss: 2.8296899795532227\n","Epoch 13/150 | Batch 67/198 | Loss: 3.150252103805542\n","Epoch 13/150 | Batch 68/198 | Loss: 3.261889696121216\n","Epoch 13/150 | Batch 69/198 | Loss: 3.147745132446289\n","Epoch 13/150 | Batch 70/198 | Loss: 2.378108024597168\n","Epoch 13/150 | Batch 71/198 | Loss: 2.991811990737915\n","Epoch 13/150 | Batch 72/198 | Loss: 3.3972222805023193\n","Epoch 13/150 | Batch 73/198 | Loss: 3.0575788021087646\n","Epoch 13/150 | Batch 74/198 | Loss: 3.332749128341675\n","Epoch 13/150 | Batch 75/198 | Loss: 3.208061456680298\n","Epoch 13/150 | Batch 76/198 | Loss: 3.5278351306915283\n","Epoch 13/150 | Batch 77/198 | Loss: 2.8952338695526123\n","Epoch 13/150 | Batch 78/198 | Loss: 3.1555018424987793\n","Epoch 13/150 | Batch 79/198 | Loss: 2.999415397644043\n","Epoch 13/150 | Batch 80/198 | Loss: 3.221712589263916\n","Epoch 13/150 | Batch 81/198 | Loss: 3.0126070976257324\n","Epoch 13/150 | Batch 82/198 | Loss: 3.1807336807250977\n","Epoch 13/150 | Batch 83/198 | Loss: 3.184791088104248\n","Epoch 13/150 | Batch 84/198 | Loss: 3.245626449584961\n","Epoch 13/150 | Batch 85/198 | Loss: 3.144541025161743\n","Epoch 13/150 | Batch 86/198 | Loss: 3.0382325649261475\n","Epoch 13/150 | Batch 87/198 | Loss: 3.4097044467926025\n","Epoch 13/150 | Batch 88/198 | Loss: 3.4561991691589355\n","Epoch 13/150 | Batch 89/198 | Loss: 3.0663177967071533\n","Epoch 13/150 | Batch 90/198 | Loss: 2.920393705368042\n","Epoch 13/150 | Batch 91/198 | Loss: 2.7700934410095215\n","Epoch 13/150 | Batch 92/198 | Loss: 2.8615329265594482\n","Epoch 13/150 | Batch 93/198 | Loss: 3.121391534805298\n","Epoch 13/150 | Batch 94/198 | Loss: 3.5532686710357666\n","Epoch 13/150 | Batch 95/198 | Loss: 3.470003843307495\n","Epoch 13/150 | Batch 96/198 | Loss: 3.285773992538452\n","Epoch 13/150 | Batch 97/198 | Loss: 3.4395272731781006\n","Epoch 13/150 | Batch 98/198 | Loss: 3.4885926246643066\n","Epoch 13/150 | Batch 99/198 | Loss: 3.027266263961792\n","Epoch 13/150 | Batch 100/198 | Loss: 3.094273090362549\n","Epoch 13/150 | Batch 101/198 | Loss: 3.1646902561187744\n","Epoch 13/150 | Batch 102/198 | Loss: 3.1778950691223145\n","Epoch 13/150 | Batch 103/198 | Loss: 3.118636131286621\n","Epoch 13/150 | Batch 104/198 | Loss: 3.141000747680664\n","Epoch 13/150 | Batch 105/198 | Loss: 3.737222671508789\n","Epoch 13/150 | Batch 106/198 | Loss: 3.208113670349121\n","Epoch 13/150 | Batch 107/198 | Loss: 3.04620361328125\n","Epoch 13/150 | Batch 108/198 | Loss: 3.3098065853118896\n","Epoch 13/150 | Batch 109/198 | Loss: 2.9286203384399414\n","Epoch 13/150 | Batch 110/198 | Loss: 3.3515613079071045\n","Epoch 13/150 | Batch 111/198 | Loss: 3.3434743881225586\n","Epoch 13/150 | Batch 112/198 | Loss: 2.950317144393921\n","Epoch 13/150 | Batch 113/198 | Loss: 3.355750322341919\n","Epoch 13/150 | Batch 114/198 | Loss: 3.149336814880371\n","Epoch 13/150 | Batch 115/198 | Loss: 2.9406821727752686\n","Epoch 13/150 | Batch 116/198 | Loss: 3.416172504425049\n","Epoch 13/150 | Batch 117/198 | Loss: 3.0770838260650635\n","Epoch 13/150 | Batch 118/198 | Loss: 2.9348299503326416\n","Epoch 13/150 | Batch 119/198 | Loss: 3.3287746906280518\n","Epoch 13/150 | Batch 120/198 | Loss: 3.00160813331604\n","Epoch 13/150 | Batch 121/198 | Loss: 3.128277063369751\n","Epoch 13/150 | Batch 122/198 | Loss: 3.476522922515869\n","Epoch 13/150 | Batch 123/198 | Loss: 3.319174289703369\n","Epoch 13/150 | Batch 124/198 | Loss: 2.8143537044525146\n","Epoch 13/150 | Batch 125/198 | Loss: 3.4260759353637695\n","Epoch 13/150 | Batch 126/198 | Loss: 2.677593231201172\n","Epoch 13/150 | Batch 127/198 | Loss: 3.2056190967559814\n","Epoch 13/150 | Batch 128/198 | Loss: 3.383226156234741\n","Epoch 13/150 | Batch 129/198 | Loss: 2.7509267330169678\n","Epoch 13/150 | Batch 130/198 | Loss: 3.138526201248169\n","Epoch 13/150 | Batch 131/198 | Loss: 3.228578805923462\n","Epoch 13/150 | Batch 132/198 | Loss: 2.7494311332702637\n","Epoch 13/150 | Batch 133/198 | Loss: 2.843329668045044\n","Epoch 13/150 | Batch 134/198 | Loss: 2.8457162380218506\n","Epoch 13/150 | Batch 135/198 | Loss: 2.8867907524108887\n","Epoch 13/150 | Batch 136/198 | Loss: 3.1930429935455322\n","Epoch 13/150 | Batch 137/198 | Loss: 3.263652801513672\n","Epoch 13/150 | Batch 138/198 | Loss: 3.386352300643921\n","Epoch 13/150 | Batch 139/198 | Loss: 2.936962842941284\n","Epoch 13/150 | Batch 140/198 | Loss: 3.266848564147949\n","Epoch 13/150 | Batch 141/198 | Loss: 3.3737215995788574\n","Epoch 13/150 | Batch 142/198 | Loss: 3.5763847827911377\n","Epoch 13/150 | Batch 143/198 | Loss: 2.876903772354126\n","Epoch 13/150 | Batch 144/198 | Loss: 3.3725292682647705\n","Epoch 13/150 | Batch 145/198 | Loss: 2.8884360790252686\n","Epoch 13/150 | Batch 146/198 | Loss: 3.717698335647583\n","Epoch 13/150 | Batch 147/198 | Loss: 3.0266380310058594\n","Epoch 13/150 | Batch 148/198 | Loss: 3.5770678520202637\n","Epoch 13/150 | Batch 149/198 | Loss: 3.629502534866333\n","Epoch 13/150 | Batch 150/198 | Loss: 3.0977413654327393\n","Epoch 13/150 | Batch 151/198 | Loss: 2.5541532039642334\n","Epoch 13/150 | Batch 152/198 | Loss: 3.2352659702301025\n","Epoch 13/150 | Batch 153/198 | Loss: 3.137592077255249\n","Epoch 13/150 | Batch 154/198 | Loss: 3.3889858722686768\n","Epoch 13/150 | Batch 155/198 | Loss: 3.459334135055542\n","Epoch 13/150 | Batch 156/198 | Loss: 3.461209297180176\n","Epoch 13/150 | Batch 157/198 | Loss: 3.337444543838501\n","Epoch 13/150 | Batch 158/198 | Loss: 3.0135457515716553\n","Epoch 13/150 | Batch 159/198 | Loss: 3.2283496856689453\n","Epoch 13/150 | Batch 160/198 | Loss: 3.0535991191864014\n","Epoch 13/150 | Batch 161/198 | Loss: 3.2921855449676514\n","Epoch 13/150 | Batch 162/198 | Loss: 3.0597167015075684\n","Epoch 13/150 | Batch 163/198 | Loss: 3.198521614074707\n","Epoch 13/150 | Batch 164/198 | Loss: 3.1543545722961426\n","Epoch 13/150 | Batch 165/198 | Loss: 3.343104362487793\n","Epoch 13/150 | Batch 166/198 | Loss: 3.482614755630493\n","Epoch 13/150 | Batch 167/198 | Loss: 3.1300902366638184\n","Epoch 13/150 | Batch 168/198 | Loss: 3.0100762844085693\n","Epoch 13/150 | Batch 169/198 | Loss: 3.687803268432617\n","Epoch 13/150 | Batch 170/198 | Loss: 2.85436749458313\n","Epoch 13/150 | Batch 171/198 | Loss: 2.892763376235962\n","Epoch 13/150 | Batch 172/198 | Loss: 3.0158345699310303\n","Epoch 13/150 | Batch 173/198 | Loss: 2.932415246963501\n","Epoch 13/150 | Batch 174/198 | Loss: 2.6878788471221924\n","Epoch 13/150 | Batch 175/198 | Loss: 3.2697854042053223\n","Epoch 13/150 | Batch 176/198 | Loss: 3.1907238960266113\n","Epoch 13/150 | Batch 177/198 | Loss: 3.3274240493774414\n","Epoch 13/150 | Batch 178/198 | Loss: 3.2292051315307617\n","Epoch 13/150 | Batch 179/198 | Loss: 3.407902717590332\n","Epoch 13/150 | Batch 180/198 | Loss: 3.395838499069214\n","Epoch 13/150 | Batch 181/198 | Loss: 3.384437322616577\n","Epoch 13/150 | Batch 182/198 | Loss: 2.8825438022613525\n","Epoch 13/150 | Batch 183/198 | Loss: 2.968986749649048\n","Epoch 13/150 | Batch 184/198 | Loss: 2.8604800701141357\n","Epoch 13/150 | Batch 185/198 | Loss: 3.560636043548584\n","Epoch 13/150 | Batch 186/198 | Loss: 3.6718547344207764\n","Epoch 13/150 | Batch 187/198 | Loss: 3.1144375801086426\n","Epoch 13/150 | Batch 188/198 | Loss: 3.184091091156006\n","Epoch 13/150 | Batch 189/198 | Loss: 3.0947461128234863\n","Epoch 13/150 | Batch 190/198 | Loss: 3.0233511924743652\n","Epoch 13/150 | Batch 191/198 | Loss: 3.421236753463745\n","Epoch 13/150 | Batch 192/198 | Loss: 3.3738231658935547\n","Epoch 13/150 | Batch 193/198 | Loss: 3.1344730854034424\n","Epoch 13/150 | Batch 194/198 | Loss: 2.9886252880096436\n","Epoch 13/150 | Batch 195/198 | Loss: 2.902376890182495\n","Epoch 13/150 | Batch 196/198 | Loss: 3.0981545448303223\n","Epoch 13/150 | Batch 197/198 | Loss: 2.3716535568237305\n","Epoch 13/150 | Batch 198/198 | Loss: 2.767608165740967\n","Epoch 13/150 | Average Loss: 3.150595940724768\n","Epoch 14/150 | Batch 1/198 | Loss: 2.7828896045684814\n","Epoch 14/150 | Batch 2/198 | Loss: 3.3610804080963135\n","Epoch 14/150 | Batch 3/198 | Loss: 3.200873613357544\n","Epoch 14/150 | Batch 4/198 | Loss: 3.1466150283813477\n","Epoch 14/150 | Batch 5/198 | Loss: 2.6619303226470947\n","Epoch 14/150 | Batch 6/198 | Loss: 3.1404976844787598\n","Epoch 14/150 | Batch 7/198 | Loss: 2.6950690746307373\n","Epoch 14/150 | Batch 8/198 | Loss: 2.863656997680664\n","Epoch 14/150 | Batch 9/198 | Loss: 2.752178907394409\n","Epoch 14/150 | Batch 10/198 | Loss: 2.7239484786987305\n","Epoch 14/150 | Batch 11/198 | Loss: 2.9168152809143066\n","Epoch 14/150 | Batch 12/198 | Loss: 2.898275852203369\n","Epoch 14/150 | Batch 13/198 | Loss: 2.8301351070404053\n","Epoch 14/150 | Batch 14/198 | Loss: 2.6988706588745117\n","Epoch 14/150 | Batch 15/198 | Loss: 3.1889078617095947\n","Epoch 14/150 | Batch 16/198 | Loss: 3.4500105381011963\n","Epoch 14/150 | Batch 17/198 | Loss: 2.910698652267456\n","Epoch 14/150 | Batch 18/198 | Loss: 3.2200493812561035\n","Epoch 14/150 | Batch 19/198 | Loss: 2.611098527908325\n","Epoch 14/150 | Batch 20/198 | Loss: 3.3879706859588623\n","Epoch 14/150 | Batch 21/198 | Loss: 3.019911050796509\n","Epoch 14/150 | Batch 22/198 | Loss: 2.613571882247925\n","Epoch 14/150 | Batch 23/198 | Loss: 2.9939303398132324\n","Epoch 14/150 | Batch 24/198 | Loss: 2.7989661693573\n","Epoch 14/150 | Batch 25/198 | Loss: 2.9121921062469482\n","Epoch 14/150 | Batch 26/198 | Loss: 3.0442137718200684\n","Epoch 14/150 | Batch 27/198 | Loss: 3.16848087310791\n","Epoch 14/150 | Batch 28/198 | Loss: 3.0216784477233887\n","Epoch 14/150 | Batch 29/198 | Loss: 3.010659694671631\n","Epoch 14/150 | Batch 30/198 | Loss: 2.961618423461914\n","Epoch 14/150 | Batch 31/198 | Loss: 3.0544607639312744\n","Epoch 14/150 | Batch 32/198 | Loss: 3.0750892162323\n","Epoch 14/150 | Batch 33/198 | Loss: 3.2852869033813477\n","Epoch 14/150 | Batch 34/198 | Loss: 2.9478020668029785\n","Epoch 14/150 | Batch 35/198 | Loss: 3.0191900730133057\n","Epoch 14/150 | Batch 36/198 | Loss: 3.1157751083374023\n","Epoch 14/150 | Batch 37/198 | Loss: 2.791179895401001\n","Epoch 14/150 | Batch 38/198 | Loss: 3.0542969703674316\n","Epoch 14/150 | Batch 39/198 | Loss: 2.9298083782196045\n","Epoch 14/150 | Batch 40/198 | Loss: 2.7950806617736816\n","Epoch 14/150 | Batch 41/198 | Loss: 2.933915853500366\n","Epoch 14/150 | Batch 42/198 | Loss: 3.1050047874450684\n","Epoch 14/150 | Batch 43/198 | Loss: 2.85339617729187\n","Epoch 14/150 | Batch 44/198 | Loss: 2.9896280765533447\n","Epoch 14/150 | Batch 45/198 | Loss: 2.3569018840789795\n","Epoch 14/150 | Batch 46/198 | Loss: 2.708566427230835\n","Epoch 14/150 | Batch 47/198 | Loss: 3.012660503387451\n","Epoch 14/150 | Batch 48/198 | Loss: 2.9365649223327637\n","Epoch 14/150 | Batch 49/198 | Loss: 2.815946102142334\n","Epoch 14/150 | Batch 50/198 | Loss: 3.4780404567718506\n","Epoch 14/150 | Batch 51/198 | Loss: 3.059145927429199\n","Epoch 14/150 | Batch 52/198 | Loss: 3.124610662460327\n","Epoch 14/150 | Batch 53/198 | Loss: 3.0254199504852295\n","Epoch 14/150 | Batch 54/198 | Loss: 3.1142702102661133\n","Epoch 14/150 | Batch 55/198 | Loss: 2.7703919410705566\n","Epoch 14/150 | Batch 56/198 | Loss: 3.05070424079895\n","Epoch 14/150 | Batch 57/198 | Loss: 3.262270450592041\n","Epoch 14/150 | Batch 58/198 | Loss: 3.2730872631073\n","Epoch 14/150 | Batch 59/198 | Loss: 2.861694574356079\n","Epoch 14/150 | Batch 60/198 | Loss: 2.9398553371429443\n","Epoch 14/150 | Batch 61/198 | Loss: 2.8164098262786865\n","Epoch 14/150 | Batch 62/198 | Loss: 2.8104045391082764\n","Epoch 14/150 | Batch 63/198 | Loss: 2.7895395755767822\n","Epoch 14/150 | Batch 64/198 | Loss: 3.4495928287506104\n","Epoch 14/150 | Batch 65/198 | Loss: 3.4012482166290283\n","Epoch 14/150 | Batch 66/198 | Loss: 3.055318832397461\n","Epoch 14/150 | Batch 67/198 | Loss: 2.9089691638946533\n","Epoch 14/150 | Batch 68/198 | Loss: 3.3542962074279785\n","Epoch 14/150 | Batch 69/198 | Loss: 3.3156864643096924\n","Epoch 14/150 | Batch 70/198 | Loss: 3.1873583793640137\n","Epoch 14/150 | Batch 71/198 | Loss: 2.697402238845825\n","Epoch 14/150 | Batch 72/198 | Loss: 2.7562150955200195\n","Epoch 14/150 | Batch 73/198 | Loss: 2.9356842041015625\n","Epoch 14/150 | Batch 74/198 | Loss: 2.5585172176361084\n","Epoch 14/150 | Batch 75/198 | Loss: 2.9114832878112793\n","Epoch 14/150 | Batch 76/198 | Loss: 2.989321231842041\n","Epoch 14/150 | Batch 77/198 | Loss: 3.0028727054595947\n","Epoch 14/150 | Batch 78/198 | Loss: 3.3056530952453613\n","Epoch 14/150 | Batch 79/198 | Loss: 2.9516847133636475\n","Epoch 14/150 | Batch 80/198 | Loss: 3.288224935531616\n","Epoch 14/150 | Batch 81/198 | Loss: 2.7389698028564453\n","Epoch 14/150 | Batch 82/198 | Loss: 3.083028793334961\n","Epoch 14/150 | Batch 83/198 | Loss: 3.5234179496765137\n","Epoch 14/150 | Batch 84/198 | Loss: 3.1483774185180664\n","Epoch 14/150 | Batch 85/198 | Loss: 2.8187339305877686\n","Epoch 14/150 | Batch 86/198 | Loss: 2.9114723205566406\n","Epoch 14/150 | Batch 87/198 | Loss: 2.796079397201538\n","Epoch 14/150 | Batch 88/198 | Loss: 3.198598861694336\n","Epoch 14/150 | Batch 89/198 | Loss: 3.105602264404297\n","Epoch 14/150 | Batch 90/198 | Loss: 2.6292107105255127\n","Epoch 14/150 | Batch 91/198 | Loss: 2.8197715282440186\n","Epoch 14/150 | Batch 92/198 | Loss: 2.972081184387207\n","Epoch 14/150 | Batch 93/198 | Loss: 2.8892292976379395\n","Epoch 14/150 | Batch 94/198 | Loss: 2.896819829940796\n","Epoch 14/150 | Batch 95/198 | Loss: 2.766430139541626\n","Epoch 14/150 | Batch 96/198 | Loss: 3.2361900806427\n","Epoch 14/150 | Batch 97/198 | Loss: 3.0232880115509033\n","Epoch 14/150 | Batch 98/198 | Loss: 3.2150819301605225\n","Epoch 14/150 | Batch 99/198 | Loss: 2.8495609760284424\n","Epoch 14/150 | Batch 100/198 | Loss: 2.915501832962036\n","Epoch 14/150 | Batch 101/198 | Loss: 3.1564342975616455\n","Epoch 14/150 | Batch 102/198 | Loss: 3.013122797012329\n","Epoch 14/150 | Batch 103/198 | Loss: 3.0082337856292725\n","Epoch 14/150 | Batch 104/198 | Loss: 3.077298641204834\n","Epoch 14/150 | Batch 105/198 | Loss: 2.778578996658325\n","Epoch 14/150 | Batch 106/198 | Loss: 3.156956672668457\n","Epoch 14/150 | Batch 107/198 | Loss: 3.0948920249938965\n","Epoch 14/150 | Batch 108/198 | Loss: 3.0176656246185303\n","Epoch 14/150 | Batch 109/198 | Loss: 2.568411350250244\n","Epoch 14/150 | Batch 110/198 | Loss: 2.7676868438720703\n","Epoch 14/150 | Batch 111/198 | Loss: 2.853285074234009\n","Epoch 14/150 | Batch 112/198 | Loss: 2.944992780685425\n","Epoch 14/150 | Batch 113/198 | Loss: 3.0010581016540527\n","Epoch 14/150 | Batch 114/198 | Loss: 2.770853281021118\n","Epoch 14/150 | Batch 115/198 | Loss: 2.7328941822052\n","Epoch 14/150 | Batch 116/198 | Loss: 2.877638578414917\n","Epoch 14/150 | Batch 117/198 | Loss: 2.9731943607330322\n","Epoch 14/150 | Batch 118/198 | Loss: 2.817096710205078\n","Epoch 14/150 | Batch 119/198 | Loss: 2.8986995220184326\n","Epoch 14/150 | Batch 120/198 | Loss: 3.011068820953369\n","Epoch 14/150 | Batch 121/198 | Loss: 3.4471940994262695\n","Epoch 14/150 | Batch 122/198 | Loss: 3.316348075866699\n","Epoch 14/150 | Batch 123/198 | Loss: 3.2072107791900635\n","Epoch 14/150 | Batch 124/198 | Loss: 2.652613401412964\n","Epoch 14/150 | Batch 125/198 | Loss: 2.5574071407318115\n","Epoch 14/150 | Batch 126/198 | Loss: 3.338545083999634\n","Epoch 14/150 | Batch 127/198 | Loss: 2.6938230991363525\n","Epoch 14/150 | Batch 128/198 | Loss: 2.96065354347229\n","Epoch 14/150 | Batch 129/198 | Loss: 2.920520544052124\n","Epoch 14/150 | Batch 130/198 | Loss: 2.9544637203216553\n","Epoch 14/150 | Batch 131/198 | Loss: 3.070892333984375\n","Epoch 14/150 | Batch 132/198 | Loss: 3.1723475456237793\n","Epoch 14/150 | Batch 133/198 | Loss: 2.933054208755493\n","Epoch 14/150 | Batch 134/198 | Loss: 3.3269407749176025\n","Epoch 14/150 | Batch 135/198 | Loss: 2.8194620609283447\n","Epoch 14/150 | Batch 136/198 | Loss: 2.989346981048584\n","Epoch 14/150 | Batch 137/198 | Loss: 2.8289403915405273\n","Epoch 14/150 | Batch 138/198 | Loss: 2.932204246520996\n","Epoch 14/150 | Batch 139/198 | Loss: 2.7869696617126465\n","Epoch 14/150 | Batch 140/198 | Loss: 2.558828830718994\n","Epoch 14/150 | Batch 141/198 | Loss: 3.0613508224487305\n","Epoch 14/150 | Batch 142/198 | Loss: 3.0307557582855225\n","Epoch 14/150 | Batch 143/198 | Loss: 2.8746888637542725\n","Epoch 14/150 | Batch 144/198 | Loss: 2.8983166217803955\n","Epoch 14/150 | Batch 145/198 | Loss: 3.193145751953125\n","Epoch 14/150 | Batch 146/198 | Loss: 3.1748600006103516\n","Epoch 14/150 | Batch 147/198 | Loss: 3.2189624309539795\n","Epoch 14/150 | Batch 148/198 | Loss: 3.182246208190918\n","Epoch 14/150 | Batch 149/198 | Loss: 3.0580356121063232\n","Epoch 14/150 | Batch 150/198 | Loss: 3.2415764331817627\n","Epoch 14/150 | Batch 151/198 | Loss: 2.932182550430298\n","Epoch 14/150 | Batch 152/198 | Loss: 2.8215019702911377\n","Epoch 14/150 | Batch 153/198 | Loss: 2.9211504459381104\n","Epoch 14/150 | Batch 154/198 | Loss: 2.8164470195770264\n","Epoch 14/150 | Batch 155/198 | Loss: 3.288877010345459\n","Epoch 14/150 | Batch 156/198 | Loss: 2.9072399139404297\n","Epoch 14/150 | Batch 157/198 | Loss: 2.7390763759613037\n","Epoch 14/150 | Batch 158/198 | Loss: 3.273845672607422\n","Epoch 14/150 | Batch 159/198 | Loss: 3.284714698791504\n","Epoch 14/150 | Batch 160/198 | Loss: 3.5516669750213623\n","Epoch 14/150 | Batch 161/198 | Loss: 2.9062397480010986\n","Epoch 14/150 | Batch 162/198 | Loss: 2.853031635284424\n","Epoch 14/150 | Batch 163/198 | Loss: 2.8435425758361816\n","Epoch 14/150 | Batch 164/198 | Loss: 2.8459012508392334\n","Epoch 14/150 | Batch 165/198 | Loss: 2.892091751098633\n","Epoch 14/150 | Batch 166/198 | Loss: 2.9170501232147217\n","Epoch 14/150 | Batch 167/198 | Loss: 3.445516347885132\n","Epoch 14/150 | Batch 168/198 | Loss: 3.2439517974853516\n","Epoch 14/150 | Batch 169/198 | Loss: 3.0687994956970215\n","Epoch 14/150 | Batch 170/198 | Loss: 2.8672053813934326\n","Epoch 14/150 | Batch 171/198 | Loss: 3.0146334171295166\n","Epoch 14/150 | Batch 172/198 | Loss: 2.7714295387268066\n","Epoch 14/150 | Batch 173/198 | Loss: 2.873617649078369\n","Epoch 14/150 | Batch 174/198 | Loss: 2.908794641494751\n","Epoch 14/150 | Batch 175/198 | Loss: 3.111616849899292\n","Epoch 14/150 | Batch 176/198 | Loss: 3.331092357635498\n","Epoch 14/150 | Batch 177/198 | Loss: 3.2500998973846436\n","Epoch 14/150 | Batch 178/198 | Loss: 3.218787670135498\n","Epoch 14/150 | Batch 179/198 | Loss: 3.3860585689544678\n","Epoch 14/150 | Batch 180/198 | Loss: 2.944247007369995\n","Epoch 14/150 | Batch 181/198 | Loss: 3.080596923828125\n","Epoch 14/150 | Batch 182/198 | Loss: 2.7503836154937744\n","Epoch 14/150 | Batch 183/198 | Loss: 2.860238790512085\n","Epoch 14/150 | Batch 184/198 | Loss: 3.04988956451416\n","Epoch 14/150 | Batch 185/198 | Loss: 2.7739224433898926\n","Epoch 14/150 | Batch 186/198 | Loss: 3.143648386001587\n","Epoch 14/150 | Batch 187/198 | Loss: 2.518627405166626\n","Epoch 14/150 | Batch 188/198 | Loss: 3.407150983810425\n","Epoch 14/150 | Batch 189/198 | Loss: 3.0893497467041016\n","Epoch 14/150 | Batch 190/198 | Loss: 3.2045083045959473\n","Epoch 14/150 | Batch 191/198 | Loss: 2.6232662200927734\n","Epoch 14/150 | Batch 192/198 | Loss: 2.5823869705200195\n","Epoch 14/150 | Batch 193/198 | Loss: 3.3935773372650146\n","Epoch 14/150 | Batch 194/198 | Loss: 3.091888189315796\n","Epoch 14/150 | Batch 195/198 | Loss: 3.0809214115142822\n","Epoch 14/150 | Batch 196/198 | Loss: 2.965764284133911\n","Epoch 14/150 | Batch 197/198 | Loss: 3.0659806728363037\n","Epoch 14/150 | Batch 198/198 | Loss: 3.174254894256592\n","Epoch 14/150 | Average Loss: 2.9937308292196256\n","Epoch 15/150 | Batch 1/198 | Loss: 2.642636299133301\n","Epoch 15/150 | Batch 2/198 | Loss: 2.824968099594116\n","Epoch 15/150 | Batch 3/198 | Loss: 2.5013539791107178\n","Epoch 15/150 | Batch 4/198 | Loss: 2.7911863327026367\n","Epoch 15/150 | Batch 5/198 | Loss: 3.3156962394714355\n","Epoch 15/150 | Batch 6/198 | Loss: 2.6819746494293213\n","Epoch 15/150 | Batch 7/198 | Loss: 2.8377301692962646\n","Epoch 15/150 | Batch 8/198 | Loss: 3.250927448272705\n","Epoch 15/150 | Batch 9/198 | Loss: 2.522088050842285\n","Epoch 15/150 | Batch 10/198 | Loss: 2.4712636470794678\n","Epoch 15/150 | Batch 11/198 | Loss: 2.8091907501220703\n","Epoch 15/150 | Batch 12/198 | Loss: 2.8058345317840576\n","Epoch 15/150 | Batch 13/198 | Loss: 2.673598289489746\n","Epoch 15/150 | Batch 14/198 | Loss: 2.482639789581299\n","Epoch 15/150 | Batch 15/198 | Loss: 2.808042287826538\n","Epoch 15/150 | Batch 16/198 | Loss: 2.592642068862915\n","Epoch 15/150 | Batch 17/198 | Loss: 2.8174266815185547\n","Epoch 15/150 | Batch 18/198 | Loss: 2.895326852798462\n","Epoch 15/150 | Batch 19/198 | Loss: 2.7970662117004395\n","Epoch 15/150 | Batch 20/198 | Loss: 2.8708581924438477\n","Epoch 15/150 | Batch 21/198 | Loss: 2.7602477073669434\n","Epoch 15/150 | Batch 22/198 | Loss: 2.464284658432007\n","Epoch 15/150 | Batch 23/198 | Loss: 2.9660794734954834\n","Epoch 15/150 | Batch 24/198 | Loss: 2.934877634048462\n","Epoch 15/150 | Batch 25/198 | Loss: 2.9443633556365967\n","Epoch 15/150 | Batch 26/198 | Loss: 2.5530614852905273\n","Epoch 15/150 | Batch 27/198 | Loss: 2.8041350841522217\n","Epoch 15/150 | Batch 28/198 | Loss: 2.8396239280700684\n","Epoch 15/150 | Batch 29/198 | Loss: 2.7373695373535156\n","Epoch 15/150 | Batch 30/198 | Loss: 2.6527631282806396\n","Epoch 15/150 | Batch 31/198 | Loss: 2.8016600608825684\n","Epoch 15/150 | Batch 32/198 | Loss: 2.5995805263519287\n","Epoch 15/150 | Batch 33/198 | Loss: 2.8186662197113037\n","Epoch 15/150 | Batch 34/198 | Loss: 3.001213312149048\n","Epoch 15/150 | Batch 35/198 | Loss: 2.8781192302703857\n","Epoch 15/150 | Batch 36/198 | Loss: 2.7552404403686523\n","Epoch 15/150 | Batch 37/198 | Loss: 2.57987904548645\n","Epoch 15/150 | Batch 38/198 | Loss: 2.7074880599975586\n","Epoch 15/150 | Batch 39/198 | Loss: 2.654038667678833\n","Epoch 15/150 | Batch 40/198 | Loss: 2.8013134002685547\n","Epoch 15/150 | Batch 41/198 | Loss: 2.704549789428711\n","Epoch 15/150 | Batch 42/198 | Loss: 2.8882791996002197\n","Epoch 15/150 | Batch 43/198 | Loss: 2.9637937545776367\n","Epoch 15/150 | Batch 44/198 | Loss: 3.0586066246032715\n","Epoch 15/150 | Batch 45/198 | Loss: 2.9177582263946533\n","Epoch 15/150 | Batch 46/198 | Loss: 3.0805630683898926\n","Epoch 15/150 | Batch 47/198 | Loss: 2.706658124923706\n","Epoch 15/150 | Batch 48/198 | Loss: 3.141930341720581\n","Epoch 15/150 | Batch 49/198 | Loss: 2.85941481590271\n","Epoch 15/150 | Batch 50/198 | Loss: 2.9363672733306885\n","Epoch 15/150 | Batch 51/198 | Loss: 2.991809606552124\n","Epoch 15/150 | Batch 52/198 | Loss: 2.7766366004943848\n","Epoch 15/150 | Batch 53/198 | Loss: 2.57962703704834\n","Epoch 15/150 | Batch 54/198 | Loss: 3.1550943851470947\n","Epoch 15/150 | Batch 55/198 | Loss: 2.7958383560180664\n","Epoch 15/150 | Batch 56/198 | Loss: 2.8988544940948486\n","Epoch 15/150 | Batch 57/198 | Loss: 2.9649596214294434\n","Epoch 15/150 | Batch 58/198 | Loss: 2.922856569290161\n","Epoch 15/150 | Batch 59/198 | Loss: 3.295583486557007\n","Epoch 15/150 | Batch 60/198 | Loss: 2.9410929679870605\n","Epoch 15/150 | Batch 61/198 | Loss: 2.8543701171875\n","Epoch 15/150 | Batch 62/198 | Loss: 2.7980847358703613\n","Epoch 15/150 | Batch 63/198 | Loss: 2.8763909339904785\n","Epoch 15/150 | Batch 64/198 | Loss: 2.992536783218384\n","Epoch 15/150 | Batch 65/198 | Loss: 2.575709819793701\n","Epoch 15/150 | Batch 66/198 | Loss: 2.801036834716797\n","Epoch 15/150 | Batch 67/198 | Loss: 2.2708077430725098\n","Epoch 15/150 | Batch 68/198 | Loss: 3.00726580619812\n","Epoch 15/150 | Batch 69/198 | Loss: 2.77343487739563\n","Epoch 15/150 | Batch 70/198 | Loss: 3.127547264099121\n","Epoch 15/150 | Batch 71/198 | Loss: 2.7670023441314697\n","Epoch 15/150 | Batch 72/198 | Loss: 2.978621244430542\n","Epoch 15/150 | Batch 73/198 | Loss: 2.5340051651000977\n","Epoch 15/150 | Batch 74/198 | Loss: 3.1565146446228027\n","Epoch 15/150 | Batch 75/198 | Loss: 3.1418206691741943\n","Epoch 15/150 | Batch 76/198 | Loss: 3.082217216491699\n","Epoch 15/150 | Batch 77/198 | Loss: 2.868018627166748\n","Epoch 15/150 | Batch 78/198 | Loss: 2.635103702545166\n","Epoch 15/150 | Batch 79/198 | Loss: 2.789811849594116\n","Epoch 15/150 | Batch 80/198 | Loss: 2.8651769161224365\n","Epoch 15/150 | Batch 81/198 | Loss: 2.9788777828216553\n","Epoch 15/150 | Batch 82/198 | Loss: 3.1425209045410156\n","Epoch 15/150 | Batch 83/198 | Loss: 2.509533643722534\n","Epoch 15/150 | Batch 84/198 | Loss: 2.8297619819641113\n","Epoch 15/150 | Batch 85/198 | Loss: 2.8533382415771484\n","Epoch 15/150 | Batch 86/198 | Loss: 2.827376365661621\n","Epoch 15/150 | Batch 87/198 | Loss: 2.6756343841552734\n","Epoch 15/150 | Batch 88/198 | Loss: 2.393526315689087\n","Epoch 15/150 | Batch 89/198 | Loss: 2.7529780864715576\n","Epoch 15/150 | Batch 90/198 | Loss: 2.7880635261535645\n","Epoch 15/150 | Batch 91/198 | Loss: 2.551281213760376\n","Epoch 15/150 | Batch 92/198 | Loss: 3.1673543453216553\n","Epoch 15/150 | Batch 93/198 | Loss: 2.673224687576294\n","Epoch 15/150 | Batch 94/198 | Loss: 2.614804744720459\n","Epoch 15/150 | Batch 95/198 | Loss: 2.3727469444274902\n","Epoch 15/150 | Batch 96/198 | Loss: 2.5668299198150635\n","Epoch 15/150 | Batch 97/198 | Loss: 2.821416139602661\n","Epoch 15/150 | Batch 98/198 | Loss: 2.9775538444519043\n","Epoch 15/150 | Batch 99/198 | Loss: 3.1101930141448975\n","Epoch 15/150 | Batch 100/198 | Loss: 2.7008326053619385\n","Epoch 15/150 | Batch 101/198 | Loss: 2.971519947052002\n","Epoch 15/150 | Batch 102/198 | Loss: 3.416146755218506\n","Epoch 15/150 | Batch 103/198 | Loss: 2.9142277240753174\n","Epoch 15/150 | Batch 104/198 | Loss: 3.1823489665985107\n","Epoch 15/150 | Batch 105/198 | Loss: 2.5773298740386963\n","Epoch 15/150 | Batch 106/198 | Loss: 2.829479455947876\n","Epoch 15/150 | Batch 107/198 | Loss: 2.8581490516662598\n","Epoch 15/150 | Batch 108/198 | Loss: 3.0608158111572266\n","Epoch 15/150 | Batch 109/198 | Loss: 2.761532783508301\n","Epoch 15/150 | Batch 110/198 | Loss: 3.134326219558716\n","Epoch 15/150 | Batch 111/198 | Loss: 3.161705255508423\n","Epoch 15/150 | Batch 112/198 | Loss: 2.9995903968811035\n","Epoch 15/150 | Batch 113/198 | Loss: 3.031104564666748\n","Epoch 15/150 | Batch 114/198 | Loss: 2.642374277114868\n","Epoch 15/150 | Batch 115/198 | Loss: 2.5396947860717773\n","Epoch 15/150 | Batch 116/198 | Loss: 2.877227783203125\n","Epoch 15/150 | Batch 117/198 | Loss: 2.4701037406921387\n","Epoch 15/150 | Batch 118/198 | Loss: 2.6618025302886963\n","Epoch 15/150 | Batch 119/198 | Loss: 2.4061625003814697\n","Epoch 15/150 | Batch 120/198 | Loss: 2.916945695877075\n","Epoch 15/150 | Batch 121/198 | Loss: 2.863762855529785\n","Epoch 15/150 | Batch 122/198 | Loss: 2.8641486167907715\n","Epoch 15/150 | Batch 123/198 | Loss: 2.7720630168914795\n","Epoch 15/150 | Batch 124/198 | Loss: 2.534125566482544\n","Epoch 15/150 | Batch 125/198 | Loss: 2.8151934146881104\n","Epoch 15/150 | Batch 126/198 | Loss: 3.119656801223755\n","Epoch 15/150 | Batch 127/198 | Loss: 2.8420894145965576\n","Epoch 15/150 | Batch 128/198 | Loss: 3.1221892833709717\n","Epoch 15/150 | Batch 129/198 | Loss: 2.8507771492004395\n","Epoch 15/150 | Batch 130/198 | Loss: 2.9425771236419678\n","Epoch 15/150 | Batch 131/198 | Loss: 2.9059712886810303\n","Epoch 15/150 | Batch 132/198 | Loss: 2.6133832931518555\n","Epoch 15/150 | Batch 133/198 | Loss: 2.756608247756958\n","Epoch 15/150 | Batch 134/198 | Loss: 3.1001598834991455\n","Epoch 15/150 | Batch 135/198 | Loss: 3.088319778442383\n","Epoch 15/150 | Batch 136/198 | Loss: 2.942610502243042\n","Epoch 15/150 | Batch 137/198 | Loss: 3.037191867828369\n","Epoch 15/150 | Batch 138/198 | Loss: 2.4240574836730957\n","Epoch 15/150 | Batch 139/198 | Loss: 3.1386706829071045\n","Epoch 15/150 | Batch 140/198 | Loss: 2.681077003479004\n","Epoch 15/150 | Batch 141/198 | Loss: 2.5423624515533447\n","Epoch 15/150 | Batch 142/198 | Loss: 2.7636568546295166\n","Epoch 15/150 | Batch 143/198 | Loss: 2.7394230365753174\n","Epoch 15/150 | Batch 144/198 | Loss: 3.300067186355591\n","Epoch 15/150 | Batch 145/198 | Loss: 2.7583422660827637\n","Epoch 15/150 | Batch 146/198 | Loss: 2.844698429107666\n","Epoch 15/150 | Batch 147/198 | Loss: 2.9098172187805176\n","Epoch 15/150 | Batch 148/198 | Loss: 3.170166254043579\n","Epoch 15/150 | Batch 149/198 | Loss: 2.6155178546905518\n","Epoch 15/150 | Batch 150/198 | Loss: 3.11798357963562\n","Epoch 15/150 | Batch 151/198 | Loss: 2.521258592605591\n","Epoch 15/150 | Batch 152/198 | Loss: 3.1450867652893066\n","Epoch 15/150 | Batch 153/198 | Loss: 2.817631244659424\n","Epoch 15/150 | Batch 154/198 | Loss: 2.943140983581543\n","Epoch 15/150 | Batch 155/198 | Loss: 3.0224199295043945\n","Epoch 15/150 | Batch 156/198 | Loss: 2.926107406616211\n","Epoch 15/150 | Batch 157/198 | Loss: 2.9027364253997803\n","Epoch 15/150 | Batch 158/198 | Loss: 2.6843409538269043\n","Epoch 15/150 | Batch 159/198 | Loss: 3.148944616317749\n","Epoch 15/150 | Batch 160/198 | Loss: 2.6495983600616455\n","Epoch 15/150 | Batch 161/198 | Loss: 2.61365008354187\n","Epoch 15/150 | Batch 162/198 | Loss: 3.286810874938965\n","Epoch 15/150 | Batch 163/198 | Loss: 2.6457276344299316\n","Epoch 15/150 | Batch 164/198 | Loss: 3.157396078109741\n","Epoch 15/150 | Batch 165/198 | Loss: 3.1038763523101807\n","Epoch 15/150 | Batch 166/198 | Loss: 2.622199773788452\n","Epoch 15/150 | Batch 167/198 | Loss: 2.9053099155426025\n","Epoch 15/150 | Batch 168/198 | Loss: 2.988369941711426\n","Epoch 15/150 | Batch 169/198 | Loss: 2.8204550743103027\n","Epoch 15/150 | Batch 170/198 | Loss: 2.7780239582061768\n","Epoch 15/150 | Batch 171/198 | Loss: 2.8157925605773926\n","Epoch 15/150 | Batch 172/198 | Loss: 2.6686089038848877\n","Epoch 15/150 | Batch 173/198 | Loss: 2.6084442138671875\n","Epoch 15/150 | Batch 174/198 | Loss: 2.689358711242676\n","Epoch 15/150 | Batch 175/198 | Loss: 2.796597719192505\n","Epoch 15/150 | Batch 176/198 | Loss: 2.650001049041748\n","Epoch 15/150 | Batch 177/198 | Loss: 2.8696932792663574\n","Epoch 15/150 | Batch 178/198 | Loss: 2.8959786891937256\n","Epoch 15/150 | Batch 179/198 | Loss: 3.1244256496429443\n","Epoch 15/150 | Batch 180/198 | Loss: 3.185506820678711\n","Epoch 15/150 | Batch 181/198 | Loss: 2.8223283290863037\n","Epoch 15/150 | Batch 182/198 | Loss: 3.1265311241149902\n","Epoch 15/150 | Batch 183/198 | Loss: 2.948608636856079\n","Epoch 15/150 | Batch 184/198 | Loss: 3.1311001777648926\n","Epoch 15/150 | Batch 185/198 | Loss: 2.866102457046509\n","Epoch 15/150 | Batch 186/198 | Loss: 2.863297462463379\n","Epoch 15/150 | Batch 187/198 | Loss: 2.9597277641296387\n","Epoch 15/150 | Batch 188/198 | Loss: 2.5359749794006348\n","Epoch 15/150 | Batch 189/198 | Loss: 2.697770833969116\n","Epoch 15/150 | Batch 190/198 | Loss: 2.468017816543579\n","Epoch 15/150 | Batch 191/198 | Loss: 3.0610203742980957\n","Epoch 15/150 | Batch 192/198 | Loss: 3.0406858921051025\n","Epoch 15/150 | Batch 193/198 | Loss: 2.7779548168182373\n","Epoch 15/150 | Batch 194/198 | Loss: 2.9708378314971924\n","Epoch 15/150 | Batch 195/198 | Loss: 2.7259023189544678\n","Epoch 15/150 | Batch 196/198 | Loss: 2.538224458694458\n","Epoch 15/150 | Batch 197/198 | Loss: 3.169186592102051\n","Epoch 15/150 | Batch 198/198 | Loss: 2.6273281574249268\n","Epoch 15/150 | Average Loss: 2.8400879038704767\n","Epoch 16/150 | Batch 1/198 | Loss: 2.8796234130859375\n","Epoch 16/150 | Batch 2/198 | Loss: 2.8603479862213135\n","Epoch 16/150 | Batch 3/198 | Loss: 2.4459376335144043\n","Epoch 16/150 | Batch 4/198 | Loss: 2.6540629863739014\n","Epoch 16/150 | Batch 5/198 | Loss: 2.813415765762329\n","Epoch 16/150 | Batch 6/198 | Loss: 2.8591086864471436\n","Epoch 16/150 | Batch 7/198 | Loss: 2.6793994903564453\n","Epoch 16/150 | Batch 8/198 | Loss: 2.5355215072631836\n","Epoch 16/150 | Batch 9/198 | Loss: 2.553738832473755\n","Epoch 16/150 | Batch 10/198 | Loss: 2.4205687046051025\n","Epoch 16/150 | Batch 11/198 | Loss: 2.7522428035736084\n","Epoch 16/150 | Batch 12/198 | Loss: 2.580801010131836\n","Epoch 16/150 | Batch 13/198 | Loss: 2.6775903701782227\n","Epoch 16/150 | Batch 14/198 | Loss: 2.889462471008301\n","Epoch 16/150 | Batch 15/198 | Loss: 2.8420867919921875\n","Epoch 16/150 | Batch 16/198 | Loss: 3.011531352996826\n","Epoch 16/150 | Batch 17/198 | Loss: 2.7973086833953857\n","Epoch 16/150 | Batch 18/198 | Loss: 2.6667046546936035\n","Epoch 16/150 | Batch 19/198 | Loss: 2.364104747772217\n","Epoch 16/150 | Batch 20/198 | Loss: 2.740386486053467\n","Epoch 16/150 | Batch 21/198 | Loss: 2.760892868041992\n","Epoch 16/150 | Batch 22/198 | Loss: 2.461148738861084\n","Epoch 16/150 | Batch 23/198 | Loss: 2.7440640926361084\n","Epoch 16/150 | Batch 24/198 | Loss: 2.4664793014526367\n","Epoch 16/150 | Batch 25/198 | Loss: 2.672987699508667\n","Epoch 16/150 | Batch 26/198 | Loss: 2.775477886199951\n","Epoch 16/150 | Batch 27/198 | Loss: 2.7025930881500244\n","Epoch 16/150 | Batch 28/198 | Loss: 2.3307371139526367\n","Epoch 16/150 | Batch 29/198 | Loss: 2.3171868324279785\n","Epoch 16/150 | Batch 30/198 | Loss: 2.9488611221313477\n","Epoch 16/150 | Batch 31/198 | Loss: 2.88809871673584\n","Epoch 16/150 | Batch 32/198 | Loss: 2.573409080505371\n","Epoch 16/150 | Batch 33/198 | Loss: 2.498748779296875\n","Epoch 16/150 | Batch 34/198 | Loss: 2.6437268257141113\n","Epoch 16/150 | Batch 35/198 | Loss: 2.5486996173858643\n","Epoch 16/150 | Batch 36/198 | Loss: 2.4634695053100586\n","Epoch 16/150 | Batch 37/198 | Loss: 2.7380013465881348\n","Epoch 16/150 | Batch 38/198 | Loss: 2.571826696395874\n","Epoch 16/150 | Batch 39/198 | Loss: 2.4772090911865234\n","Epoch 16/150 | Batch 40/198 | Loss: 2.9155702590942383\n","Epoch 16/150 | Batch 41/198 | Loss: 2.4740941524505615\n","Epoch 16/150 | Batch 42/198 | Loss: 2.627183198928833\n","Epoch 16/150 | Batch 43/198 | Loss: 2.7308294773101807\n","Epoch 16/150 | Batch 44/198 | Loss: 2.582831859588623\n","Epoch 16/150 | Batch 45/198 | Loss: 2.965446710586548\n","Epoch 16/150 | Batch 46/198 | Loss: 2.5000905990600586\n","Epoch 16/150 | Batch 47/198 | Loss: 2.582792043685913\n","Epoch 16/150 | Batch 48/198 | Loss: 2.853971004486084\n","Epoch 16/150 | Batch 49/198 | Loss: 2.6497719287872314\n","Epoch 16/150 | Batch 50/198 | Loss: 2.356304168701172\n","Epoch 16/150 | Batch 51/198 | Loss: 2.3134851455688477\n","Epoch 16/150 | Batch 52/198 | Loss: 2.7071287631988525\n","Epoch 16/150 | Batch 53/198 | Loss: 2.4369328022003174\n","Epoch 16/150 | Batch 54/198 | Loss: 2.6991007328033447\n","Epoch 16/150 | Batch 55/198 | Loss: 2.8109209537506104\n","Epoch 16/150 | Batch 56/198 | Loss: 2.9353723526000977\n","Epoch 16/150 | Batch 57/198 | Loss: 2.5713350772857666\n","Epoch 16/150 | Batch 58/198 | Loss: 2.5634238719940186\n","Epoch 16/150 | Batch 59/198 | Loss: 2.678924798965454\n","Epoch 16/150 | Batch 60/198 | Loss: 2.852893590927124\n","Epoch 16/150 | Batch 61/198 | Loss: 2.9597058296203613\n","Epoch 16/150 | Batch 62/198 | Loss: 2.5784101486206055\n","Epoch 16/150 | Batch 63/198 | Loss: 2.8498990535736084\n","Epoch 16/150 | Batch 64/198 | Loss: 2.6957521438598633\n","Epoch 16/150 | Batch 65/198 | Loss: 2.8624091148376465\n","Epoch 16/150 | Batch 66/198 | Loss: 2.7858974933624268\n","Epoch 16/150 | Batch 67/198 | Loss: 2.675593376159668\n","Epoch 16/150 | Batch 68/198 | Loss: 2.5659523010253906\n","Epoch 16/150 | Batch 69/198 | Loss: 2.726722478866577\n","Epoch 16/150 | Batch 70/198 | Loss: 2.531912088394165\n","Epoch 16/150 | Batch 71/198 | Loss: 2.6371543407440186\n","Epoch 16/150 | Batch 72/198 | Loss: 2.6987733840942383\n","Epoch 16/150 | Batch 73/198 | Loss: 2.5753979682922363\n","Epoch 16/150 | Batch 74/198 | Loss: 2.736067056655884\n","Epoch 16/150 | Batch 75/198 | Loss: 2.7666561603546143\n","Epoch 16/150 | Batch 76/198 | Loss: 2.5631229877471924\n","Epoch 16/150 | Batch 77/198 | Loss: 2.668152332305908\n","Epoch 16/150 | Batch 78/198 | Loss: 2.4828755855560303\n","Epoch 16/150 | Batch 79/198 | Loss: 2.44547700881958\n","Epoch 16/150 | Batch 80/198 | Loss: 2.6274900436401367\n","Epoch 16/150 | Batch 81/198 | Loss: 2.486111640930176\n","Epoch 16/150 | Batch 82/198 | Loss: 2.817049741744995\n","Epoch 16/150 | Batch 83/198 | Loss: 2.597658395767212\n","Epoch 16/150 | Batch 84/198 | Loss: 2.695793867111206\n","Epoch 16/150 | Batch 85/198 | Loss: 2.908031940460205\n","Epoch 16/150 | Batch 86/198 | Loss: 2.462042808532715\n","Epoch 16/150 | Batch 87/198 | Loss: 2.559141159057617\n","Epoch 16/150 | Batch 88/198 | Loss: 2.718381881713867\n","Epoch 16/150 | Batch 89/198 | Loss: 2.8520445823669434\n","Epoch 16/150 | Batch 90/198 | Loss: 2.705646276473999\n","Epoch 16/150 | Batch 91/198 | Loss: 2.7238705158233643\n","Epoch 16/150 | Batch 92/198 | Loss: 2.3784351348876953\n","Epoch 16/150 | Batch 93/198 | Loss: 2.6929373741149902\n","Epoch 16/150 | Batch 94/198 | Loss: 2.884103775024414\n","Epoch 16/150 | Batch 95/198 | Loss: 2.635413408279419\n","Epoch 16/150 | Batch 96/198 | Loss: 2.8619651794433594\n","Epoch 16/150 | Batch 97/198 | Loss: 2.812580108642578\n","Epoch 16/150 | Batch 98/198 | Loss: 2.2572455406188965\n","Epoch 16/150 | Batch 99/198 | Loss: 2.445491313934326\n","Epoch 16/150 | Batch 100/198 | Loss: 2.420509099960327\n","Epoch 16/150 | Batch 101/198 | Loss: 3.006795883178711\n","Epoch 16/150 | Batch 102/198 | Loss: 2.4771623611450195\n","Epoch 16/150 | Batch 103/198 | Loss: 2.757097005844116\n","Epoch 16/150 | Batch 104/198 | Loss: 3.0293519496917725\n","Epoch 16/150 | Batch 105/198 | Loss: 2.8568248748779297\n","Epoch 16/150 | Batch 106/198 | Loss: 2.7541799545288086\n","Epoch 16/150 | Batch 107/198 | Loss: 2.551654815673828\n","Epoch 16/150 | Batch 108/198 | Loss: 2.652019739151001\n","Epoch 16/150 | Batch 109/198 | Loss: 2.9001855850219727\n","Epoch 16/150 | Batch 110/198 | Loss: 2.527306318283081\n","Epoch 16/150 | Batch 111/198 | Loss: 2.735023021697998\n","Epoch 16/150 | Batch 112/198 | Loss: 2.6363189220428467\n","Epoch 16/150 | Batch 113/198 | Loss: 2.6806528568267822\n","Epoch 16/150 | Batch 114/198 | Loss: 2.896937370300293\n","Epoch 16/150 | Batch 115/198 | Loss: 2.543729543685913\n","Epoch 16/150 | Batch 116/198 | Loss: 2.592517614364624\n","Epoch 16/150 | Batch 117/198 | Loss: 2.6451923847198486\n","Epoch 16/150 | Batch 118/198 | Loss: 2.6722333431243896\n","Epoch 16/150 | Batch 119/198 | Loss: 2.8863775730133057\n","Epoch 16/150 | Batch 120/198 | Loss: 2.7168872356414795\n","Epoch 16/150 | Batch 121/198 | Loss: 2.288271427154541\n","Epoch 16/150 | Batch 122/198 | Loss: 2.6877803802490234\n","Epoch 16/150 | Batch 123/198 | Loss: 2.8417489528656006\n","Epoch 16/150 | Batch 124/198 | Loss: 2.8183414936065674\n","Epoch 16/150 | Batch 125/198 | Loss: 2.985736846923828\n","Epoch 16/150 | Batch 126/198 | Loss: 2.830054998397827\n","Epoch 16/150 | Batch 127/198 | Loss: 2.621286630630493\n","Epoch 16/150 | Batch 128/198 | Loss: 2.7780141830444336\n","Epoch 16/150 | Batch 129/198 | Loss: 2.473971366882324\n","Epoch 16/150 | Batch 130/198 | Loss: 2.752121686935425\n","Epoch 16/150 | Batch 131/198 | Loss: 2.6936492919921875\n","Epoch 16/150 | Batch 132/198 | Loss: 2.788156270980835\n","Epoch 16/150 | Batch 133/198 | Loss: 2.9757649898529053\n","Epoch 16/150 | Batch 134/198 | Loss: 2.790318727493286\n","Epoch 16/150 | Batch 135/198 | Loss: 2.5692455768585205\n","Epoch 16/150 | Batch 136/198 | Loss: 2.3427515029907227\n","Epoch 16/150 | Batch 137/198 | Loss: 2.7494890689849854\n","Epoch 16/150 | Batch 138/198 | Loss: 2.848222494125366\n","Epoch 16/150 | Batch 139/198 | Loss: 2.6836225986480713\n","Epoch 16/150 | Batch 140/198 | Loss: 2.57393217086792\n","Epoch 16/150 | Batch 141/198 | Loss: 2.603121757507324\n","Epoch 16/150 | Batch 142/198 | Loss: 2.8452649116516113\n","Epoch 16/150 | Batch 143/198 | Loss: 2.767341136932373\n","Epoch 16/150 | Batch 144/198 | Loss: 2.700057029724121\n","Epoch 16/150 | Batch 145/198 | Loss: 2.872828960418701\n","Epoch 16/150 | Batch 146/198 | Loss: 3.1599509716033936\n","Epoch 16/150 | Batch 147/198 | Loss: 2.896817684173584\n","Epoch 16/150 | Batch 148/198 | Loss: 2.572176694869995\n","Epoch 16/150 | Batch 149/198 | Loss: 2.8198447227478027\n","Epoch 16/150 | Batch 150/198 | Loss: 2.7985472679138184\n","Epoch 16/150 | Batch 151/198 | Loss: 2.760612726211548\n","Epoch 16/150 | Batch 152/198 | Loss: 2.4106311798095703\n","Epoch 16/150 | Batch 153/198 | Loss: 3.1921191215515137\n","Epoch 16/150 | Batch 154/198 | Loss: 2.5689697265625\n","Epoch 16/150 | Batch 155/198 | Loss: 2.81693959236145\n","Epoch 16/150 | Batch 156/198 | Loss: 3.1548562049865723\n","Epoch 16/150 | Batch 157/198 | Loss: 2.609785318374634\n","Epoch 16/150 | Batch 158/198 | Loss: 2.752493143081665\n","Epoch 16/150 | Batch 159/198 | Loss: 2.745168685913086\n","Epoch 16/150 | Batch 160/198 | Loss: 2.5704965591430664\n","Epoch 16/150 | Batch 161/198 | Loss: 3.0537424087524414\n","Epoch 16/150 | Batch 162/198 | Loss: 2.505898952484131\n","Epoch 16/150 | Batch 163/198 | Loss: 2.7605550289154053\n","Epoch 16/150 | Batch 164/198 | Loss: 2.933347702026367\n","Epoch 16/150 | Batch 165/198 | Loss: 2.612476110458374\n","Epoch 16/150 | Batch 166/198 | Loss: 2.9525890350341797\n","Epoch 16/150 | Batch 167/198 | Loss: 3.0193045139312744\n","Epoch 16/150 | Batch 168/198 | Loss: 2.44311261177063\n","Epoch 16/150 | Batch 169/198 | Loss: 2.8352668285369873\n","Epoch 16/150 | Batch 170/198 | Loss: 2.6581335067749023\n","Epoch 16/150 | Batch 171/198 | Loss: 3.067748785018921\n","Epoch 16/150 | Batch 172/198 | Loss: 2.848551034927368\n","Epoch 16/150 | Batch 173/198 | Loss: 2.848374605178833\n","Epoch 16/150 | Batch 174/198 | Loss: 2.343197822570801\n","Epoch 16/150 | Batch 175/198 | Loss: 2.713306427001953\n","Epoch 16/150 | Batch 176/198 | Loss: 2.6768431663513184\n","Epoch 16/150 | Batch 177/198 | Loss: 2.9171016216278076\n","Epoch 16/150 | Batch 178/198 | Loss: 2.616983652114868\n","Epoch 16/150 | Batch 179/198 | Loss: 2.5791208744049072\n","Epoch 16/150 | Batch 180/198 | Loss: 2.890671730041504\n","Epoch 16/150 | Batch 181/198 | Loss: 2.5758776664733887\n","Epoch 16/150 | Batch 182/198 | Loss: 2.8256747722625732\n","Epoch 16/150 | Batch 183/198 | Loss: 2.661618947982788\n","Epoch 16/150 | Batch 184/198 | Loss: 2.915555477142334\n","Epoch 16/150 | Batch 185/198 | Loss: 2.7934460639953613\n","Epoch 16/150 | Batch 186/198 | Loss: 2.550604820251465\n","Epoch 16/150 | Batch 187/198 | Loss: 2.3831045627593994\n","Epoch 16/150 | Batch 188/198 | Loss: 2.647470235824585\n","Epoch 16/150 | Batch 189/198 | Loss: 2.7233822345733643\n","Epoch 16/150 | Batch 190/198 | Loss: 2.9728293418884277\n","Epoch 16/150 | Batch 191/198 | Loss: 2.6578619480133057\n","Epoch 16/150 | Batch 192/198 | Loss: 2.54561185836792\n","Epoch 16/150 | Batch 193/198 | Loss: 2.3008861541748047\n","Epoch 16/150 | Batch 194/198 | Loss: 2.3693594932556152\n","Epoch 16/150 | Batch 195/198 | Loss: 3.1642324924468994\n","Epoch 16/150 | Batch 196/198 | Loss: 2.784766435623169\n","Epoch 16/150 | Batch 197/198 | Loss: 3.111807107925415\n","Epoch 16/150 | Batch 198/198 | Loss: 2.674281120300293\n","Epoch 16/150 | Average Loss: 2.6966034872363314\n","Epoch 17/150 | Batch 1/198 | Loss: 2.2911505699157715\n","Epoch 17/150 | Batch 2/198 | Loss: 2.3705527782440186\n","Epoch 17/150 | Batch 3/198 | Loss: 2.52878475189209\n","Epoch 17/150 | Batch 4/198 | Loss: 2.4296813011169434\n","Epoch 17/150 | Batch 5/198 | Loss: 2.5250084400177\n","Epoch 17/150 | Batch 6/198 | Loss: 2.104807138442993\n","Epoch 17/150 | Batch 7/198 | Loss: 2.7487969398498535\n","Epoch 17/150 | Batch 8/198 | Loss: 2.291747570037842\n","Epoch 17/150 | Batch 9/198 | Loss: 2.376467704772949\n","Epoch 17/150 | Batch 10/198 | Loss: 2.6400721073150635\n","Epoch 17/150 | Batch 11/198 | Loss: 2.3803093433380127\n","Epoch 17/150 | Batch 12/198 | Loss: 2.611530303955078\n","Epoch 17/150 | Batch 13/198 | Loss: 1.9638941287994385\n","Epoch 17/150 | Batch 14/198 | Loss: 2.3339123725891113\n","Epoch 17/150 | Batch 15/198 | Loss: 2.1788411140441895\n","Epoch 17/150 | Batch 16/198 | Loss: 2.27781343460083\n","Epoch 17/150 | Batch 17/198 | Loss: 2.510159730911255\n","Epoch 17/150 | Batch 18/198 | Loss: 2.763270139694214\n","Epoch 17/150 | Batch 19/198 | Loss: 2.3990612030029297\n","Epoch 17/150 | Batch 20/198 | Loss: 2.324547529220581\n","Epoch 17/150 | Batch 21/198 | Loss: 2.6502935886383057\n","Epoch 17/150 | Batch 22/198 | Loss: 2.606637954711914\n","Epoch 17/150 | Batch 23/198 | Loss: 2.5244176387786865\n","Epoch 17/150 | Batch 24/198 | Loss: 2.703819751739502\n","Epoch 17/150 | Batch 25/198 | Loss: 2.371591091156006\n","Epoch 17/150 | Batch 26/198 | Loss: 2.264317035675049\n","Epoch 17/150 | Batch 27/198 | Loss: 2.574725866317749\n","Epoch 17/150 | Batch 28/198 | Loss: 2.2363553047180176\n","Epoch 17/150 | Batch 29/198 | Loss: 2.7732934951782227\n","Epoch 17/150 | Batch 30/198 | Loss: 2.757939338684082\n","Epoch 17/150 | Batch 31/198 | Loss: 2.5386435985565186\n","Epoch 17/150 | Batch 32/198 | Loss: 2.341808557510376\n","Epoch 17/150 | Batch 33/198 | Loss: 2.854607105255127\n","Epoch 17/150 | Batch 34/198 | Loss: 2.4137773513793945\n","Epoch 17/150 | Batch 35/198 | Loss: 2.709879159927368\n","Epoch 17/150 | Batch 36/198 | Loss: 2.512641191482544\n","Epoch 17/150 | Batch 37/198 | Loss: 2.6892919540405273\n","Epoch 17/150 | Batch 38/198 | Loss: 2.6508984565734863\n","Epoch 17/150 | Batch 39/198 | Loss: 2.8770360946655273\n","Epoch 17/150 | Batch 40/198 | Loss: 2.6799561977386475\n","Epoch 17/150 | Batch 41/198 | Loss: 2.7525997161865234\n","Epoch 17/150 | Batch 42/198 | Loss: 2.5289080142974854\n","Epoch 17/150 | Batch 43/198 | Loss: 2.868834972381592\n","Epoch 17/150 | Batch 44/198 | Loss: 2.5182039737701416\n","Epoch 17/150 | Batch 45/198 | Loss: 2.350623607635498\n","Epoch 17/150 | Batch 46/198 | Loss: 2.396627187728882\n","Epoch 17/150 | Batch 47/198 | Loss: 2.496680736541748\n","Epoch 17/150 | Batch 48/198 | Loss: 3.044076442718506\n","Epoch 17/150 | Batch 49/198 | Loss: 2.346572160720825\n","Epoch 17/150 | Batch 50/198 | Loss: 2.587639808654785\n","Epoch 17/150 | Batch 51/198 | Loss: 2.5782859325408936\n","Epoch 17/150 | Batch 52/198 | Loss: 2.3470706939697266\n","Epoch 17/150 | Batch 53/198 | Loss: 2.4235727787017822\n","Epoch 17/150 | Batch 54/198 | Loss: 2.580418586730957\n","Epoch 17/150 | Batch 55/198 | Loss: 2.3053150177001953\n","Epoch 17/150 | Batch 56/198 | Loss: 2.5642263889312744\n","Epoch 17/150 | Batch 57/198 | Loss: 3.020703077316284\n","Epoch 17/150 | Batch 58/198 | Loss: 2.9972009658813477\n","Epoch 17/150 | Batch 59/198 | Loss: 2.6864147186279297\n","Epoch 17/150 | Batch 60/198 | Loss: 2.156759262084961\n","Epoch 17/150 | Batch 61/198 | Loss: 2.6513254642486572\n","Epoch 17/150 | Batch 62/198 | Loss: 2.531869411468506\n","Epoch 17/150 | Batch 63/198 | Loss: 2.3275442123413086\n","Epoch 17/150 | Batch 64/198 | Loss: 2.7287445068359375\n","Epoch 17/150 | Batch 65/198 | Loss: 2.2324256896972656\n","Epoch 17/150 | Batch 66/198 | Loss: 2.6396844387054443\n","Epoch 17/150 | Batch 67/198 | Loss: 2.3469622135162354\n","Epoch 17/150 | Batch 68/198 | Loss: 2.7200024127960205\n","Epoch 17/150 | Batch 69/198 | Loss: 2.66910457611084\n","Epoch 17/150 | Batch 70/198 | Loss: 2.582455635070801\n","Epoch 17/150 | Batch 71/198 | Loss: 2.4191036224365234\n","Epoch 17/150 | Batch 72/198 | Loss: 2.400665044784546\n","Epoch 17/150 | Batch 73/198 | Loss: 2.7809813022613525\n","Epoch 17/150 | Batch 74/198 | Loss: 2.4595847129821777\n","Epoch 17/150 | Batch 75/198 | Loss: 2.4966251850128174\n","Epoch 17/150 | Batch 76/198 | Loss: 2.6894071102142334\n","Epoch 17/150 | Batch 77/198 | Loss: 2.470086097717285\n","Epoch 17/150 | Batch 78/198 | Loss: 2.3381004333496094\n","Epoch 17/150 | Batch 79/198 | Loss: 2.5139565467834473\n","Epoch 17/150 | Batch 80/198 | Loss: 2.4446074962615967\n","Epoch 17/150 | Batch 81/198 | Loss: 2.8312172889709473\n","Epoch 17/150 | Batch 82/198 | Loss: 2.470487594604492\n","Epoch 17/150 | Batch 83/198 | Loss: 2.6134114265441895\n","Epoch 17/150 | Batch 84/198 | Loss: 2.811134099960327\n","Epoch 17/150 | Batch 85/198 | Loss: 2.5767555236816406\n","Epoch 17/150 | Batch 86/198 | Loss: 2.8249142169952393\n","Epoch 17/150 | Batch 87/198 | Loss: 2.2850911617279053\n","Epoch 17/150 | Batch 88/198 | Loss: 2.272613763809204\n","Epoch 17/150 | Batch 89/198 | Loss: 2.676814079284668\n","Epoch 17/150 | Batch 90/198 | Loss: 2.322986125946045\n","Epoch 17/150 | Batch 91/198 | Loss: 2.442889928817749\n","Epoch 17/150 | Batch 92/198 | Loss: 2.69484806060791\n","Epoch 17/150 | Batch 93/198 | Loss: 2.813952684402466\n","Epoch 17/150 | Batch 94/198 | Loss: 2.635042190551758\n","Epoch 17/150 | Batch 95/198 | Loss: 2.87662672996521\n","Epoch 17/150 | Batch 96/198 | Loss: 2.822840452194214\n","Epoch 17/150 | Batch 97/198 | Loss: 2.4876914024353027\n","Epoch 17/150 | Batch 98/198 | Loss: 2.5656704902648926\n","Epoch 17/150 | Batch 99/198 | Loss: 2.864103317260742\n","Epoch 17/150 | Batch 100/198 | Loss: 2.463611364364624\n","Epoch 17/150 | Batch 101/198 | Loss: 2.7681546211242676\n","Epoch 17/150 | Batch 102/198 | Loss: 2.5214438438415527\n","Epoch 17/150 | Batch 103/198 | Loss: 2.6106362342834473\n","Epoch 17/150 | Batch 104/198 | Loss: 2.1152522563934326\n","Epoch 17/150 | Batch 105/198 | Loss: 2.537504196166992\n","Epoch 17/150 | Batch 106/198 | Loss: 2.4793241024017334\n","Epoch 17/150 | Batch 107/198 | Loss: 2.5323023796081543\n","Epoch 17/150 | Batch 108/198 | Loss: 2.4636080265045166\n","Epoch 17/150 | Batch 109/198 | Loss: 2.536318302154541\n","Epoch 17/150 | Batch 110/198 | Loss: 2.3737547397613525\n","Epoch 17/150 | Batch 111/198 | Loss: 2.752237558364868\n","Epoch 17/150 | Batch 112/198 | Loss: 2.5502917766571045\n","Epoch 17/150 | Batch 113/198 | Loss: 2.4247992038726807\n","Epoch 17/150 | Batch 114/198 | Loss: 2.489546775817871\n","Epoch 17/150 | Batch 115/198 | Loss: 2.8498830795288086\n","Epoch 17/150 | Batch 116/198 | Loss: 2.6884167194366455\n","Epoch 17/150 | Batch 117/198 | Loss: 2.9842710494995117\n","Epoch 17/150 | Batch 118/198 | Loss: 2.8445749282836914\n","Epoch 17/150 | Batch 119/198 | Loss: 2.5566794872283936\n","Epoch 17/150 | Batch 120/198 | Loss: 2.665472984313965\n","Epoch 17/150 | Batch 121/198 | Loss: 2.7577810287475586\n","Epoch 17/150 | Batch 122/198 | Loss: 2.5952742099761963\n","Epoch 17/150 | Batch 123/198 | Loss: 2.67362904548645\n","Epoch 17/150 | Batch 124/198 | Loss: 2.5000710487365723\n","Epoch 17/150 | Batch 125/198 | Loss: 2.758124351501465\n","Epoch 17/150 | Batch 126/198 | Loss: 2.483206033706665\n","Epoch 17/150 | Batch 127/198 | Loss: 2.6289896965026855\n","Epoch 17/150 | Batch 128/198 | Loss: 2.888092517852783\n","Epoch 17/150 | Batch 129/198 | Loss: 2.214681386947632\n","Epoch 17/150 | Batch 130/198 | Loss: 2.97293758392334\n","Epoch 17/150 | Batch 131/198 | Loss: 2.4942820072174072\n","Epoch 17/150 | Batch 132/198 | Loss: 2.720818519592285\n","Epoch 17/150 | Batch 133/198 | Loss: 2.4205851554870605\n","Epoch 17/150 | Batch 134/198 | Loss: 2.7602427005767822\n","Epoch 17/150 | Batch 135/198 | Loss: 2.771162748336792\n","Epoch 17/150 | Batch 136/198 | Loss: 2.4776365756988525\n","Epoch 17/150 | Batch 137/198 | Loss: 2.6292829513549805\n","Epoch 17/150 | Batch 138/198 | Loss: 2.3802692890167236\n","Epoch 17/150 | Batch 139/198 | Loss: 2.4920949935913086\n","Epoch 17/150 | Batch 140/198 | Loss: 2.7279720306396484\n","Epoch 17/150 | Batch 141/198 | Loss: 2.536349058151245\n","Epoch 17/150 | Batch 142/198 | Loss: 2.5471596717834473\n","Epoch 17/150 | Batch 143/198 | Loss: 2.6561758518218994\n","Epoch 17/150 | Batch 144/198 | Loss: 2.772477388381958\n","Epoch 17/150 | Batch 145/198 | Loss: 2.4189276695251465\n","Epoch 17/150 | Batch 146/198 | Loss: 2.439032554626465\n","Epoch 17/150 | Batch 147/198 | Loss: 2.442686080932617\n","Epoch 17/150 | Batch 148/198 | Loss: 2.5507652759552\n","Epoch 17/150 | Batch 149/198 | Loss: 2.330413579940796\n","Epoch 17/150 | Batch 150/198 | Loss: 2.4269399642944336\n","Epoch 17/150 | Batch 151/198 | Loss: 2.340965747833252\n","Epoch 17/150 | Batch 152/198 | Loss: 2.8201024532318115\n","Epoch 17/150 | Batch 153/198 | Loss: 2.99465274810791\n","Epoch 17/150 | Batch 154/198 | Loss: 2.6789071559906006\n","Epoch 17/150 | Batch 155/198 | Loss: 2.7174694538116455\n","Epoch 17/150 | Batch 156/198 | Loss: 2.3581905364990234\n","Epoch 17/150 | Batch 157/198 | Loss: 2.475446939468384\n","Epoch 17/150 | Batch 158/198 | Loss: 2.7750163078308105\n","Epoch 17/150 | Batch 159/198 | Loss: 2.3152120113372803\n","Epoch 17/150 | Batch 160/198 | Loss: 2.5132713317871094\n","Epoch 17/150 | Batch 161/198 | Loss: 2.8594884872436523\n","Epoch 17/150 | Batch 162/198 | Loss: 2.690113067626953\n","Epoch 17/150 | Batch 163/198 | Loss: 2.4670658111572266\n","Epoch 17/150 | Batch 164/198 | Loss: 2.096416711807251\n","Epoch 17/150 | Batch 165/198 | Loss: 2.6188409328460693\n","Epoch 17/150 | Batch 166/198 | Loss: 2.617814302444458\n","Epoch 17/150 | Batch 167/198 | Loss: 2.364161968231201\n","Epoch 17/150 | Batch 168/198 | Loss: 2.6269378662109375\n","Epoch 17/150 | Batch 169/198 | Loss: 2.6039440631866455\n","Epoch 17/150 | Batch 170/198 | Loss: 2.613767147064209\n","Epoch 17/150 | Batch 171/198 | Loss: 2.785961627960205\n","Epoch 17/150 | Batch 172/198 | Loss: 2.851301431655884\n","Epoch 17/150 | Batch 173/198 | Loss: 2.5184450149536133\n","Epoch 17/150 | Batch 174/198 | Loss: 2.7202389240264893\n","Epoch 17/150 | Batch 175/198 | Loss: 2.614767074584961\n","Epoch 17/150 | Batch 176/198 | Loss: 2.5961315631866455\n","Epoch 17/150 | Batch 177/198 | Loss: 2.7974390983581543\n","Epoch 17/150 | Batch 178/198 | Loss: 3.031095266342163\n","Epoch 17/150 | Batch 179/198 | Loss: 2.4130876064300537\n","Epoch 17/150 | Batch 180/198 | Loss: 2.6690421104431152\n","Epoch 17/150 | Batch 181/198 | Loss: 2.2675485610961914\n","Epoch 17/150 | Batch 182/198 | Loss: 2.3228108882904053\n","Epoch 17/150 | Batch 183/198 | Loss: 2.520084857940674\n","Epoch 17/150 | Batch 184/198 | Loss: 2.751465320587158\n","Epoch 17/150 | Batch 185/198 | Loss: 2.766390085220337\n","Epoch 17/150 | Batch 186/198 | Loss: 2.294783592224121\n","Epoch 17/150 | Batch 187/198 | Loss: 2.4859323501586914\n","Epoch 17/150 | Batch 188/198 | Loss: 2.508439302444458\n","Epoch 17/150 | Batch 189/198 | Loss: 2.6718363761901855\n","Epoch 17/150 | Batch 190/198 | Loss: 2.4398326873779297\n","Epoch 17/150 | Batch 191/198 | Loss: 2.824918270111084\n","Epoch 17/150 | Batch 192/198 | Loss: 2.719071388244629\n","Epoch 17/150 | Batch 193/198 | Loss: 2.6699817180633545\n","Epoch 17/150 | Batch 194/198 | Loss: 2.4973745346069336\n","Epoch 17/150 | Batch 195/198 | Loss: 2.847999095916748\n","Epoch 17/150 | Batch 196/198 | Loss: 2.4582765102386475\n","Epoch 17/150 | Batch 197/198 | Loss: 2.5216023921966553\n","Epoch 17/150 | Batch 198/198 | Loss: 2.298515558242798\n","Epoch 17/150 | Average Loss: 2.564412640802788\n","Epoch 18/150 | Batch 1/198 | Loss: 2.561307907104492\n","Epoch 18/150 | Batch 2/198 | Loss: 2.8269476890563965\n","Epoch 18/150 | Batch 3/198 | Loss: 2.348511219024658\n","Epoch 18/150 | Batch 4/198 | Loss: 2.1656992435455322\n","Epoch 18/150 | Batch 5/198 | Loss: 2.3920085430145264\n","Epoch 18/150 | Batch 6/198 | Loss: 2.463634490966797\n","Epoch 18/150 | Batch 7/198 | Loss: 2.4607582092285156\n","Epoch 18/150 | Batch 8/198 | Loss: 2.3559749126434326\n","Epoch 18/150 | Batch 9/198 | Loss: 2.3299617767333984\n","Epoch 18/150 | Batch 10/198 | Loss: 2.2942821979522705\n","Epoch 18/150 | Batch 11/198 | Loss: 2.796074151992798\n","Epoch 18/150 | Batch 12/198 | Loss: 2.494418144226074\n","Epoch 18/150 | Batch 13/198 | Loss: 2.5502092838287354\n","Epoch 18/150 | Batch 14/198 | Loss: 2.2490599155426025\n","Epoch 18/150 | Batch 15/198 | Loss: 2.1232669353485107\n","Epoch 18/150 | Batch 16/198 | Loss: 2.667499303817749\n","Epoch 18/150 | Batch 17/198 | Loss: 2.262157678604126\n","Epoch 18/150 | Batch 18/198 | Loss: 2.2221410274505615\n","Epoch 18/150 | Batch 19/198 | Loss: 2.2708656787872314\n","Epoch 18/150 | Batch 20/198 | Loss: 2.413361072540283\n","Epoch 18/150 | Batch 21/198 | Loss: 2.5338127613067627\n","Epoch 18/150 | Batch 22/198 | Loss: 2.267683267593384\n","Epoch 18/150 | Batch 23/198 | Loss: 2.6633694171905518\n","Epoch 18/150 | Batch 24/198 | Loss: 2.598672866821289\n","Epoch 18/150 | Batch 25/198 | Loss: 2.302367687225342\n","Epoch 18/150 | Batch 26/198 | Loss: 2.4877712726593018\n","Epoch 18/150 | Batch 27/198 | Loss: 2.483985185623169\n","Epoch 18/150 | Batch 28/198 | Loss: 2.384488105773926\n","Epoch 18/150 | Batch 29/198 | Loss: 2.383554697036743\n","Epoch 18/150 | Batch 30/198 | Loss: 2.164787769317627\n","Epoch 18/150 | Batch 31/198 | Loss: 2.5791256427764893\n","Epoch 18/150 | Batch 32/198 | Loss: 2.4567205905914307\n","Epoch 18/150 | Batch 33/198 | Loss: 2.5004830360412598\n","Epoch 18/150 | Batch 34/198 | Loss: 2.332819938659668\n","Epoch 18/150 | Batch 35/198 | Loss: 2.329552412033081\n","Epoch 18/150 | Batch 36/198 | Loss: 2.898138999938965\n","Epoch 18/150 | Batch 37/198 | Loss: 2.324570655822754\n","Epoch 18/150 | Batch 38/198 | Loss: 2.6361238956451416\n","Epoch 18/150 | Batch 39/198 | Loss: 2.370640277862549\n","Epoch 18/150 | Batch 40/198 | Loss: 2.617102861404419\n","Epoch 18/150 | Batch 41/198 | Loss: 2.479454755783081\n","Epoch 18/150 | Batch 42/198 | Loss: 2.4283862113952637\n","Epoch 18/150 | Batch 43/198 | Loss: 2.7524993419647217\n","Epoch 18/150 | Batch 44/198 | Loss: 2.150712251663208\n","Epoch 18/150 | Batch 45/198 | Loss: 2.610753297805786\n","Epoch 18/150 | Batch 46/198 | Loss: 2.6601457595825195\n","Epoch 18/150 | Batch 47/198 | Loss: 2.393343925476074\n","Epoch 18/150 | Batch 48/198 | Loss: 2.6892645359039307\n","Epoch 18/150 | Batch 49/198 | Loss: 2.3183093070983887\n","Epoch 18/150 | Batch 50/198 | Loss: 2.6617610454559326\n","Epoch 18/150 | Batch 51/198 | Loss: 2.254711627960205\n","Epoch 18/150 | Batch 52/198 | Loss: 2.659829616546631\n","Epoch 18/150 | Batch 53/198 | Loss: 2.310697555541992\n","Epoch 18/150 | Batch 54/198 | Loss: 2.6518208980560303\n","Epoch 18/150 | Batch 55/198 | Loss: 2.397573709487915\n","Epoch 18/150 | Batch 56/198 | Loss: 2.0539751052856445\n","Epoch 18/150 | Batch 57/198 | Loss: 2.5649209022521973\n","Epoch 18/150 | Batch 58/198 | Loss: 2.4684994220733643\n","Epoch 18/150 | Batch 59/198 | Loss: 2.5352942943573\n","Epoch 18/150 | Batch 60/198 | Loss: 2.176117181777954\n","Epoch 18/150 | Batch 61/198 | Loss: 2.5595808029174805\n","Epoch 18/150 | Batch 62/198 | Loss: 2.116642475128174\n","Epoch 18/150 | Batch 63/198 | Loss: 2.4244296550750732\n","Epoch 18/150 | Batch 64/198 | Loss: 2.477501630783081\n","Epoch 18/150 | Batch 65/198 | Loss: 2.371129274368286\n","Epoch 18/150 | Batch 66/198 | Loss: 2.3285865783691406\n","Epoch 18/150 | Batch 67/198 | Loss: 2.662196636199951\n","Epoch 18/150 | Batch 68/198 | Loss: 2.8308863639831543\n","Epoch 18/150 | Batch 69/198 | Loss: 2.5534212589263916\n","Epoch 18/150 | Batch 70/198 | Loss: 2.15549373626709\n","Epoch 18/150 | Batch 71/198 | Loss: 2.178009271621704\n","Epoch 18/150 | Batch 72/198 | Loss: 2.3126583099365234\n","Epoch 18/150 | Batch 73/198 | Loss: 2.544297456741333\n","Epoch 18/150 | Batch 74/198 | Loss: 2.339393377304077\n","Epoch 18/150 | Batch 75/198 | Loss: 2.2702393531799316\n","Epoch 18/150 | Batch 76/198 | Loss: 2.389719247817993\n","Epoch 18/150 | Batch 77/198 | Loss: 2.325103759765625\n","Epoch 18/150 | Batch 78/198 | Loss: 2.5036346912384033\n","Epoch 18/150 | Batch 79/198 | Loss: 2.4200477600097656\n","Epoch 18/150 | Batch 80/198 | Loss: 2.6539485454559326\n","Epoch 18/150 | Batch 81/198 | Loss: 2.400425434112549\n","Epoch 18/150 | Batch 82/198 | Loss: 2.6909055709838867\n","Epoch 18/150 | Batch 83/198 | Loss: 2.266047954559326\n","Epoch 18/150 | Batch 84/198 | Loss: 2.2470099925994873\n","Epoch 18/150 | Batch 85/198 | Loss: 2.53912091255188\n","Epoch 18/150 | Batch 86/198 | Loss: 2.4629228115081787\n","Epoch 18/150 | Batch 87/198 | Loss: 2.470822811126709\n","Epoch 18/150 | Batch 88/198 | Loss: 2.591693878173828\n","Epoch 18/150 | Batch 89/198 | Loss: 2.4031641483306885\n","Epoch 18/150 | Batch 90/198 | Loss: 2.4919402599334717\n","Epoch 18/150 | Batch 91/198 | Loss: 2.3732733726501465\n","Epoch 18/150 | Batch 92/198 | Loss: 2.573829412460327\n","Epoch 18/150 | Batch 93/198 | Loss: 2.1827075481414795\n","Epoch 18/150 | Batch 94/198 | Loss: 2.6080944538116455\n","Epoch 18/150 | Batch 95/198 | Loss: 2.694035530090332\n","Epoch 18/150 | Batch 96/198 | Loss: 2.350985050201416\n","Epoch 18/150 | Batch 97/198 | Loss: 2.463495969772339\n","Epoch 18/150 | Batch 98/198 | Loss: 2.261016368865967\n","Epoch 18/150 | Batch 99/198 | Loss: 2.600584030151367\n","Epoch 18/150 | Batch 100/198 | Loss: 2.4693331718444824\n","Epoch 18/150 | Batch 101/198 | Loss: 2.3398780822753906\n","Epoch 18/150 | Batch 102/198 | Loss: 2.4132778644561768\n","Epoch 18/150 | Batch 103/198 | Loss: 2.473146438598633\n","Epoch 18/150 | Batch 104/198 | Loss: 2.4248709678649902\n","Epoch 18/150 | Batch 105/198 | Loss: 2.6921849250793457\n","Epoch 18/150 | Batch 106/198 | Loss: 2.5056850910186768\n","Epoch 18/150 | Batch 107/198 | Loss: 2.221505641937256\n","Epoch 18/150 | Batch 108/198 | Loss: 2.3465659618377686\n","Epoch 18/150 | Batch 109/198 | Loss: 2.431939125061035\n","Epoch 18/150 | Batch 110/198 | Loss: 2.7570812702178955\n","Epoch 18/150 | Batch 111/198 | Loss: 2.5187830924987793\n","Epoch 18/150 | Batch 112/198 | Loss: 2.169372081756592\n","Epoch 18/150 | Batch 113/198 | Loss: 2.47151517868042\n","Epoch 18/150 | Batch 114/198 | Loss: 2.2580926418304443\n","Epoch 18/150 | Batch 115/198 | Loss: 2.4355220794677734\n","Epoch 18/150 | Batch 116/198 | Loss: 2.7345380783081055\n","Epoch 18/150 | Batch 117/198 | Loss: 2.3365962505340576\n","Epoch 18/150 | Batch 118/198 | Loss: 2.5120913982391357\n","Epoch 18/150 | Batch 119/198 | Loss: 2.271665334701538\n","Epoch 18/150 | Batch 120/198 | Loss: 2.3510725498199463\n","Epoch 18/150 | Batch 121/198 | Loss: 2.6038663387298584\n","Epoch 18/150 | Batch 122/198 | Loss: 2.457094430923462\n","Epoch 18/150 | Batch 123/198 | Loss: 2.6327130794525146\n","Epoch 18/150 | Batch 124/198 | Loss: 2.5423409938812256\n","Epoch 18/150 | Batch 125/198 | Loss: 2.6376793384552\n","Epoch 18/150 | Batch 126/198 | Loss: 2.6330907344818115\n","Epoch 18/150 | Batch 127/198 | Loss: 2.326950788497925\n","Epoch 18/150 | Batch 128/198 | Loss: 2.2322375774383545\n","Epoch 18/150 | Batch 129/198 | Loss: 2.3477234840393066\n","Epoch 18/150 | Batch 130/198 | Loss: 2.674245834350586\n","Epoch 18/150 | Batch 131/198 | Loss: 2.859382390975952\n","Epoch 18/150 | Batch 132/198 | Loss: 2.7162024974823\n","Epoch 18/150 | Batch 133/198 | Loss: 2.5475096702575684\n","Epoch 18/150 | Batch 134/198 | Loss: 2.2497589588165283\n","Epoch 18/150 | Batch 135/198 | Loss: 2.250061511993408\n","Epoch 18/150 | Batch 136/198 | Loss: 2.176888942718506\n","Epoch 18/150 | Batch 137/198 | Loss: 2.4195563793182373\n","Epoch 18/150 | Batch 138/198 | Loss: 2.5165929794311523\n","Epoch 18/150 | Batch 139/198 | Loss: 2.165163040161133\n","Epoch 18/150 | Batch 140/198 | Loss: 2.5853400230407715\n","Epoch 18/150 | Batch 141/198 | Loss: 2.3888823986053467\n","Epoch 18/150 | Batch 142/198 | Loss: 2.6430091857910156\n","Epoch 18/150 | Batch 143/198 | Loss: 2.623722553253174\n","Epoch 18/150 | Batch 144/198 | Loss: 2.482665777206421\n","Epoch 18/150 | Batch 145/198 | Loss: 2.6130964756011963\n","Epoch 18/150 | Batch 146/198 | Loss: 2.343675374984741\n","Epoch 18/150 | Batch 147/198 | Loss: 2.3735389709472656\n","Epoch 18/150 | Batch 148/198 | Loss: 2.247509717941284\n","Epoch 18/150 | Batch 149/198 | Loss: 2.693819999694824\n","Epoch 18/150 | Batch 150/198 | Loss: 2.5205399990081787\n","Epoch 18/150 | Batch 151/198 | Loss: 2.2215981483459473\n","Epoch 18/150 | Batch 152/198 | Loss: 2.328110933303833\n","Epoch 18/150 | Batch 153/198 | Loss: 2.2823967933654785\n","Epoch 18/150 | Batch 154/198 | Loss: 2.5180318355560303\n","Epoch 18/150 | Batch 155/198 | Loss: 2.295865058898926\n","Epoch 18/150 | Batch 156/198 | Loss: 2.808924674987793\n","Epoch 18/150 | Batch 157/198 | Loss: 2.2480974197387695\n","Epoch 18/150 | Batch 158/198 | Loss: 2.5721771717071533\n","Epoch 18/150 | Batch 159/198 | Loss: 2.4980249404907227\n","Epoch 18/150 | Batch 160/198 | Loss: 2.405346155166626\n","Epoch 18/150 | Batch 161/198 | Loss: 2.2028346061706543\n","Epoch 18/150 | Batch 162/198 | Loss: 2.6588234901428223\n","Epoch 18/150 | Batch 163/198 | Loss: 2.323284149169922\n","Epoch 18/150 | Batch 164/198 | Loss: 2.6511471271514893\n","Epoch 18/150 | Batch 165/198 | Loss: 2.3962457180023193\n","Epoch 18/150 | Batch 166/198 | Loss: 2.908339262008667\n","Epoch 18/150 | Batch 167/198 | Loss: 2.7665016651153564\n","Epoch 18/150 | Batch 168/198 | Loss: 2.3264544010162354\n","Epoch 18/150 | Batch 169/198 | Loss: 2.43186616897583\n","Epoch 18/150 | Batch 170/198 | Loss: 2.0276005268096924\n","Epoch 18/150 | Batch 171/198 | Loss: 2.920560121536255\n","Epoch 18/150 | Batch 172/198 | Loss: 2.380840539932251\n","Epoch 18/150 | Batch 173/198 | Loss: 2.425342559814453\n","Epoch 18/150 | Batch 174/198 | Loss: 2.2113656997680664\n","Epoch 18/150 | Batch 175/198 | Loss: 2.6868343353271484\n","Epoch 18/150 | Batch 176/198 | Loss: 2.4109721183776855\n","Epoch 18/150 | Batch 177/198 | Loss: 2.586730480194092\n","Epoch 18/150 | Batch 178/198 | Loss: 2.257620096206665\n","Epoch 18/150 | Batch 179/198 | Loss: 2.1951282024383545\n","Epoch 18/150 | Batch 180/198 | Loss: 2.213090181350708\n","Epoch 18/150 | Batch 181/198 | Loss: 2.2374324798583984\n","Epoch 18/150 | Batch 182/198 | Loss: 2.6679036617279053\n","Epoch 18/150 | Batch 183/198 | Loss: 2.401235342025757\n","Epoch 18/150 | Batch 184/198 | Loss: 2.2354414463043213\n","Epoch 18/150 | Batch 185/198 | Loss: 2.37729811668396\n","Epoch 18/150 | Batch 186/198 | Loss: 2.4584662914276123\n","Epoch 18/150 | Batch 187/198 | Loss: 2.739780902862549\n","Epoch 18/150 | Batch 188/198 | Loss: 2.434589385986328\n","Epoch 18/150 | Batch 189/198 | Loss: 2.2700843811035156\n","Epoch 18/150 | Batch 190/198 | Loss: 2.3731820583343506\n","Epoch 18/150 | Batch 191/198 | Loss: 2.397430896759033\n","Epoch 18/150 | Batch 192/198 | Loss: 2.1940555572509766\n","Epoch 18/150 | Batch 193/198 | Loss: 2.483670949935913\n","Epoch 18/150 | Batch 194/198 | Loss: 2.5437064170837402\n","Epoch 18/150 | Batch 195/198 | Loss: 2.35313081741333\n","Epoch 18/150 | Batch 196/198 | Loss: 2.744852066040039\n","Epoch 18/150 | Batch 197/198 | Loss: 2.4119367599487305\n","Epoch 18/150 | Batch 198/198 | Loss: 2.3113996982574463\n","Epoch 18/150 | Average Loss: 2.445991442661093\n","Epoch 19/150 | Batch 1/198 | Loss: 2.059002637863159\n","Epoch 19/150 | Batch 2/198 | Loss: 2.462048292160034\n","Epoch 19/150 | Batch 3/198 | Loss: 1.9660208225250244\n","Epoch 19/150 | Batch 4/198 | Loss: 2.3970425128936768\n","Epoch 19/150 | Batch 5/198 | Loss: 2.3973710536956787\n","Epoch 19/150 | Batch 6/198 | Loss: 2.16336727142334\n","Epoch 19/150 | Batch 7/198 | Loss: 2.066132068634033\n","Epoch 19/150 | Batch 8/198 | Loss: 2.2570087909698486\n","Epoch 19/150 | Batch 9/198 | Loss: 2.354069232940674\n","Epoch 19/150 | Batch 10/198 | Loss: 2.2450952529907227\n","Epoch 19/150 | Batch 11/198 | Loss: 1.9591370820999146\n","Epoch 19/150 | Batch 12/198 | Loss: 2.626288414001465\n","Epoch 19/150 | Batch 13/198 | Loss: 2.5926718711853027\n","Epoch 19/150 | Batch 14/198 | Loss: 2.4638612270355225\n","Epoch 19/150 | Batch 15/198 | Loss: 2.2310123443603516\n","Epoch 19/150 | Batch 16/198 | Loss: 2.459730863571167\n","Epoch 19/150 | Batch 17/198 | Loss: 2.397705554962158\n","Epoch 19/150 | Batch 18/198 | Loss: 2.5386223793029785\n","Epoch 19/150 | Batch 19/198 | Loss: 2.454025983810425\n","Epoch 19/150 | Batch 20/198 | Loss: 2.3605995178222656\n","Epoch 19/150 | Batch 21/198 | Loss: 2.2338438034057617\n","Epoch 19/150 | Batch 22/198 | Loss: 1.7788223028182983\n","Epoch 19/150 | Batch 23/198 | Loss: 2.47640323638916\n","Epoch 19/150 | Batch 24/198 | Loss: 2.477778911590576\n","Epoch 19/150 | Batch 25/198 | Loss: 2.4357974529266357\n","Epoch 19/150 | Batch 26/198 | Loss: 2.3008415699005127\n","Epoch 19/150 | Batch 27/198 | Loss: 2.488210439682007\n","Epoch 19/150 | Batch 28/198 | Loss: 2.1355247497558594\n","Epoch 19/150 | Batch 29/198 | Loss: 2.3081917762756348\n","Epoch 19/150 | Batch 30/198 | Loss: 2.1426563262939453\n","Epoch 19/150 | Batch 31/198 | Loss: 2.4533886909484863\n","Epoch 19/150 | Batch 32/198 | Loss: 2.3988916873931885\n","Epoch 19/150 | Batch 33/198 | Loss: 2.578883647918701\n","Epoch 19/150 | Batch 34/198 | Loss: 2.2827329635620117\n","Epoch 19/150 | Batch 35/198 | Loss: 2.491865396499634\n","Epoch 19/150 | Batch 36/198 | Loss: 2.4684195518493652\n","Epoch 19/150 | Batch 37/198 | Loss: 2.203453540802002\n","Epoch 19/150 | Batch 38/198 | Loss: 2.3852171897888184\n","Epoch 19/150 | Batch 39/198 | Loss: 2.582819700241089\n","Epoch 19/150 | Batch 40/198 | Loss: 2.311549186706543\n","Epoch 19/150 | Batch 41/198 | Loss: 2.6938107013702393\n","Epoch 19/150 | Batch 42/198 | Loss: 2.239914655685425\n","Epoch 19/150 | Batch 43/198 | Loss: 2.4102699756622314\n","Epoch 19/150 | Batch 44/198 | Loss: 2.2970573902130127\n","Epoch 19/150 | Batch 45/198 | Loss: 2.4475464820861816\n","Epoch 19/150 | Batch 46/198 | Loss: 2.2541143894195557\n","Epoch 19/150 | Batch 47/198 | Loss: 2.1983044147491455\n","Epoch 19/150 | Batch 48/198 | Loss: 2.222529172897339\n","Epoch 19/150 | Batch 49/198 | Loss: 2.413769245147705\n","Epoch 19/150 | Batch 50/198 | Loss: 2.51822829246521\n","Epoch 19/150 | Batch 51/198 | Loss: 2.3869097232818604\n","Epoch 19/150 | Batch 52/198 | Loss: 2.32279109954834\n","Epoch 19/150 | Batch 53/198 | Loss: 2.3746724128723145\n","Epoch 19/150 | Batch 54/198 | Loss: 2.463303327560425\n","Epoch 19/150 | Batch 55/198 | Loss: 2.486539602279663\n","Epoch 19/150 | Batch 56/198 | Loss: 2.330070972442627\n","Epoch 19/150 | Batch 57/198 | Loss: 2.454728364944458\n","Epoch 19/150 | Batch 58/198 | Loss: 2.5225698947906494\n","Epoch 19/150 | Batch 59/198 | Loss: 2.05865740776062\n","Epoch 19/150 | Batch 60/198 | Loss: 2.4386777877807617\n","Epoch 19/150 | Batch 61/198 | Loss: 2.0036520957946777\n","Epoch 19/150 | Batch 62/198 | Loss: 1.8759338855743408\n","Epoch 19/150 | Batch 63/198 | Loss: 2.264387845993042\n","Epoch 19/150 | Batch 64/198 | Loss: 2.4781131744384766\n","Epoch 19/150 | Batch 65/198 | Loss: 2.437096118927002\n","Epoch 19/150 | Batch 66/198 | Loss: 2.274454116821289\n","Epoch 19/150 | Batch 67/198 | Loss: 2.3876426219940186\n","Epoch 19/150 | Batch 68/198 | Loss: 2.152316093444824\n","Epoch 19/150 | Batch 69/198 | Loss: 2.3164968490600586\n","Epoch 19/150 | Batch 70/198 | Loss: 2.113985538482666\n","Epoch 19/150 | Batch 71/198 | Loss: 2.514282703399658\n","Epoch 19/150 | Batch 72/198 | Loss: 2.197754144668579\n","Epoch 19/150 | Batch 73/198 | Loss: 2.163928270339966\n","Epoch 19/150 | Batch 74/198 | Loss: 2.199597120285034\n","Epoch 19/150 | Batch 75/198 | Loss: 2.169246196746826\n","Epoch 19/150 | Batch 76/198 | Loss: 2.694490909576416\n","Epoch 19/150 | Batch 77/198 | Loss: 2.344303607940674\n","Epoch 19/150 | Batch 78/198 | Loss: 2.3739044666290283\n","Epoch 19/150 | Batch 79/198 | Loss: 2.368293523788452\n","Epoch 19/150 | Batch 80/198 | Loss: 2.705364227294922\n","Epoch 19/150 | Batch 81/198 | Loss: 2.5112709999084473\n","Epoch 19/150 | Batch 82/198 | Loss: 2.6828367710113525\n","Epoch 19/150 | Batch 83/198 | Loss: 2.5191516876220703\n","Epoch 19/150 | Batch 84/198 | Loss: 2.330794095993042\n","Epoch 19/150 | Batch 85/198 | Loss: 2.3015637397766113\n","Epoch 19/150 | Batch 86/198 | Loss: 2.4062817096710205\n","Epoch 19/150 | Batch 87/198 | Loss: 2.1851048469543457\n","Epoch 19/150 | Batch 88/198 | Loss: 2.222296953201294\n","Epoch 19/150 | Batch 89/198 | Loss: 2.371650218963623\n","Epoch 19/150 | Batch 90/198 | Loss: 2.4761385917663574\n","Epoch 19/150 | Batch 91/198 | Loss: 2.479534387588501\n","Epoch 19/150 | Batch 92/198 | Loss: 2.3830158710479736\n","Epoch 19/150 | Batch 93/198 | Loss: 2.1177241802215576\n","Epoch 19/150 | Batch 94/198 | Loss: 1.7818297147750854\n","Epoch 19/150 | Batch 95/198 | Loss: 2.521430015563965\n","Epoch 19/150 | Batch 96/198 | Loss: 2.3836569786071777\n","Epoch 19/150 | Batch 97/198 | Loss: 2.469947576522827\n","Epoch 19/150 | Batch 98/198 | Loss: 2.217381715774536\n","Epoch 19/150 | Batch 99/198 | Loss: 2.3352060317993164\n","Epoch 19/150 | Batch 100/198 | Loss: 2.339597225189209\n","Epoch 19/150 | Batch 101/198 | Loss: 2.7511677742004395\n","Epoch 19/150 | Batch 102/198 | Loss: 2.3886969089508057\n","Epoch 19/150 | Batch 103/198 | Loss: 2.2934186458587646\n","Epoch 19/150 | Batch 104/198 | Loss: 2.213439464569092\n","Epoch 19/150 | Batch 105/198 | Loss: 2.5187947750091553\n","Epoch 19/150 | Batch 106/198 | Loss: 2.0108020305633545\n","Epoch 19/150 | Batch 107/198 | Loss: 2.007941246032715\n","Epoch 19/150 | Batch 108/198 | Loss: 2.452997922897339\n","Epoch 19/150 | Batch 109/198 | Loss: 2.4080891609191895\n","Epoch 19/150 | Batch 110/198 | Loss: 2.142106056213379\n","Epoch 19/150 | Batch 111/198 | Loss: 2.476854085922241\n","Epoch 19/150 | Batch 112/198 | Loss: 2.218899726867676\n","Epoch 19/150 | Batch 113/198 | Loss: 2.134056806564331\n","Epoch 19/150 | Batch 114/198 | Loss: 2.286811351776123\n","Epoch 19/150 | Batch 115/198 | Loss: 2.3598358631134033\n","Epoch 19/150 | Batch 116/198 | Loss: 1.9520809650421143\n","Epoch 19/150 | Batch 117/198 | Loss: 2.2005105018615723\n","Epoch 19/150 | Batch 118/198 | Loss: 2.3406331539154053\n","Epoch 19/150 | Batch 119/198 | Loss: 2.7060678005218506\n","Epoch 19/150 | Batch 120/198 | Loss: 2.486665964126587\n","Epoch 19/150 | Batch 121/198 | Loss: 2.2867801189422607\n","Epoch 19/150 | Batch 122/198 | Loss: 2.4254186153411865\n","Epoch 19/150 | Batch 123/198 | Loss: 2.441946506500244\n","Epoch 19/150 | Batch 124/198 | Loss: 2.5057735443115234\n","Epoch 19/150 | Batch 125/198 | Loss: 2.682673215866089\n","Epoch 19/150 | Batch 126/198 | Loss: 2.4952070713043213\n","Epoch 19/150 | Batch 127/198 | Loss: 2.3407795429229736\n","Epoch 19/150 | Batch 128/198 | Loss: 2.272433042526245\n","Epoch 19/150 | Batch 129/198 | Loss: 2.0154569149017334\n","Epoch 19/150 | Batch 130/198 | Loss: 2.067204713821411\n","Epoch 19/150 | Batch 131/198 | Loss: 2.6214194297790527\n","Epoch 19/150 | Batch 132/198 | Loss: 2.635647773742676\n","Epoch 19/150 | Batch 133/198 | Loss: 2.3304953575134277\n","Epoch 19/150 | Batch 134/198 | Loss: 2.502812623977661\n","Epoch 19/150 | Batch 135/198 | Loss: 2.4119489192962646\n","Epoch 19/150 | Batch 136/198 | Loss: 2.2902894020080566\n","Epoch 19/150 | Batch 137/198 | Loss: 2.3645689487457275\n","Epoch 19/150 | Batch 138/198 | Loss: 2.3223562240600586\n","Epoch 19/150 | Batch 139/198 | Loss: 1.8124631643295288\n","Epoch 19/150 | Batch 140/198 | Loss: 2.580314874649048\n","Epoch 19/150 | Batch 141/198 | Loss: 2.19682240486145\n","Epoch 19/150 | Batch 142/198 | Loss: 2.4662883281707764\n","Epoch 19/150 | Batch 143/198 | Loss: 2.1225197315216064\n","Epoch 19/150 | Batch 144/198 | Loss: 2.4725542068481445\n","Epoch 19/150 | Batch 145/198 | Loss: 2.383953809738159\n","Epoch 19/150 | Batch 146/198 | Loss: 2.4049341678619385\n","Epoch 19/150 | Batch 147/198 | Loss: 2.284670114517212\n","Epoch 19/150 | Batch 148/198 | Loss: 2.353400230407715\n","Epoch 19/150 | Batch 149/198 | Loss: 2.1526331901550293\n","Epoch 19/150 | Batch 150/198 | Loss: 2.238182544708252\n","Epoch 19/150 | Batch 151/198 | Loss: 2.3229176998138428\n","Epoch 19/150 | Batch 152/198 | Loss: 2.394754409790039\n","Epoch 19/150 | Batch 153/198 | Loss: 2.3675239086151123\n","Epoch 19/150 | Batch 154/198 | Loss: 2.3028533458709717\n","Epoch 19/150 | Batch 155/198 | Loss: 2.134805202484131\n","Epoch 19/150 | Batch 156/198 | Loss: 2.1559345722198486\n","Epoch 19/150 | Batch 157/198 | Loss: 2.399237632751465\n","Epoch 19/150 | Batch 158/198 | Loss: 2.0641164779663086\n","Epoch 19/150 | Batch 159/198 | Loss: 2.5101118087768555\n","Epoch 19/150 | Batch 160/198 | Loss: 2.3013293743133545\n","Epoch 19/150 | Batch 161/198 | Loss: 2.5031285285949707\n","Epoch 19/150 | Batch 162/198 | Loss: 2.4411542415618896\n","Epoch 19/150 | Batch 163/198 | Loss: 2.493626594543457\n","Epoch 19/150 | Batch 164/198 | Loss: 2.1868062019348145\n","Epoch 19/150 | Batch 165/198 | Loss: 2.071075201034546\n","Epoch 19/150 | Batch 166/198 | Loss: 2.305877923965454\n","Epoch 19/150 | Batch 167/198 | Loss: 2.3520560264587402\n","Epoch 19/150 | Batch 168/198 | Loss: 2.056769609451294\n","Epoch 19/150 | Batch 169/198 | Loss: 2.0857110023498535\n","Epoch 19/150 | Batch 170/198 | Loss: 2.528773069381714\n","Epoch 19/150 | Batch 171/198 | Loss: 2.2215845584869385\n","Epoch 19/150 | Batch 172/198 | Loss: 2.1361775398254395\n","Epoch 19/150 | Batch 173/198 | Loss: 2.2339799404144287\n","Epoch 19/150 | Batch 174/198 | Loss: 2.4717864990234375\n","Epoch 19/150 | Batch 175/198 | Loss: 2.379979372024536\n","Epoch 19/150 | Batch 176/198 | Loss: 2.407315254211426\n","Epoch 19/150 | Batch 177/198 | Loss: 2.3576438426971436\n","Epoch 19/150 | Batch 178/198 | Loss: 2.54297137260437\n","Epoch 19/150 | Batch 179/198 | Loss: 2.4546380043029785\n","Epoch 19/150 | Batch 180/198 | Loss: 2.51627254486084\n","Epoch 19/150 | Batch 181/198 | Loss: 2.4885191917419434\n","Epoch 19/150 | Batch 182/198 | Loss: 2.360605478286743\n","Epoch 19/150 | Batch 183/198 | Loss: 2.164623498916626\n","Epoch 19/150 | Batch 184/198 | Loss: 2.5292718410491943\n","Epoch 19/150 | Batch 185/198 | Loss: 2.7454380989074707\n","Epoch 19/150 | Batch 186/198 | Loss: 2.4857819080352783\n","Epoch 19/150 | Batch 187/198 | Loss: 2.3798718452453613\n","Epoch 19/150 | Batch 188/198 | Loss: 2.551710367202759\n","Epoch 19/150 | Batch 189/198 | Loss: 2.352904796600342\n","Epoch 19/150 | Batch 190/198 | Loss: 2.4427473545074463\n","Epoch 19/150 | Batch 191/198 | Loss: 2.3984508514404297\n","Epoch 19/150 | Batch 192/198 | Loss: 2.42228364944458\n","Epoch 19/150 | Batch 193/198 | Loss: 2.452465295791626\n","Epoch 19/150 | Batch 194/198 | Loss: 2.039968252182007\n","Epoch 19/150 | Batch 195/198 | Loss: 2.327651262283325\n","Epoch 19/150 | Batch 196/198 | Loss: 1.9217238426208496\n","Epoch 19/150 | Batch 197/198 | Loss: 2.29353666305542\n","Epoch 19/150 | Batch 198/198 | Loss: 2.474496364593506\n","Epoch 19/150 | Average Loss: 2.3401869126040524\n","Epoch 20/150 | Batch 1/198 | Loss: 2.191140651702881\n","Epoch 20/150 | Batch 2/198 | Loss: 2.3732645511627197\n","Epoch 20/150 | Batch 3/198 | Loss: 2.1467320919036865\n","Epoch 20/150 | Batch 4/198 | Loss: 2.2628417015075684\n","Epoch 20/150 | Batch 5/198 | Loss: 2.2113630771636963\n","Epoch 20/150 | Batch 6/198 | Loss: 2.4043972492218018\n","Epoch 20/150 | Batch 7/198 | Loss: 2.135749340057373\n","Epoch 20/150 | Batch 8/198 | Loss: 2.2476346492767334\n","Epoch 20/150 | Batch 9/198 | Loss: 1.9431320428848267\n","Epoch 20/150 | Batch 10/198 | Loss: 2.144667625427246\n","Epoch 20/150 | Batch 11/198 | Loss: 2.1427555084228516\n","Epoch 20/150 | Batch 12/198 | Loss: 2.318136692047119\n","Epoch 20/150 | Batch 13/198 | Loss: 2.27060866355896\n","Epoch 20/150 | Batch 14/198 | Loss: 2.145768404006958\n","Epoch 20/150 | Batch 15/198 | Loss: 2.2842936515808105\n","Epoch 20/150 | Batch 16/198 | Loss: 2.164914131164551\n","Epoch 20/150 | Batch 17/198 | Loss: 2.269883155822754\n","Epoch 20/150 | Batch 18/198 | Loss: 2.421112537384033\n","Epoch 20/150 | Batch 19/198 | Loss: 2.3587801456451416\n","Epoch 20/150 | Batch 20/198 | Loss: 2.3289847373962402\n","Epoch 20/150 | Batch 21/198 | Loss: 2.155496597290039\n","Epoch 20/150 | Batch 22/198 | Loss: 2.1952853202819824\n","Epoch 20/150 | Batch 23/198 | Loss: 2.1773250102996826\n","Epoch 20/150 | Batch 24/198 | Loss: 2.093143939971924\n","Epoch 20/150 | Batch 25/198 | Loss: 2.2848479747772217\n","Epoch 20/150 | Batch 26/198 | Loss: 2.2724764347076416\n","Epoch 20/150 | Batch 27/198 | Loss: 2.116875410079956\n","Epoch 20/150 | Batch 28/198 | Loss: 2.161785125732422\n","Epoch 20/150 | Batch 29/198 | Loss: 2.0118908882141113\n","Epoch 20/150 | Batch 30/198 | Loss: 2.2353856563568115\n","Epoch 20/150 | Batch 31/198 | Loss: 2.529953956604004\n","Epoch 20/150 | Batch 32/198 | Loss: 2.3564674854278564\n","Epoch 20/150 | Batch 33/198 | Loss: 1.9813508987426758\n","Epoch 20/150 | Batch 34/198 | Loss: 2.379882574081421\n","Epoch 20/150 | Batch 35/198 | Loss: 1.7943060398101807\n","Epoch 20/150 | Batch 36/198 | Loss: 1.9521607160568237\n","Epoch 20/150 | Batch 37/198 | Loss: 2.2600154876708984\n","Epoch 20/150 | Batch 38/198 | Loss: 2.039597511291504\n","Epoch 20/150 | Batch 39/198 | Loss: 2.3584651947021484\n","Epoch 20/150 | Batch 40/198 | Loss: 2.2806036472320557\n","Epoch 20/150 | Batch 41/198 | Loss: 2.131507158279419\n","Epoch 20/150 | Batch 42/198 | Loss: 2.1217522621154785\n","Epoch 20/150 | Batch 43/198 | Loss: 2.4277310371398926\n","Epoch 20/150 | Batch 44/198 | Loss: 2.059311866760254\n","Epoch 20/150 | Batch 45/198 | Loss: 2.3792455196380615\n","Epoch 20/150 | Batch 46/198 | Loss: 2.5027196407318115\n","Epoch 20/150 | Batch 47/198 | Loss: 2.0321743488311768\n","Epoch 20/150 | Batch 48/198 | Loss: 2.073625326156616\n","Epoch 20/150 | Batch 49/198 | Loss: 2.303248882293701\n","Epoch 20/150 | Batch 50/198 | Loss: 2.0493433475494385\n","Epoch 20/150 | Batch 51/198 | Loss: 2.268843412399292\n","Epoch 20/150 | Batch 52/198 | Loss: 2.366161346435547\n","Epoch 20/150 | Batch 53/198 | Loss: 2.0299084186553955\n","Epoch 20/150 | Batch 54/198 | Loss: 2.2143383026123047\n","Epoch 20/150 | Batch 55/198 | Loss: 1.9512512683868408\n","Epoch 20/150 | Batch 56/198 | Loss: 1.8715835809707642\n","Epoch 20/150 | Batch 57/198 | Loss: 2.0344998836517334\n","Epoch 20/150 | Batch 58/198 | Loss: 2.224639415740967\n","Epoch 20/150 | Batch 59/198 | Loss: 2.455150842666626\n","Epoch 20/150 | Batch 60/198 | Loss: 2.2247607707977295\n","Epoch 20/150 | Batch 61/198 | Loss: 2.040419816970825\n","Epoch 20/150 | Batch 62/198 | Loss: 2.4801032543182373\n","Epoch 20/150 | Batch 63/198 | Loss: 2.236403226852417\n","Epoch 20/150 | Batch 64/198 | Loss: 2.522048234939575\n","Epoch 20/150 | Batch 65/198 | Loss: 1.8920356035232544\n","Epoch 20/150 | Batch 66/198 | Loss: 2.0850830078125\n","Epoch 20/150 | Batch 67/198 | Loss: 2.1621265411376953\n","Epoch 20/150 | Batch 68/198 | Loss: 2.1752684116363525\n","Epoch 20/150 | Batch 69/198 | Loss: 2.0778729915618896\n","Epoch 20/150 | Batch 70/198 | Loss: 2.0783450603485107\n","Epoch 20/150 | Batch 71/198 | Loss: 2.1375572681427\n","Epoch 20/150 | Batch 72/198 | Loss: 2.1102490425109863\n","Epoch 20/150 | Batch 73/198 | Loss: 2.2591099739074707\n","Epoch 20/150 | Batch 74/198 | Loss: 2.1328065395355225\n","Epoch 20/150 | Batch 75/198 | Loss: 2.117135524749756\n","Epoch 20/150 | Batch 76/198 | Loss: 2.1861443519592285\n","Epoch 20/150 | Batch 77/198 | Loss: 2.4937872886657715\n","Epoch 20/150 | Batch 78/198 | Loss: 2.5441629886627197\n","Epoch 20/150 | Batch 79/198 | Loss: 2.068148612976074\n","Epoch 20/150 | Batch 80/198 | Loss: 2.0295112133026123\n","Epoch 20/150 | Batch 81/198 | Loss: 2.0891172885894775\n","Epoch 20/150 | Batch 82/198 | Loss: 1.9777555465698242\n","Epoch 20/150 | Batch 83/198 | Loss: 2.319936752319336\n","Epoch 20/150 | Batch 84/198 | Loss: 2.383054256439209\n","Epoch 20/150 | Batch 85/198 | Loss: 2.292870044708252\n","Epoch 20/150 | Batch 86/198 | Loss: 2.036912679672241\n","Epoch 20/150 | Batch 87/198 | Loss: 2.2616846561431885\n","Epoch 20/150 | Batch 88/198 | Loss: 2.2959771156311035\n","Epoch 20/150 | Batch 89/198 | Loss: 2.346627712249756\n","Epoch 20/150 | Batch 90/198 | Loss: 2.2901864051818848\n","Epoch 20/150 | Batch 91/198 | Loss: 2.2603323459625244\n","Epoch 20/150 | Batch 92/198 | Loss: 2.4444448947906494\n","Epoch 20/150 | Batch 93/198 | Loss: 2.393125057220459\n","Epoch 20/150 | Batch 94/198 | Loss: 1.890929937362671\n","Epoch 20/150 | Batch 95/198 | Loss: 2.3629069328308105\n","Epoch 20/150 | Batch 96/198 | Loss: 2.2627546787261963\n","Epoch 20/150 | Batch 97/198 | Loss: 2.4533488750457764\n","Epoch 20/150 | Batch 98/198 | Loss: 2.3741190433502197\n","Epoch 20/150 | Batch 99/198 | Loss: 2.2498619556427\n","Epoch 20/150 | Batch 100/198 | Loss: 2.0709872245788574\n","Epoch 20/150 | Batch 101/198 | Loss: 2.0011277198791504\n","Epoch 20/150 | Batch 102/198 | Loss: 2.251117706298828\n","Epoch 20/150 | Batch 103/198 | Loss: 2.2032454013824463\n","Epoch 20/150 | Batch 104/198 | Loss: 2.3046112060546875\n","Epoch 20/150 | Batch 105/198 | Loss: 2.278292179107666\n","Epoch 20/150 | Batch 106/198 | Loss: 2.399491310119629\n","Epoch 20/150 | Batch 107/198 | Loss: 2.3038508892059326\n","Epoch 20/150 | Batch 108/198 | Loss: 2.34295916557312\n","Epoch 20/150 | Batch 109/198 | Loss: 2.5163183212280273\n","Epoch 20/150 | Batch 110/198 | Loss: 2.1519265174865723\n","Epoch 20/150 | Batch 111/198 | Loss: 2.1953704357147217\n","Epoch 20/150 | Batch 112/198 | Loss: 2.1485767364501953\n","Epoch 20/150 | Batch 113/198 | Loss: 2.3158295154571533\n","Epoch 20/150 | Batch 114/198 | Loss: 2.205836296081543\n","Epoch 20/150 | Batch 115/198 | Loss: 2.3252949714660645\n","Epoch 20/150 | Batch 116/198 | Loss: 2.152991533279419\n","Epoch 20/150 | Batch 117/198 | Loss: 2.4579989910125732\n","Epoch 20/150 | Batch 118/198 | Loss: 1.9221796989440918\n","Epoch 20/150 | Batch 119/198 | Loss: 2.0580453872680664\n","Epoch 20/150 | Batch 120/198 | Loss: 1.9562026262283325\n","Epoch 20/150 | Batch 121/198 | Loss: 2.079435110092163\n","Epoch 20/150 | Batch 122/198 | Loss: 2.2056660652160645\n","Epoch 20/150 | Batch 123/198 | Loss: 2.5387778282165527\n","Epoch 20/150 | Batch 124/198 | Loss: 2.2840027809143066\n","Epoch 20/150 | Batch 125/198 | Loss: 2.2057807445526123\n","Epoch 20/150 | Batch 126/198 | Loss: 2.412572145462036\n","Epoch 20/150 | Batch 127/198 | Loss: 1.961248517036438\n","Epoch 20/150 | Batch 128/198 | Loss: 2.2184557914733887\n","Epoch 20/150 | Batch 129/198 | Loss: 2.358820676803589\n","Epoch 20/150 | Batch 130/198 | Loss: 2.2291312217712402\n","Epoch 20/150 | Batch 131/198 | Loss: 2.2683796882629395\n","Epoch 20/150 | Batch 132/198 | Loss: 2.4794163703918457\n","Epoch 20/150 | Batch 133/198 | Loss: 2.1137235164642334\n","Epoch 20/150 | Batch 134/198 | Loss: 2.506171464920044\n","Epoch 20/150 | Batch 135/198 | Loss: 2.0431313514709473\n","Epoch 20/150 | Batch 136/198 | Loss: 2.142866611480713\n","Epoch 20/150 | Batch 137/198 | Loss: 2.542750358581543\n","Epoch 20/150 | Batch 138/198 | Loss: 2.371978282928467\n","Epoch 20/150 | Batch 139/198 | Loss: 2.405477285385132\n","Epoch 20/150 | Batch 140/198 | Loss: 2.3895018100738525\n","Epoch 20/150 | Batch 141/198 | Loss: 2.06441593170166\n","Epoch 20/150 | Batch 142/198 | Loss: 2.5054690837860107\n","Epoch 20/150 | Batch 143/198 | Loss: 2.2060861587524414\n","Epoch 20/150 | Batch 144/198 | Loss: 2.536717414855957\n","Epoch 20/150 | Batch 145/198 | Loss: 2.29579496383667\n","Epoch 20/150 | Batch 146/198 | Loss: 2.4395751953125\n","Epoch 20/150 | Batch 147/198 | Loss: 2.231602191925049\n","Epoch 20/150 | Batch 148/198 | Loss: 2.3651111125946045\n","Epoch 20/150 | Batch 149/198 | Loss: 2.48238468170166\n","Epoch 20/150 | Batch 150/198 | Loss: 2.0411276817321777\n","Epoch 20/150 | Batch 151/198 | Loss: 2.4973883628845215\n","Epoch 20/150 | Batch 152/198 | Loss: 2.3905045986175537\n","Epoch 20/150 | Batch 153/198 | Loss: 2.4027254581451416\n","Epoch 20/150 | Batch 154/198 | Loss: 2.117145299911499\n","Epoch 20/150 | Batch 155/198 | Loss: 2.191373348236084\n","Epoch 20/150 | Batch 156/198 | Loss: 2.2199108600616455\n","Epoch 20/150 | Batch 157/198 | Loss: 2.2422034740448\n","Epoch 20/150 | Batch 158/198 | Loss: 1.9197487831115723\n","Epoch 20/150 | Batch 159/198 | Loss: 2.1707944869995117\n","Epoch 20/150 | Batch 160/198 | Loss: 2.2585816383361816\n","Epoch 20/150 | Batch 161/198 | Loss: 2.502774953842163\n","Epoch 20/150 | Batch 162/198 | Loss: 2.356151819229126\n","Epoch 20/150 | Batch 163/198 | Loss: 2.3003597259521484\n","Epoch 20/150 | Batch 164/198 | Loss: 2.4951257705688477\n","Epoch 20/150 | Batch 165/198 | Loss: 2.3521738052368164\n","Epoch 20/150 | Batch 166/198 | Loss: 2.238419532775879\n","Epoch 20/150 | Batch 167/198 | Loss: 2.3925602436065674\n","Epoch 20/150 | Batch 168/198 | Loss: 2.413203716278076\n","Epoch 20/150 | Batch 169/198 | Loss: 2.2644097805023193\n","Epoch 20/150 | Batch 170/198 | Loss: 2.4828007221221924\n","Epoch 20/150 | Batch 171/198 | Loss: 2.1688296794891357\n","Epoch 20/150 | Batch 172/198 | Loss: 2.073183536529541\n","Epoch 20/150 | Batch 173/198 | Loss: 2.1487631797790527\n","Epoch 20/150 | Batch 174/198 | Loss: 2.3647775650024414\n","Epoch 20/150 | Batch 175/198 | Loss: 2.1489408016204834\n","Epoch 20/150 | Batch 176/198 | Loss: 2.4018394947052\n","Epoch 20/150 | Batch 177/198 | Loss: 2.1596262454986572\n","Epoch 20/150 | Batch 178/198 | Loss: 2.5205330848693848\n","Epoch 20/150 | Batch 179/198 | Loss: 2.659874439239502\n","Epoch 20/150 | Batch 180/198 | Loss: 2.069689989089966\n","Epoch 20/150 | Batch 181/198 | Loss: 1.9637807607650757\n","Epoch 20/150 | Batch 182/198 | Loss: 2.0497236251831055\n","Epoch 20/150 | Batch 183/198 | Loss: 2.6066737174987793\n","Epoch 20/150 | Batch 184/198 | Loss: 2.2965879440307617\n","Epoch 20/150 | Batch 185/198 | Loss: 2.3178341388702393\n","Epoch 20/150 | Batch 186/198 | Loss: 2.413642168045044\n","Epoch 20/150 | Batch 187/198 | Loss: 2.184967517852783\n","Epoch 20/150 | Batch 188/198 | Loss: 2.3190395832061768\n","Epoch 20/150 | Batch 189/198 | Loss: 2.1384124755859375\n","Epoch 20/150 | Batch 190/198 | Loss: 2.3699302673339844\n","Epoch 20/150 | Batch 191/198 | Loss: 2.3238584995269775\n","Epoch 20/150 | Batch 192/198 | Loss: 2.3400938510894775\n","Epoch 20/150 | Batch 193/198 | Loss: 2.513479709625244\n","Epoch 20/150 | Batch 194/198 | Loss: 2.3606865406036377\n","Epoch 20/150 | Batch 195/198 | Loss: 2.0908327102661133\n","Epoch 20/150 | Batch 196/198 | Loss: 2.248195171356201\n","Epoch 20/150 | Batch 197/198 | Loss: 2.3365631103515625\n","Epoch 20/150 | Batch 198/198 | Loss: 2.1954128742218018\n","Epoch 20/150 | Average Loss: 2.2445529958214423\n","Epoch 21/150 | Batch 1/198 | Loss: 2.092489004135132\n","Epoch 21/150 | Batch 2/198 | Loss: 2.0701236724853516\n","Epoch 21/150 | Batch 3/198 | Loss: 2.231847047805786\n","Epoch 21/150 | Batch 4/198 | Loss: 2.4434545040130615\n","Epoch 21/150 | Batch 5/198 | Loss: 2.3430473804473877\n","Epoch 21/150 | Batch 6/198 | Loss: 1.8118997812271118\n","Epoch 21/150 | Batch 7/198 | Loss: 2.4834659099578857\n","Epoch 21/150 | Batch 8/198 | Loss: 1.7533814907073975\n","Epoch 21/150 | Batch 9/198 | Loss: 2.2571604251861572\n","Epoch 21/150 | Batch 10/198 | Loss: 2.039045572280884\n","Epoch 21/150 | Batch 11/198 | Loss: 2.018256664276123\n","Epoch 21/150 | Batch 12/198 | Loss: 2.111931800842285\n","Epoch 21/150 | Batch 13/198 | Loss: 2.2823965549468994\n","Epoch 21/150 | Batch 14/198 | Loss: 2.037536859512329\n","Epoch 21/150 | Batch 15/198 | Loss: 2.289297580718994\n","Epoch 21/150 | Batch 16/198 | Loss: 1.9301738739013672\n","Epoch 21/150 | Batch 17/198 | Loss: 2.374115467071533\n","Epoch 21/150 | Batch 18/198 | Loss: 1.7741801738739014\n","Epoch 21/150 | Batch 19/198 | Loss: 2.303598642349243\n","Epoch 21/150 | Batch 20/198 | Loss: 1.7278441190719604\n","Epoch 21/150 | Batch 21/198 | Loss: 2.263488292694092\n","Epoch 21/150 | Batch 22/198 | Loss: 1.9331676959991455\n","Epoch 21/150 | Batch 23/198 | Loss: 1.917717456817627\n","Epoch 21/150 | Batch 24/198 | Loss: 1.9212321043014526\n","Epoch 21/150 | Batch 25/198 | Loss: 2.124122142791748\n","Epoch 21/150 | Batch 26/198 | Loss: 2.1561057567596436\n","Epoch 21/150 | Batch 27/198 | Loss: 2.419820547103882\n","Epoch 21/150 | Batch 28/198 | Loss: 2.040422201156616\n","Epoch 21/150 | Batch 29/198 | Loss: 2.150771379470825\n","Epoch 21/150 | Batch 30/198 | Loss: 1.977128505706787\n","Epoch 21/150 | Batch 31/198 | Loss: 2.0663208961486816\n","Epoch 21/150 | Batch 32/198 | Loss: 1.9297417402267456\n","Epoch 21/150 | Batch 33/198 | Loss: 2.338958501815796\n","Epoch 21/150 | Batch 34/198 | Loss: 2.2365882396698\n","Epoch 21/150 | Batch 35/198 | Loss: 2.3219640254974365\n","Epoch 21/150 | Batch 36/198 | Loss: 2.2326345443725586\n","Epoch 21/150 | Batch 37/198 | Loss: 2.035156726837158\n","Epoch 21/150 | Batch 38/198 | Loss: 2.001619577407837\n","Epoch 21/150 | Batch 39/198 | Loss: 1.8741225004196167\n","Epoch 21/150 | Batch 40/198 | Loss: 2.189819812774658\n","Epoch 21/150 | Batch 41/198 | Loss: 2.2347588539123535\n","Epoch 21/150 | Batch 42/198 | Loss: 2.207369804382324\n","Epoch 21/150 | Batch 43/198 | Loss: 2.225001335144043\n","Epoch 21/150 | Batch 44/198 | Loss: 2.0683484077453613\n","Epoch 21/150 | Batch 45/198 | Loss: 2.154250383377075\n","Epoch 21/150 | Batch 46/198 | Loss: 2.221026659011841\n","Epoch 21/150 | Batch 47/198 | Loss: 2.1708931922912598\n","Epoch 21/150 | Batch 48/198 | Loss: 2.0744524002075195\n","Epoch 21/150 | Batch 49/198 | Loss: 1.8593829870224\n","Epoch 21/150 | Batch 50/198 | Loss: 2.071485757827759\n","Epoch 21/150 | Batch 51/198 | Loss: 1.8987414836883545\n","Epoch 21/150 | Batch 52/198 | Loss: 2.005916118621826\n","Epoch 21/150 | Batch 53/198 | Loss: 2.1424665451049805\n","Epoch 21/150 | Batch 54/198 | Loss: 2.042044162750244\n","Epoch 21/150 | Batch 55/198 | Loss: 1.855141520500183\n","Epoch 21/150 | Batch 56/198 | Loss: 2.187119483947754\n","Epoch 21/150 | Batch 57/198 | Loss: 2.334268808364868\n","Epoch 21/150 | Batch 58/198 | Loss: 2.2194511890411377\n","Epoch 21/150 | Batch 59/198 | Loss: 2.469982385635376\n","Epoch 21/150 | Batch 60/198 | Loss: 2.1183884143829346\n","Epoch 21/150 | Batch 61/198 | Loss: 2.1160130500793457\n","Epoch 21/150 | Batch 62/198 | Loss: 1.8900541067123413\n","Epoch 21/150 | Batch 63/198 | Loss: 2.4591898918151855\n","Epoch 21/150 | Batch 64/198 | Loss: 1.8949769735336304\n","Epoch 21/150 | Batch 65/198 | Loss: 2.255577564239502\n","Epoch 21/150 | Batch 66/198 | Loss: 2.036902904510498\n","Epoch 21/150 | Batch 67/198 | Loss: 2.0259180068969727\n","Epoch 21/150 | Batch 68/198 | Loss: 2.104989528656006\n","Epoch 21/150 | Batch 69/198 | Loss: 1.9153494834899902\n","Epoch 21/150 | Batch 70/198 | Loss: 2.3033318519592285\n","Epoch 21/150 | Batch 71/198 | Loss: 2.2903246879577637\n","Epoch 21/150 | Batch 72/198 | Loss: 2.216702938079834\n","Epoch 21/150 | Batch 73/198 | Loss: 2.129222869873047\n","Epoch 21/150 | Batch 74/198 | Loss: 1.9665864706039429\n","Epoch 21/150 | Batch 75/198 | Loss: 2.1718902587890625\n","Epoch 21/150 | Batch 76/198 | Loss: 1.8000837564468384\n","Epoch 21/150 | Batch 77/198 | Loss: 2.236173391342163\n","Epoch 21/150 | Batch 78/198 | Loss: 2.1714165210723877\n","Epoch 21/150 | Batch 79/198 | Loss: 2.3392748832702637\n","Epoch 21/150 | Batch 80/198 | Loss: 2.0568809509277344\n","Epoch 21/150 | Batch 81/198 | Loss: 2.5090627670288086\n","Epoch 21/150 | Batch 82/198 | Loss: 2.353151798248291\n","Epoch 21/150 | Batch 83/198 | Loss: 1.9919488430023193\n","Epoch 21/150 | Batch 84/198 | Loss: 2.263699531555176\n","Epoch 21/150 | Batch 85/198 | Loss: 1.7801350355148315\n","Epoch 21/150 | Batch 86/198 | Loss: 2.3472909927368164\n","Epoch 21/150 | Batch 87/198 | Loss: 2.002721071243286\n","Epoch 21/150 | Batch 88/198 | Loss: 2.2589271068573\n","Epoch 21/150 | Batch 89/198 | Loss: 2.4780478477478027\n","Epoch 21/150 | Batch 90/198 | Loss: 2.118121862411499\n","Epoch 21/150 | Batch 91/198 | Loss: 2.203094482421875\n","Epoch 21/150 | Batch 92/198 | Loss: 2.223996639251709\n","Epoch 21/150 | Batch 93/198 | Loss: 2.1764957904815674\n","Epoch 21/150 | Batch 94/198 | Loss: 2.3004143238067627\n","Epoch 21/150 | Batch 95/198 | Loss: 2.518649101257324\n","Epoch 21/150 | Batch 96/198 | Loss: 2.3673834800720215\n","Epoch 21/150 | Batch 97/198 | Loss: 2.3986926078796387\n","Epoch 21/150 | Batch 98/198 | Loss: 2.121394395828247\n","Epoch 21/150 | Batch 99/198 | Loss: 2.3989834785461426\n","Epoch 21/150 | Batch 100/198 | Loss: 2.5343310832977295\n","Epoch 21/150 | Batch 101/198 | Loss: 2.05562162399292\n","Epoch 21/150 | Batch 102/198 | Loss: 2.2075676918029785\n","Epoch 21/150 | Batch 103/198 | Loss: 2.069720983505249\n","Epoch 21/150 | Batch 104/198 | Loss: 2.13848614692688\n","Epoch 21/150 | Batch 105/198 | Loss: 2.1621954441070557\n","Epoch 21/150 | Batch 106/198 | Loss: 2.061678409576416\n","Epoch 21/150 | Batch 107/198 | Loss: 2.270270824432373\n","Epoch 21/150 | Batch 108/198 | Loss: 2.070246696472168\n","Epoch 21/150 | Batch 109/198 | Loss: 2.2849996089935303\n","Epoch 21/150 | Batch 110/198 | Loss: 2.174028158187866\n","Epoch 21/150 | Batch 111/198 | Loss: 2.2027599811553955\n","Epoch 21/150 | Batch 112/198 | Loss: 2.288358449935913\n","Epoch 21/150 | Batch 113/198 | Loss: 2.1038360595703125\n","Epoch 21/150 | Batch 114/198 | Loss: 2.2238993644714355\n","Epoch 21/150 | Batch 115/198 | Loss: 2.045300006866455\n","Epoch 21/150 | Batch 116/198 | Loss: 2.3314716815948486\n","Epoch 21/150 | Batch 117/198 | Loss: 2.069063186645508\n","Epoch 21/150 | Batch 118/198 | Loss: 1.923796534538269\n","Epoch 21/150 | Batch 119/198 | Loss: 2.2221179008483887\n","Epoch 21/150 | Batch 120/198 | Loss: 2.1894586086273193\n","Epoch 21/150 | Batch 121/198 | Loss: 2.3896596431732178\n","Epoch 21/150 | Batch 122/198 | Loss: 2.3811397552490234\n","Epoch 21/150 | Batch 123/198 | Loss: 1.973989725112915\n","Epoch 21/150 | Batch 124/198 | Loss: 2.112804889678955\n","Epoch 21/150 | Batch 125/198 | Loss: 2.042592763900757\n","Epoch 21/150 | Batch 126/198 | Loss: 2.3314127922058105\n","Epoch 21/150 | Batch 127/198 | Loss: 2.61538028717041\n","Epoch 21/150 | Batch 128/198 | Loss: 2.172149419784546\n","Epoch 21/150 | Batch 129/198 | Loss: 2.280151844024658\n","Epoch 21/150 | Batch 130/198 | Loss: 1.8594385385513306\n","Epoch 21/150 | Batch 131/198 | Loss: 2.38122820854187\n","Epoch 21/150 | Batch 132/198 | Loss: 2.267923593521118\n","Epoch 21/150 | Batch 133/198 | Loss: 2.2247352600097656\n","Epoch 21/150 | Batch 134/198 | Loss: 2.154933452606201\n","Epoch 21/150 | Batch 135/198 | Loss: 2.305722951889038\n","Epoch 21/150 | Batch 136/198 | Loss: 2.4812278747558594\n","Epoch 21/150 | Batch 137/198 | Loss: 2.0729570388793945\n","Epoch 21/150 | Batch 138/198 | Loss: 2.0333075523376465\n","Epoch 21/150 | Batch 139/198 | Loss: 2.6041383743286133\n","Epoch 21/150 | Batch 140/198 | Loss: 2.0990185737609863\n","Epoch 21/150 | Batch 141/198 | Loss: 2.297065019607544\n","Epoch 21/150 | Batch 142/198 | Loss: 2.1000730991363525\n","Epoch 21/150 | Batch 143/198 | Loss: 2.0179848670959473\n","Epoch 21/150 | Batch 144/198 | Loss: 1.9585720300674438\n","Epoch 21/150 | Batch 145/198 | Loss: 2.27022647857666\n","Epoch 21/150 | Batch 146/198 | Loss: 1.808664083480835\n","Epoch 21/150 | Batch 147/198 | Loss: 2.200592517852783\n","Epoch 21/150 | Batch 148/198 | Loss: 2.13411021232605\n","Epoch 21/150 | Batch 149/198 | Loss: 2.0783140659332275\n","Epoch 21/150 | Batch 150/198 | Loss: 2.2279486656188965\n","Epoch 21/150 | Batch 151/198 | Loss: 2.205162286758423\n","Epoch 21/150 | Batch 152/198 | Loss: 1.9357937574386597\n","Epoch 21/150 | Batch 153/198 | Loss: 2.0337107181549072\n","Epoch 21/150 | Batch 154/198 | Loss: 2.5780210494995117\n","Epoch 21/150 | Batch 155/198 | Loss: 2.2364463806152344\n","Epoch 21/150 | Batch 156/198 | Loss: 1.9092634916305542\n","Epoch 21/150 | Batch 157/198 | Loss: 2.5609748363494873\n","Epoch 21/150 | Batch 158/198 | Loss: 2.2623493671417236\n","Epoch 21/150 | Batch 159/198 | Loss: 2.172393798828125\n","Epoch 21/150 | Batch 160/198 | Loss: 2.2844858169555664\n","Epoch 21/150 | Batch 161/198 | Loss: 1.9411523342132568\n","Epoch 21/150 | Batch 162/198 | Loss: 2.2973833084106445\n","Epoch 21/150 | Batch 163/198 | Loss: 1.890413761138916\n","Epoch 21/150 | Batch 164/198 | Loss: 2.0606541633605957\n","Epoch 21/150 | Batch 165/198 | Loss: 2.0511250495910645\n","Epoch 21/150 | Batch 166/198 | Loss: 2.1982452869415283\n","Epoch 21/150 | Batch 167/198 | Loss: 2.3196089267730713\n","Epoch 21/150 | Batch 168/198 | Loss: 2.248964309692383\n","Epoch 21/150 | Batch 169/198 | Loss: 2.1773247718811035\n","Epoch 21/150 | Batch 170/198 | Loss: 1.9979398250579834\n","Epoch 21/150 | Batch 171/198 | Loss: 1.965623140335083\n","Epoch 21/150 | Batch 172/198 | Loss: 2.2999656200408936\n","Epoch 21/150 | Batch 173/198 | Loss: 2.2322871685028076\n","Epoch 21/150 | Batch 174/198 | Loss: 2.0630884170532227\n","Epoch 21/150 | Batch 175/198 | Loss: 2.311048746109009\n","Epoch 21/150 | Batch 176/198 | Loss: 2.17935848236084\n","Epoch 21/150 | Batch 177/198 | Loss: 2.2198405265808105\n","Epoch 21/150 | Batch 178/198 | Loss: 2.351062774658203\n","Epoch 21/150 | Batch 179/198 | Loss: 2.1684956550598145\n","Epoch 21/150 | Batch 180/198 | Loss: 2.4696249961853027\n","Epoch 21/150 | Batch 181/198 | Loss: 2.137059211730957\n","Epoch 21/150 | Batch 182/198 | Loss: 1.995455026626587\n","Epoch 21/150 | Batch 183/198 | Loss: 2.120319366455078\n","Epoch 21/150 | Batch 184/198 | Loss: 2.217221260070801\n","Epoch 21/150 | Batch 185/198 | Loss: 2.1706926822662354\n","Epoch 21/150 | Batch 186/198 | Loss: 1.9765300750732422\n","Epoch 21/150 | Batch 187/198 | Loss: 2.1794424057006836\n","Epoch 21/150 | Batch 188/198 | Loss: 1.9184083938598633\n","Epoch 21/150 | Batch 189/198 | Loss: 2.133326530456543\n","Epoch 21/150 | Batch 190/198 | Loss: 2.234351634979248\n","Epoch 21/150 | Batch 191/198 | Loss: 2.2642910480499268\n","Epoch 21/150 | Batch 192/198 | Loss: 2.206731081008911\n","Epoch 21/150 | Batch 193/198 | Loss: 2.1621382236480713\n","Epoch 21/150 | Batch 194/198 | Loss: 2.261707067489624\n","Epoch 21/150 | Batch 195/198 | Loss: 1.9182651042938232\n","Epoch 21/150 | Batch 196/198 | Loss: 2.173065423965454\n","Epoch 21/150 | Batch 197/198 | Loss: 1.9929174184799194\n","Epoch 21/150 | Batch 198/198 | Loss: 2.3137614727020264\n","Epoch 21/150 | Average Loss: 2.160423608741375\n","Epoch 22/150 | Batch 1/198 | Loss: 2.076788902282715\n","Epoch 22/150 | Batch 2/198 | Loss: 2.3470683097839355\n","Epoch 22/150 | Batch 3/198 | Loss: 2.1715149879455566\n","Epoch 22/150 | Batch 4/198 | Loss: 2.2390904426574707\n","Epoch 22/150 | Batch 5/198 | Loss: 2.119966506958008\n","Epoch 22/150 | Batch 6/198 | Loss: 2.2536816596984863\n","Epoch 22/150 | Batch 7/198 | Loss: 1.5590540170669556\n","Epoch 22/150 | Batch 8/198 | Loss: 1.9982779026031494\n","Epoch 22/150 | Batch 9/198 | Loss: 2.0261569023132324\n","Epoch 22/150 | Batch 10/198 | Loss: 1.8785312175750732\n","Epoch 22/150 | Batch 11/198 | Loss: 2.018836736679077\n","Epoch 22/150 | Batch 12/198 | Loss: 2.1800222396850586\n","Epoch 22/150 | Batch 13/198 | Loss: 2.04441237449646\n","Epoch 22/150 | Batch 14/198 | Loss: 1.6700855493545532\n","Epoch 22/150 | Batch 15/198 | Loss: 2.0370779037475586\n","Epoch 22/150 | Batch 16/198 | Loss: 2.01672101020813\n","Epoch 22/150 | Batch 17/198 | Loss: 2.140547513961792\n","Epoch 22/150 | Batch 18/198 | Loss: 2.2956275939941406\n","Epoch 22/150 | Batch 19/198 | Loss: 1.9200011491775513\n","Epoch 22/150 | Batch 20/198 | Loss: 1.8659809827804565\n","Epoch 22/150 | Batch 21/198 | Loss: 2.057621955871582\n","Epoch 22/150 | Batch 22/198 | Loss: 2.1996543407440186\n","Epoch 22/150 | Batch 23/198 | Loss: 1.7015247344970703\n","Epoch 22/150 | Batch 24/198 | Loss: 1.807523250579834\n","Epoch 22/150 | Batch 25/198 | Loss: 2.4056599140167236\n","Epoch 22/150 | Batch 26/198 | Loss: 1.8726789951324463\n","Epoch 22/150 | Batch 27/198 | Loss: 2.3186540603637695\n","Epoch 22/150 | Batch 28/198 | Loss: 2.1387133598327637\n","Epoch 22/150 | Batch 29/198 | Loss: 1.7148723602294922\n","Epoch 22/150 | Batch 30/198 | Loss: 2.1874122619628906\n","Epoch 22/150 | Batch 31/198 | Loss: 2.0680792331695557\n","Epoch 22/150 | Batch 32/198 | Loss: 2.0385661125183105\n","Epoch 22/150 | Batch 33/198 | Loss: 1.8221877813339233\n","Epoch 22/150 | Batch 34/198 | Loss: 2.1077446937561035\n","Epoch 22/150 | Batch 35/198 | Loss: 2.286126136779785\n","Epoch 22/150 | Batch 36/198 | Loss: 2.1065471172332764\n","Epoch 22/150 | Batch 37/198 | Loss: 1.9448503255844116\n","Epoch 22/150 | Batch 38/198 | Loss: 2.2213737964630127\n","Epoch 22/150 | Batch 39/198 | Loss: 2.0030734539031982\n","Epoch 22/150 | Batch 40/198 | Loss: 2.1593639850616455\n","Epoch 22/150 | Batch 41/198 | Loss: 1.9623926877975464\n","Epoch 22/150 | Batch 42/198 | Loss: 1.7957912683486938\n","Epoch 22/150 | Batch 43/198 | Loss: 2.226621150970459\n","Epoch 22/150 | Batch 44/198 | Loss: 2.1076786518096924\n","Epoch 22/150 | Batch 45/198 | Loss: 1.7884615659713745\n","Epoch 22/150 | Batch 46/198 | Loss: 2.029478073120117\n","Epoch 22/150 | Batch 47/198 | Loss: 2.392148971557617\n","Epoch 22/150 | Batch 48/198 | Loss: 2.0879950523376465\n","Epoch 22/150 | Batch 49/198 | Loss: 2.0058858394622803\n","Epoch 22/150 | Batch 50/198 | Loss: 2.15177845954895\n","Epoch 22/150 | Batch 51/198 | Loss: 2.0216434001922607\n","Epoch 22/150 | Batch 52/198 | Loss: 2.130491018295288\n","Epoch 22/150 | Batch 53/198 | Loss: 2.106329917907715\n","Epoch 22/150 | Batch 54/198 | Loss: 2.2723162174224854\n","Epoch 22/150 | Batch 55/198 | Loss: 2.1757819652557373\n","Epoch 22/150 | Batch 56/198 | Loss: 2.1057627201080322\n","Epoch 22/150 | Batch 57/198 | Loss: 2.1013059616088867\n","Epoch 22/150 | Batch 58/198 | Loss: 1.9052523374557495\n","Epoch 22/150 | Batch 59/198 | Loss: 2.1800010204315186\n","Epoch 22/150 | Batch 60/198 | Loss: 2.0352048873901367\n","Epoch 22/150 | Batch 61/198 | Loss: 1.9743056297302246\n","Epoch 22/150 | Batch 62/198 | Loss: 2.09167742729187\n","Epoch 22/150 | Batch 63/198 | Loss: 2.354865789413452\n","Epoch 22/150 | Batch 64/198 | Loss: 2.2440481185913086\n","Epoch 22/150 | Batch 65/198 | Loss: 1.8704553842544556\n","Epoch 22/150 | Batch 66/198 | Loss: 2.0506784915924072\n","Epoch 22/150 | Batch 67/198 | Loss: 2.0588889122009277\n","Epoch 22/150 | Batch 68/198 | Loss: 2.119351863861084\n","Epoch 22/150 | Batch 69/198 | Loss: 2.0195226669311523\n","Epoch 22/150 | Batch 70/198 | Loss: 1.8014212846755981\n","Epoch 22/150 | Batch 71/198 | Loss: 1.8723320960998535\n","Epoch 22/150 | Batch 72/198 | Loss: 2.202779769897461\n","Epoch 22/150 | Batch 73/198 | Loss: 2.0491437911987305\n","Epoch 22/150 | Batch 74/198 | Loss: 1.947099208831787\n","Epoch 22/150 | Batch 75/198 | Loss: 1.9405790567398071\n","Epoch 22/150 | Batch 76/198 | Loss: 1.6431939601898193\n","Epoch 22/150 | Batch 77/198 | Loss: 2.0108354091644287\n","Epoch 22/150 | Batch 78/198 | Loss: 2.5069448947906494\n","Epoch 22/150 | Batch 79/198 | Loss: 2.4359965324401855\n","Epoch 22/150 | Batch 80/198 | Loss: 1.641046404838562\n","Epoch 22/150 | Batch 81/198 | Loss: 2.1824636459350586\n","Epoch 22/150 | Batch 82/198 | Loss: 2.287862539291382\n","Epoch 22/150 | Batch 83/198 | Loss: 2.064393997192383\n","Epoch 22/150 | Batch 84/198 | Loss: 2.437601089477539\n","Epoch 22/150 | Batch 85/198 | Loss: 2.2428174018859863\n","Epoch 22/150 | Batch 86/198 | Loss: 2.189481496810913\n","Epoch 22/150 | Batch 87/198 | Loss: 1.9668376445770264\n","Epoch 22/150 | Batch 88/198 | Loss: 2.080188512802124\n","Epoch 22/150 | Batch 89/198 | Loss: 2.20068359375\n","Epoch 22/150 | Batch 90/198 | Loss: 2.056614637374878\n","Epoch 22/150 | Batch 91/198 | Loss: 2.065765857696533\n","Epoch 22/150 | Batch 92/198 | Loss: 2.1446807384490967\n","Epoch 22/150 | Batch 93/198 | Loss: 2.4664466381073\n","Epoch 22/150 | Batch 94/198 | Loss: 2.1091456413269043\n","Epoch 22/150 | Batch 95/198 | Loss: 2.0106911659240723\n","Epoch 22/150 | Batch 96/198 | Loss: 2.1174368858337402\n","Epoch 22/150 | Batch 97/198 | Loss: 1.76810884475708\n","Epoch 22/150 | Batch 98/198 | Loss: 2.380169153213501\n","Epoch 22/150 | Batch 99/198 | Loss: 2.3746049404144287\n","Epoch 22/150 | Batch 100/198 | Loss: 2.250173330307007\n","Epoch 22/150 | Batch 101/198 | Loss: 2.2359135150909424\n","Epoch 22/150 | Batch 102/198 | Loss: 1.7851184606552124\n","Epoch 22/150 | Batch 103/198 | Loss: 2.2592012882232666\n","Epoch 22/150 | Batch 104/198 | Loss: 1.8314895629882812\n","Epoch 22/150 | Batch 105/198 | Loss: 1.9528933763504028\n","Epoch 22/150 | Batch 106/198 | Loss: 2.058715581893921\n","Epoch 22/150 | Batch 107/198 | Loss: 1.7382174730300903\n","Epoch 22/150 | Batch 108/198 | Loss: 2.1652660369873047\n","Epoch 22/150 | Batch 109/198 | Loss: 2.196481466293335\n","Epoch 22/150 | Batch 110/198 | Loss: 1.831286907196045\n","Epoch 22/150 | Batch 111/198 | Loss: 2.0448055267333984\n","Epoch 22/150 | Batch 112/198 | Loss: 1.9991663694381714\n","Epoch 22/150 | Batch 113/198 | Loss: 2.0991175174713135\n","Epoch 22/150 | Batch 114/198 | Loss: 1.943339228630066\n","Epoch 22/150 | Batch 115/198 | Loss: 2.126720428466797\n","Epoch 22/150 | Batch 116/198 | Loss: 1.7002363204956055\n","Epoch 22/150 | Batch 117/198 | Loss: 2.356585741043091\n","Epoch 22/150 | Batch 118/198 | Loss: 2.1239569187164307\n","Epoch 22/150 | Batch 119/198 | Loss: 2.2959001064300537\n","Epoch 22/150 | Batch 120/198 | Loss: 1.8401825428009033\n","Epoch 22/150 | Batch 121/198 | Loss: 2.2991433143615723\n","Epoch 22/150 | Batch 122/198 | Loss: 2.126546859741211\n","Epoch 22/150 | Batch 123/198 | Loss: 2.1839773654937744\n","Epoch 22/150 | Batch 124/198 | Loss: 2.220947504043579\n","Epoch 22/150 | Batch 125/198 | Loss: 1.9577339887619019\n","Epoch 22/150 | Batch 126/198 | Loss: 1.9561152458190918\n","Epoch 22/150 | Batch 127/198 | Loss: 2.0289852619171143\n","Epoch 22/150 | Batch 128/198 | Loss: 2.2497506141662598\n","Epoch 22/150 | Batch 129/198 | Loss: 2.111713171005249\n","Epoch 22/150 | Batch 130/198 | Loss: 2.3232007026672363\n","Epoch 22/150 | Batch 131/198 | Loss: 2.1567203998565674\n","Epoch 22/150 | Batch 132/198 | Loss: 2.2412073612213135\n","Epoch 22/150 | Batch 133/198 | Loss: 2.0169191360473633\n","Epoch 22/150 | Batch 134/198 | Loss: 2.070619821548462\n","Epoch 22/150 | Batch 135/198 | Loss: 1.9751657247543335\n","Epoch 22/150 | Batch 136/198 | Loss: 1.8739819526672363\n","Epoch 22/150 | Batch 137/198 | Loss: 1.7779462337493896\n","Epoch 22/150 | Batch 138/198 | Loss: 1.9250661134719849\n","Epoch 22/150 | Batch 139/198 | Loss: 2.140854835510254\n","Epoch 22/150 | Batch 140/198 | Loss: 2.417565107345581\n","Epoch 22/150 | Batch 141/198 | Loss: 1.6380102634429932\n","Epoch 22/150 | Batch 142/198 | Loss: 2.0996785163879395\n","Epoch 22/150 | Batch 143/198 | Loss: 1.9811596870422363\n","Epoch 22/150 | Batch 144/198 | Loss: 2.2581212520599365\n","Epoch 22/150 | Batch 145/198 | Loss: 2.1367647647857666\n","Epoch 22/150 | Batch 146/198 | Loss: 2.1856067180633545\n","Epoch 22/150 | Batch 147/198 | Loss: 1.8807011842727661\n","Epoch 22/150 | Batch 148/198 | Loss: 2.1113100051879883\n","Epoch 22/150 | Batch 149/198 | Loss: 1.8712142705917358\n","Epoch 22/150 | Batch 150/198 | Loss: 1.8351010084152222\n","Epoch 22/150 | Batch 151/198 | Loss: 1.9552851915359497\n","Epoch 22/150 | Batch 152/198 | Loss: 2.295762062072754\n","Epoch 22/150 | Batch 153/198 | Loss: 1.9764654636383057\n","Epoch 22/150 | Batch 154/198 | Loss: 2.2428102493286133\n","Epoch 22/150 | Batch 155/198 | Loss: 2.0399928092956543\n","Epoch 22/150 | Batch 156/198 | Loss: 1.9310579299926758\n","Epoch 22/150 | Batch 157/198 | Loss: 2.1194558143615723\n","Epoch 22/150 | Batch 158/198 | Loss: 2.3059043884277344\n","Epoch 22/150 | Batch 159/198 | Loss: 1.9934691190719604\n","Epoch 22/150 | Batch 160/198 | Loss: 2.2854621410369873\n","Epoch 22/150 | Batch 161/198 | Loss: 2.222118854522705\n","Epoch 22/150 | Batch 162/198 | Loss: 2.2551629543304443\n","Epoch 22/150 | Batch 163/198 | Loss: 2.268674373626709\n","Epoch 22/150 | Batch 164/198 | Loss: 1.8203908205032349\n","Epoch 22/150 | Batch 165/198 | Loss: 2.418137311935425\n","Epoch 22/150 | Batch 166/198 | Loss: 1.850464105606079\n","Epoch 22/150 | Batch 167/198 | Loss: 1.9414383172988892\n","Epoch 22/150 | Batch 168/198 | Loss: 1.8906824588775635\n","Epoch 22/150 | Batch 169/198 | Loss: 2.0619733333587646\n","Epoch 22/150 | Batch 170/198 | Loss: 2.079160213470459\n","Epoch 22/150 | Batch 171/198 | Loss: 2.1699421405792236\n","Epoch 22/150 | Batch 172/198 | Loss: 2.1214139461517334\n","Epoch 22/150 | Batch 173/198 | Loss: 1.963747262954712\n","Epoch 22/150 | Batch 174/198 | Loss: 2.1330699920654297\n","Epoch 22/150 | Batch 175/198 | Loss: 2.354779005050659\n","Epoch 22/150 | Batch 176/198 | Loss: 2.18432354927063\n","Epoch 22/150 | Batch 177/198 | Loss: 2.1025564670562744\n","Epoch 22/150 | Batch 178/198 | Loss: 2.2910549640655518\n","Epoch 22/150 | Batch 179/198 | Loss: 2.0559582710266113\n","Epoch 22/150 | Batch 180/198 | Loss: 2.108220338821411\n","Epoch 22/150 | Batch 181/198 | Loss: 1.9866969585418701\n","Epoch 22/150 | Batch 182/198 | Loss: 2.0672130584716797\n","Epoch 22/150 | Batch 183/198 | Loss: 2.225651264190674\n","Epoch 22/150 | Batch 184/198 | Loss: 2.1539812088012695\n","Epoch 22/150 | Batch 185/198 | Loss: 2.2546167373657227\n","Epoch 22/150 | Batch 186/198 | Loss: 2.091294527053833\n","Epoch 22/150 | Batch 187/198 | Loss: 2.2804625034332275\n","Epoch 22/150 | Batch 188/198 | Loss: 2.2379190921783447\n","Epoch 22/150 | Batch 189/198 | Loss: 2.1155502796173096\n","Epoch 22/150 | Batch 190/198 | Loss: 2.2090978622436523\n","Epoch 22/150 | Batch 191/198 | Loss: 1.9859344959259033\n","Epoch 22/150 | Batch 192/198 | Loss: 2.122603178024292\n","Epoch 22/150 | Batch 193/198 | Loss: 1.9936628341674805\n","Epoch 22/150 | Batch 194/198 | Loss: 2.15983510017395\n","Epoch 22/150 | Batch 195/198 | Loss: 2.3483259677886963\n","Epoch 22/150 | Batch 196/198 | Loss: 2.196711540222168\n","Epoch 22/150 | Batch 197/198 | Loss: 1.982195496559143\n","Epoch 22/150 | Batch 198/198 | Loss: 2.4901182651519775\n","Epoch 22/150 | Average Loss: 2.0849118907042223\n","Epoch 23/150 | Batch 1/198 | Loss: 1.9784995317459106\n","Epoch 23/150 | Batch 2/198 | Loss: 1.8768824338912964\n","Epoch 23/150 | Batch 3/198 | Loss: 1.9617092609405518\n","Epoch 23/150 | Batch 4/198 | Loss: 1.7084338665008545\n","Epoch 23/150 | Batch 5/198 | Loss: 1.8859632015228271\n","Epoch 23/150 | Batch 6/198 | Loss: 1.9840601682662964\n","Epoch 23/150 | Batch 7/198 | Loss: 1.9277275800704956\n","Epoch 23/150 | Batch 8/198 | Loss: 1.8306292295455933\n","Epoch 23/150 | Batch 9/198 | Loss: 1.653325080871582\n","Epoch 23/150 | Batch 10/198 | Loss: 2.078178882598877\n","Epoch 23/150 | Batch 11/198 | Loss: 1.9307063817977905\n","Epoch 23/150 | Batch 12/198 | Loss: 2.031007766723633\n","Epoch 23/150 | Batch 13/198 | Loss: 1.9206238985061646\n","Epoch 23/150 | Batch 14/198 | Loss: 2.220653772354126\n","Epoch 23/150 | Batch 15/198 | Loss: 1.9729913473129272\n","Epoch 23/150 | Batch 16/198 | Loss: 2.0128543376922607\n","Epoch 23/150 | Batch 17/198 | Loss: 2.14638614654541\n","Epoch 23/150 | Batch 18/198 | Loss: 1.402221441268921\n","Epoch 23/150 | Batch 19/198 | Loss: 1.8023009300231934\n","Epoch 23/150 | Batch 20/198 | Loss: 1.9759740829467773\n","Epoch 23/150 | Batch 21/198 | Loss: 1.9921154975891113\n","Epoch 23/150 | Batch 22/198 | Loss: 1.9812434911727905\n","Epoch 23/150 | Batch 23/198 | Loss: 1.912345290184021\n","Epoch 23/150 | Batch 24/198 | Loss: 1.8175170421600342\n","Epoch 23/150 | Batch 25/198 | Loss: 2.2501590251922607\n","Epoch 23/150 | Batch 26/198 | Loss: 2.0522987842559814\n","Epoch 23/150 | Batch 27/198 | Loss: 1.9257256984710693\n","Epoch 23/150 | Batch 28/198 | Loss: 1.762940764427185\n","Epoch 23/150 | Batch 29/198 | Loss: 1.970179796218872\n","Epoch 23/150 | Batch 30/198 | Loss: 1.9520964622497559\n","Epoch 23/150 | Batch 31/198 | Loss: 2.008312463760376\n","Epoch 23/150 | Batch 32/198 | Loss: 2.097459554672241\n","Epoch 23/150 | Batch 33/198 | Loss: 2.073963165283203\n","Epoch 23/150 | Batch 34/198 | Loss: 1.8634939193725586\n","Epoch 23/150 | Batch 35/198 | Loss: 2.015923261642456\n","Epoch 23/150 | Batch 36/198 | Loss: 1.822198748588562\n","Epoch 23/150 | Batch 37/198 | Loss: 2.0242724418640137\n","Epoch 23/150 | Batch 38/198 | Loss: 2.0499980449676514\n","Epoch 23/150 | Batch 39/198 | Loss: 1.898566722869873\n","Epoch 23/150 | Batch 40/198 | Loss: 1.9653284549713135\n","Epoch 23/150 | Batch 41/198 | Loss: 2.1824941635131836\n","Epoch 23/150 | Batch 42/198 | Loss: 2.220736026763916\n","Epoch 23/150 | Batch 43/198 | Loss: 1.7927050590515137\n","Epoch 23/150 | Batch 44/198 | Loss: 2.079017162322998\n","Epoch 23/150 | Batch 45/198 | Loss: 2.1941518783569336\n","Epoch 23/150 | Batch 46/198 | Loss: 2.102957248687744\n","Epoch 23/150 | Batch 47/198 | Loss: 2.0513367652893066\n","Epoch 23/150 | Batch 48/198 | Loss: 2.0134358406066895\n","Epoch 23/150 | Batch 49/198 | Loss: 1.9086300134658813\n","Epoch 23/150 | Batch 50/198 | Loss: 1.8637826442718506\n","Epoch 23/150 | Batch 51/198 | Loss: 2.1322762966156006\n","Epoch 23/150 | Batch 52/198 | Loss: 1.957793951034546\n","Epoch 23/150 | Batch 53/198 | Loss: 2.0527451038360596\n","Epoch 23/150 | Batch 54/198 | Loss: 1.9142426252365112\n","Epoch 23/150 | Batch 55/198 | Loss: 1.8346322774887085\n","Epoch 23/150 | Batch 56/198 | Loss: 2.0653114318847656\n","Epoch 23/150 | Batch 57/198 | Loss: 2.062623977661133\n","Epoch 23/150 | Batch 58/198 | Loss: 2.2399168014526367\n","Epoch 23/150 | Batch 59/198 | Loss: 2.0600438117980957\n","Epoch 23/150 | Batch 60/198 | Loss: 1.9148237705230713\n","Epoch 23/150 | Batch 61/198 | Loss: 2.160038709640503\n","Epoch 23/150 | Batch 62/198 | Loss: 1.8470194339752197\n","Epoch 23/150 | Batch 63/198 | Loss: 2.0778796672821045\n","Epoch 23/150 | Batch 64/198 | Loss: 1.79240882396698\n","Epoch 23/150 | Batch 65/198 | Loss: 1.9669259786605835\n","Epoch 23/150 | Batch 66/198 | Loss: 2.082280158996582\n","Epoch 23/150 | Batch 67/198 | Loss: 1.9660975933074951\n","Epoch 23/150 | Batch 68/198 | Loss: 2.1683671474456787\n","Epoch 23/150 | Batch 69/198 | Loss: 2.2519915103912354\n","Epoch 23/150 | Batch 70/198 | Loss: 2.1847164630889893\n","Epoch 23/150 | Batch 71/198 | Loss: 1.936118245124817\n","Epoch 23/150 | Batch 72/198 | Loss: 2.0208184719085693\n","Epoch 23/150 | Batch 73/198 | Loss: 1.9247918128967285\n","Epoch 23/150 | Batch 74/198 | Loss: 1.8453763723373413\n","Epoch 23/150 | Batch 75/198 | Loss: 2.2317516803741455\n","Epoch 23/150 | Batch 76/198 | Loss: 2.1096386909484863\n","Epoch 23/150 | Batch 77/198 | Loss: 1.750699520111084\n","Epoch 23/150 | Batch 78/198 | Loss: 2.208566904067993\n","Epoch 23/150 | Batch 79/198 | Loss: 1.7984402179718018\n","Epoch 23/150 | Batch 80/198 | Loss: 2.052114486694336\n","Epoch 23/150 | Batch 81/198 | Loss: 2.001805543899536\n","Epoch 23/150 | Batch 82/198 | Loss: 1.7367697954177856\n","Epoch 23/150 | Batch 83/198 | Loss: 2.1182820796966553\n","Epoch 23/150 | Batch 84/198 | Loss: 2.200617551803589\n","Epoch 23/150 | Batch 85/198 | Loss: 1.847574234008789\n","Epoch 23/150 | Batch 86/198 | Loss: 2.1474905014038086\n","Epoch 23/150 | Batch 87/198 | Loss: 2.0105721950531006\n","Epoch 23/150 | Batch 88/198 | Loss: 1.8889788389205933\n","Epoch 23/150 | Batch 89/198 | Loss: 2.0387704372406006\n","Epoch 23/150 | Batch 90/198 | Loss: 2.0231189727783203\n","Epoch 23/150 | Batch 91/198 | Loss: 1.9964807033538818\n","Epoch 23/150 | Batch 92/198 | Loss: 2.2046167850494385\n","Epoch 23/150 | Batch 93/198 | Loss: 2.229203939437866\n","Epoch 23/150 | Batch 94/198 | Loss: 2.1252362728118896\n","Epoch 23/150 | Batch 95/198 | Loss: 1.734066128730774\n","Epoch 23/150 | Batch 96/198 | Loss: 2.123032331466675\n","Epoch 23/150 | Batch 97/198 | Loss: 1.9250890016555786\n","Epoch 23/150 | Batch 98/198 | Loss: 1.8799223899841309\n","Epoch 23/150 | Batch 99/198 | Loss: 2.0368785858154297\n","Epoch 23/150 | Batch 100/198 | Loss: 2.017047643661499\n","Epoch 23/150 | Batch 101/198 | Loss: 2.0387516021728516\n","Epoch 23/150 | Batch 102/198 | Loss: 2.0076942443847656\n","Epoch 23/150 | Batch 103/198 | Loss: 1.9153177738189697\n","Epoch 23/150 | Batch 104/198 | Loss: 2.116877317428589\n","Epoch 23/150 | Batch 105/198 | Loss: 2.0399580001831055\n","Epoch 23/150 | Batch 106/198 | Loss: 1.9394985437393188\n","Epoch 23/150 | Batch 107/198 | Loss: 2.1742093563079834\n","Epoch 23/150 | Batch 108/198 | Loss: 1.8808046579360962\n","Epoch 23/150 | Batch 109/198 | Loss: 2.1719143390655518\n","Epoch 23/150 | Batch 110/198 | Loss: 2.0224366188049316\n","Epoch 23/150 | Batch 111/198 | Loss: 2.0473568439483643\n","Epoch 23/150 | Batch 112/198 | Loss: 2.228588342666626\n","Epoch 23/150 | Batch 113/198 | Loss: 2.220909595489502\n","Epoch 23/150 | Batch 114/198 | Loss: 2.1081595420837402\n","Epoch 23/150 | Batch 115/198 | Loss: 2.128936767578125\n","Epoch 23/150 | Batch 116/198 | Loss: 1.9266576766967773\n","Epoch 23/150 | Batch 117/198 | Loss: 2.1536943912506104\n","Epoch 23/150 | Batch 118/198 | Loss: 2.0745882987976074\n","Epoch 23/150 | Batch 119/198 | Loss: 1.9666454792022705\n","Epoch 23/150 | Batch 120/198 | Loss: 2.1501271724700928\n","Epoch 23/150 | Batch 121/198 | Loss: 2.081904172897339\n","Epoch 23/150 | Batch 122/198 | Loss: 2.0171902179718018\n","Epoch 23/150 | Batch 123/198 | Loss: 1.9162111282348633\n","Epoch 23/150 | Batch 124/198 | Loss: 2.0248591899871826\n","Epoch 23/150 | Batch 125/198 | Loss: 2.2235047817230225\n","Epoch 23/150 | Batch 126/198 | Loss: 2.206151008605957\n","Epoch 23/150 | Batch 127/198 | Loss: 1.9038209915161133\n","Epoch 23/150 | Batch 128/198 | Loss: 2.1248204708099365\n","Epoch 23/150 | Batch 129/198 | Loss: 2.180392265319824\n","Epoch 23/150 | Batch 130/198 | Loss: 1.837902307510376\n","Epoch 23/150 | Batch 131/198 | Loss: 1.9568780660629272\n","Epoch 23/150 | Batch 132/198 | Loss: 2.2078919410705566\n","Epoch 23/150 | Batch 133/198 | Loss: 1.9843835830688477\n","Epoch 23/150 | Batch 134/198 | Loss: 2.1225056648254395\n","Epoch 23/150 | Batch 135/198 | Loss: 1.9962064027786255\n","Epoch 23/150 | Batch 136/198 | Loss: 1.8062810897827148\n","Epoch 23/150 | Batch 137/198 | Loss: 2.1764769554138184\n","Epoch 23/150 | Batch 138/198 | Loss: 1.7253695726394653\n","Epoch 23/150 | Batch 139/198 | Loss: 2.287950277328491\n","Epoch 23/150 | Batch 140/198 | Loss: 2.1730995178222656\n","Epoch 23/150 | Batch 141/198 | Loss: 2.0836997032165527\n","Epoch 23/150 | Batch 142/198 | Loss: 1.9225184917449951\n","Epoch 23/150 | Batch 143/198 | Loss: 1.9407851696014404\n","Epoch 23/150 | Batch 144/198 | Loss: 1.89349365234375\n","Epoch 23/150 | Batch 145/198 | Loss: 2.1994543075561523\n","Epoch 23/150 | Batch 146/198 | Loss: 2.0853495597839355\n","Epoch 23/150 | Batch 147/198 | Loss: 2.0956437587738037\n","Epoch 23/150 | Batch 148/198 | Loss: 2.2232518196105957\n","Epoch 23/150 | Batch 149/198 | Loss: 1.8523577451705933\n","Epoch 23/150 | Batch 150/198 | Loss: 1.774113655090332\n","Epoch 23/150 | Batch 151/198 | Loss: 1.9215000867843628\n","Epoch 23/150 | Batch 152/198 | Loss: 2.058806896209717\n","Epoch 23/150 | Batch 153/198 | Loss: 2.1087090969085693\n","Epoch 23/150 | Batch 154/198 | Loss: 2.007387399673462\n","Epoch 23/150 | Batch 155/198 | Loss: 2.014547824859619\n","Epoch 23/150 | Batch 156/198 | Loss: 2.3103456497192383\n","Epoch 23/150 | Batch 157/198 | Loss: 2.218639373779297\n","Epoch 23/150 | Batch 158/198 | Loss: 1.893086314201355\n","Epoch 23/150 | Batch 159/198 | Loss: 1.8770471811294556\n","Epoch 23/150 | Batch 160/198 | Loss: 1.8132277727127075\n","Epoch 23/150 | Batch 161/198 | Loss: 1.9373947381973267\n","Epoch 23/150 | Batch 162/198 | Loss: 2.0779500007629395\n","Epoch 23/150 | Batch 163/198 | Loss: 1.990504264831543\n","Epoch 23/150 | Batch 164/198 | Loss: 2.110419988632202\n","Epoch 23/150 | Batch 165/198 | Loss: 1.8092067241668701\n","Epoch 23/150 | Batch 166/198 | Loss: 2.0257623195648193\n","Epoch 23/150 | Batch 167/198 | Loss: 1.816499948501587\n","Epoch 23/150 | Batch 168/198 | Loss: 2.3767354488372803\n","Epoch 23/150 | Batch 169/198 | Loss: 2.3045005798339844\n","Epoch 23/150 | Batch 170/198 | Loss: 1.7511067390441895\n","Epoch 23/150 | Batch 171/198 | Loss: 1.9111627340316772\n","Epoch 23/150 | Batch 172/198 | Loss: 2.351569890975952\n","Epoch 23/150 | Batch 173/198 | Loss: 2.1117894649505615\n","Epoch 23/150 | Batch 174/198 | Loss: 2.1707305908203125\n","Epoch 23/150 | Batch 175/198 | Loss: 2.1220388412475586\n","Epoch 23/150 | Batch 176/198 | Loss: 2.0066936016082764\n","Epoch 23/150 | Batch 177/198 | Loss: 1.817525863647461\n","Epoch 23/150 | Batch 178/198 | Loss: 1.9572274684906006\n","Epoch 23/150 | Batch 179/198 | Loss: 2.3231260776519775\n","Epoch 23/150 | Batch 180/198 | Loss: 2.0182652473449707\n","Epoch 23/150 | Batch 181/198 | Loss: 1.842724323272705\n","Epoch 23/150 | Batch 182/198 | Loss: 2.144418478012085\n","Epoch 23/150 | Batch 183/198 | Loss: 2.1237094402313232\n","Epoch 23/150 | Batch 184/198 | Loss: 2.0724925994873047\n","Epoch 23/150 | Batch 185/198 | Loss: 2.1648967266082764\n","Epoch 23/150 | Batch 186/198 | Loss: 2.1899871826171875\n","Epoch 23/150 | Batch 187/198 | Loss: 2.1329383850097656\n","Epoch 23/150 | Batch 188/198 | Loss: 2.0430514812469482\n","Epoch 23/150 | Batch 189/198 | Loss: 2.014240026473999\n","Epoch 23/150 | Batch 190/198 | Loss: 2.0881898403167725\n","Epoch 23/150 | Batch 191/198 | Loss: 1.912924885749817\n","Epoch 23/150 | Batch 192/198 | Loss: 2.2059948444366455\n","Epoch 23/150 | Batch 193/198 | Loss: 2.192716360092163\n","Epoch 23/150 | Batch 194/198 | Loss: 2.0373058319091797\n","Epoch 23/150 | Batch 195/198 | Loss: 1.8612178564071655\n","Epoch 23/150 | Batch 196/198 | Loss: 2.013183832168579\n","Epoch 23/150 | Batch 197/198 | Loss: 1.5958635807037354\n","Epoch 23/150 | Batch 198/198 | Loss: 2.028367519378662\n","Epoch 23/150 | Average Loss: 2.016453767665709\n","Epoch 24/150 | Batch 1/198 | Loss: 1.7209833860397339\n","Epoch 24/150 | Batch 2/198 | Loss: 2.0205018520355225\n","Epoch 24/150 | Batch 3/198 | Loss: 2.083500862121582\n","Epoch 24/150 | Batch 4/198 | Loss: 1.8514975309371948\n","Epoch 24/150 | Batch 5/198 | Loss: 2.219161033630371\n","Epoch 24/150 | Batch 6/198 | Loss: 1.933105707168579\n","Epoch 24/150 | Batch 7/198 | Loss: 1.9103796482086182\n","Epoch 24/150 | Batch 8/198 | Loss: 2.050180435180664\n","Epoch 24/150 | Batch 9/198 | Loss: 2.006099224090576\n","Epoch 24/150 | Batch 10/198 | Loss: 1.996826171875\n","Epoch 24/150 | Batch 11/198 | Loss: 1.8892748355865479\n","Epoch 24/150 | Batch 12/198 | Loss: 1.7011831998825073\n","Epoch 24/150 | Batch 13/198 | Loss: 2.1129372119903564\n","Epoch 24/150 | Batch 14/198 | Loss: 1.8795088529586792\n","Epoch 24/150 | Batch 15/198 | Loss: 1.8998392820358276\n","Epoch 24/150 | Batch 16/198 | Loss: 1.8765628337860107\n","Epoch 24/150 | Batch 17/198 | Loss: 1.971684217453003\n","Epoch 24/150 | Batch 18/198 | Loss: 1.976280689239502\n","Epoch 24/150 | Batch 19/198 | Loss: 2.192194938659668\n","Epoch 24/150 | Batch 20/198 | Loss: 1.8632858991622925\n","Epoch 24/150 | Batch 21/198 | Loss: 1.975880742073059\n","Epoch 24/150 | Batch 22/198 | Loss: 2.0420918464660645\n","Epoch 24/150 | Batch 23/198 | Loss: 2.0886075496673584\n","Epoch 24/150 | Batch 24/198 | Loss: 1.5926990509033203\n","Epoch 24/150 | Batch 25/198 | Loss: 1.72173011302948\n","Epoch 24/150 | Batch 26/198 | Loss: 1.8328078985214233\n","Epoch 24/150 | Batch 27/198 | Loss: 2.009704351425171\n","Epoch 24/150 | Batch 28/198 | Loss: 2.003355026245117\n","Epoch 24/150 | Batch 29/198 | Loss: 1.9375379085540771\n","Epoch 24/150 | Batch 30/198 | Loss: 2.186889886856079\n","Epoch 24/150 | Batch 31/198 | Loss: 1.9611685276031494\n","Epoch 24/150 | Batch 32/198 | Loss: 1.7721302509307861\n","Epoch 24/150 | Batch 33/198 | Loss: 2.0602760314941406\n","Epoch 24/150 | Batch 34/198 | Loss: 1.7873705625534058\n","Epoch 24/150 | Batch 35/198 | Loss: 1.737620234489441\n","Epoch 24/150 | Batch 36/198 | Loss: 1.9577467441558838\n","Epoch 24/150 | Batch 37/198 | Loss: 2.216843366622925\n","Epoch 24/150 | Batch 38/198 | Loss: 2.0148913860321045\n","Epoch 24/150 | Batch 39/198 | Loss: 2.1043038368225098\n","Epoch 24/150 | Batch 40/198 | Loss: 1.844273328781128\n","Epoch 24/150 | Batch 41/198 | Loss: 1.739717721939087\n","Epoch 24/150 | Batch 42/198 | Loss: 2.123486042022705\n","Epoch 24/150 | Batch 43/198 | Loss: 1.8847754001617432\n","Epoch 24/150 | Batch 44/198 | Loss: 1.9165570735931396\n","Epoch 24/150 | Batch 45/198 | Loss: 2.1705780029296875\n","Epoch 24/150 | Batch 46/198 | Loss: 2.007474184036255\n","Epoch 24/150 | Batch 47/198 | Loss: 2.0416812896728516\n","Epoch 24/150 | Batch 48/198 | Loss: 1.6601418256759644\n","Epoch 24/150 | Batch 49/198 | Loss: 2.053572416305542\n","Epoch 24/150 | Batch 50/198 | Loss: 2.2852487564086914\n","Epoch 24/150 | Batch 51/198 | Loss: 1.771315336227417\n","Epoch 24/150 | Batch 52/198 | Loss: 2.2694222927093506\n","Epoch 24/150 | Batch 53/198 | Loss: 2.07745099067688\n","Epoch 24/150 | Batch 54/198 | Loss: 2.0378944873809814\n","Epoch 24/150 | Batch 55/198 | Loss: 1.695550799369812\n","Epoch 24/150 | Batch 56/198 | Loss: 1.9502661228179932\n","Epoch 24/150 | Batch 57/198 | Loss: 1.9949114322662354\n","Epoch 24/150 | Batch 58/198 | Loss: 1.874248743057251\n","Epoch 24/150 | Batch 59/198 | Loss: 2.0842981338500977\n","Epoch 24/150 | Batch 60/198 | Loss: 1.9534577131271362\n","Epoch 24/150 | Batch 61/198 | Loss: 1.8497647047042847\n","Epoch 24/150 | Batch 62/198 | Loss: 2.033499002456665\n","Epoch 24/150 | Batch 63/198 | Loss: 1.8178958892822266\n","Epoch 24/150 | Batch 64/198 | Loss: 2.0677359104156494\n","Epoch 24/150 | Batch 65/198 | Loss: 1.8047412633895874\n","Epoch 24/150 | Batch 66/198 | Loss: 1.9501434564590454\n","Epoch 24/150 | Batch 67/198 | Loss: 2.2628772258758545\n","Epoch 24/150 | Batch 68/198 | Loss: 1.8123366832733154\n","Epoch 24/150 | Batch 69/198 | Loss: 2.042847156524658\n","Epoch 24/150 | Batch 70/198 | Loss: 2.1103250980377197\n","Epoch 24/150 | Batch 71/198 | Loss: 2.1930596828460693\n","Epoch 24/150 | Batch 72/198 | Loss: 2.1001029014587402\n","Epoch 24/150 | Batch 73/198 | Loss: 1.8782094717025757\n","Epoch 24/150 | Batch 74/198 | Loss: 1.9869548082351685\n","Epoch 24/150 | Batch 75/198 | Loss: 1.8162521123886108\n","Epoch 24/150 | Batch 76/198 | Loss: 1.8160219192504883\n","Epoch 24/150 | Batch 77/198 | Loss: 1.8793543577194214\n","Epoch 24/150 | Batch 78/198 | Loss: 1.9641458988189697\n","Epoch 24/150 | Batch 79/198 | Loss: 2.010793924331665\n","Epoch 24/150 | Batch 80/198 | Loss: 1.8295607566833496\n","Epoch 24/150 | Batch 81/198 | Loss: 1.870048999786377\n","Epoch 24/150 | Batch 82/198 | Loss: 1.8149867057800293\n","Epoch 24/150 | Batch 83/198 | Loss: 1.6525064706802368\n","Epoch 24/150 | Batch 84/198 | Loss: 1.8998509645462036\n","Epoch 24/150 | Batch 85/198 | Loss: 1.781916618347168\n","Epoch 24/150 | Batch 86/198 | Loss: 1.6857808828353882\n","Epoch 24/150 | Batch 87/198 | Loss: 2.0229811668395996\n","Epoch 24/150 | Batch 88/198 | Loss: 1.7353285551071167\n","Epoch 24/150 | Batch 89/198 | Loss: 1.9105509519577026\n","Epoch 24/150 | Batch 90/198 | Loss: 1.9064385890960693\n","Epoch 24/150 | Batch 91/198 | Loss: 2.1053361892700195\n","Epoch 24/150 | Batch 92/198 | Loss: 2.0827293395996094\n","Epoch 24/150 | Batch 93/198 | Loss: 1.8316395282745361\n","Epoch 24/150 | Batch 94/198 | Loss: 2.2258477210998535\n","Epoch 24/150 | Batch 95/198 | Loss: 2.1066761016845703\n","Epoch 24/150 | Batch 96/198 | Loss: 1.9140623807907104\n","Epoch 24/150 | Batch 97/198 | Loss: 1.9242409467697144\n","Epoch 24/150 | Batch 98/198 | Loss: 1.73326575756073\n","Epoch 24/150 | Batch 99/198 | Loss: 2.1874046325683594\n","Epoch 24/150 | Batch 100/198 | Loss: 1.933909296989441\n","Epoch 24/150 | Batch 101/198 | Loss: 1.7155969142913818\n","Epoch 24/150 | Batch 102/198 | Loss: 1.9630298614501953\n","Epoch 24/150 | Batch 103/198 | Loss: 1.981308937072754\n","Epoch 24/150 | Batch 104/198 | Loss: 2.0998599529266357\n","Epoch 24/150 | Batch 105/198 | Loss: 1.8285781145095825\n","Epoch 24/150 | Batch 106/198 | Loss: 1.7705456018447876\n","Epoch 24/150 | Batch 107/198 | Loss: 1.954870343208313\n","Epoch 24/150 | Batch 108/198 | Loss: 2.008733034133911\n","Epoch 24/150 | Batch 109/198 | Loss: 1.8725932836532593\n","Epoch 24/150 | Batch 110/198 | Loss: 2.0032739639282227\n","Epoch 24/150 | Batch 111/198 | Loss: 1.8268038034439087\n","Epoch 24/150 | Batch 112/198 | Loss: 2.1722826957702637\n","Epoch 24/150 | Batch 113/198 | Loss: 2.017416000366211\n","Epoch 24/150 | Batch 114/198 | Loss: 2.2239115238189697\n","Epoch 24/150 | Batch 115/198 | Loss: 1.8046536445617676\n","Epoch 24/150 | Batch 116/198 | Loss: 1.9446942806243896\n","Epoch 24/150 | Batch 117/198 | Loss: 1.8049488067626953\n","Epoch 24/150 | Batch 118/198 | Loss: 1.866918921470642\n","Epoch 24/150 | Batch 119/198 | Loss: 1.995412826538086\n","Epoch 24/150 | Batch 120/198 | Loss: 1.7654486894607544\n","Epoch 24/150 | Batch 121/198 | Loss: 1.7945551872253418\n","Epoch 24/150 | Batch 122/198 | Loss: 1.6064122915267944\n","Epoch 24/150 | Batch 123/198 | Loss: 2.0476503372192383\n","Epoch 24/150 | Batch 124/198 | Loss: 2.0230894088745117\n","Epoch 24/150 | Batch 125/198 | Loss: 2.050022602081299\n","Epoch 24/150 | Batch 126/198 | Loss: 2.043281316757202\n","Epoch 24/150 | Batch 127/198 | Loss: 2.0548102855682373\n","Epoch 24/150 | Batch 128/198 | Loss: 2.0259759426116943\n","Epoch 24/150 | Batch 129/198 | Loss: 1.9449777603149414\n","Epoch 24/150 | Batch 130/198 | Loss: 2.088073253631592\n","Epoch 24/150 | Batch 131/198 | Loss: 2.039400339126587\n","Epoch 24/150 | Batch 132/198 | Loss: 2.1542630195617676\n","Epoch 24/150 | Batch 133/198 | Loss: 1.8583921194076538\n","Epoch 24/150 | Batch 134/198 | Loss: 2.111931562423706\n","Epoch 24/150 | Batch 135/198 | Loss: 1.8528414964675903\n","Epoch 24/150 | Batch 136/198 | Loss: 1.842369556427002\n","Epoch 24/150 | Batch 137/198 | Loss: 2.0851681232452393\n","Epoch 24/150 | Batch 138/198 | Loss: 1.7331762313842773\n","Epoch 24/150 | Batch 139/198 | Loss: 1.6638247966766357\n","Epoch 24/150 | Batch 140/198 | Loss: 1.7929524183273315\n","Epoch 24/150 | Batch 141/198 | Loss: 1.9632847309112549\n","Epoch 24/150 | Batch 142/198 | Loss: 1.6106516122817993\n","Epoch 24/150 | Batch 143/198 | Loss: 2.011415958404541\n","Epoch 24/150 | Batch 144/198 | Loss: 1.9048327207565308\n","Epoch 24/150 | Batch 145/198 | Loss: 1.6572877168655396\n","Epoch 24/150 | Batch 146/198 | Loss: 2.0338053703308105\n","Epoch 24/150 | Batch 147/198 | Loss: 1.787211298942566\n","Epoch 24/150 | Batch 148/198 | Loss: 2.117576837539673\n","Epoch 24/150 | Batch 149/198 | Loss: 1.916922688484192\n","Epoch 24/150 | Batch 150/198 | Loss: 1.8295247554779053\n","Epoch 24/150 | Batch 151/198 | Loss: 1.9217990636825562\n","Epoch 24/150 | Batch 152/198 | Loss: 2.01781964302063\n","Epoch 24/150 | Batch 153/198 | Loss: 2.1517019271850586\n","Epoch 24/150 | Batch 154/198 | Loss: 1.942177176475525\n","Epoch 24/150 | Batch 155/198 | Loss: 1.8796905279159546\n","Epoch 24/150 | Batch 156/198 | Loss: 2.0784389972686768\n","Epoch 24/150 | Batch 157/198 | Loss: 2.002650737762451\n","Epoch 24/150 | Batch 158/198 | Loss: 2.1729700565338135\n","Epoch 24/150 | Batch 159/198 | Loss: 2.082491397857666\n","Epoch 24/150 | Batch 160/198 | Loss: 2.199589729309082\n","Epoch 24/150 | Batch 161/198 | Loss: 2.1037352085113525\n","Epoch 24/150 | Batch 162/198 | Loss: 2.002809524536133\n","Epoch 24/150 | Batch 163/198 | Loss: 1.9885876178741455\n","Epoch 24/150 | Batch 164/198 | Loss: 1.7384202480316162\n","Epoch 24/150 | Batch 165/198 | Loss: 1.778183102607727\n","Epoch 24/150 | Batch 166/198 | Loss: 2.076486349105835\n","Epoch 24/150 | Batch 167/198 | Loss: 2.30916166305542\n","Epoch 24/150 | Batch 168/198 | Loss: 1.9802751541137695\n","Epoch 24/150 | Batch 169/198 | Loss: 1.979899525642395\n","Epoch 24/150 | Batch 170/198 | Loss: 2.277164936065674\n","Epoch 24/150 | Batch 171/198 | Loss: 1.9034779071807861\n","Epoch 24/150 | Batch 172/198 | Loss: 2.171327590942383\n","Epoch 24/150 | Batch 173/198 | Loss: 1.9411500692367554\n","Epoch 24/150 | Batch 174/198 | Loss: 1.9990869760513306\n","Epoch 24/150 | Batch 175/198 | Loss: 1.6362203359603882\n","Epoch 24/150 | Batch 176/198 | Loss: 2.1323225498199463\n","Epoch 24/150 | Batch 177/198 | Loss: 2.214261293411255\n","Epoch 24/150 | Batch 178/198 | Loss: 1.7472649812698364\n","Epoch 24/150 | Batch 179/198 | Loss: 1.8383373022079468\n","Epoch 24/150 | Batch 180/198 | Loss: 1.8137328624725342\n","Epoch 24/150 | Batch 181/198 | Loss: 2.2129647731781006\n","Epoch 24/150 | Batch 182/198 | Loss: 2.1671230792999268\n","Epoch 24/150 | Batch 183/198 | Loss: 2.0993998050689697\n","Epoch 24/150 | Batch 184/198 | Loss: 2.0831384658813477\n","Epoch 24/150 | Batch 185/198 | Loss: 1.792670726776123\n","Epoch 24/150 | Batch 186/198 | Loss: 1.9654914140701294\n","Epoch 24/150 | Batch 187/198 | Loss: 1.5754512548446655\n","Epoch 24/150 | Batch 188/198 | Loss: 2.0919439792633057\n","Epoch 24/150 | Batch 189/198 | Loss: 1.8812215328216553\n","Epoch 24/150 | Batch 190/198 | Loss: 2.03981351852417\n","Epoch 24/150 | Batch 191/198 | Loss: 2.167760133743286\n","Epoch 24/150 | Batch 192/198 | Loss: 1.8767849206924438\n","Epoch 24/150 | Batch 193/198 | Loss: 1.957180142402649\n","Epoch 24/150 | Batch 194/198 | Loss: 2.1560659408569336\n","Epoch 24/150 | Batch 195/198 | Loss: 2.0756709575653076\n","Epoch 24/150 | Batch 196/198 | Loss: 1.887853980064392\n","Epoch 24/150 | Batch 197/198 | Loss: 2.0583486557006836\n","Epoch 24/150 | Batch 198/198 | Loss: 1.8527804613113403\n","Epoch 24/150 | Average Loss: 1.956750254438381\n","Epoch 25/150 | Batch 1/198 | Loss: 1.5789859294891357\n","Epoch 25/150 | Batch 2/198 | Loss: 1.966597080230713\n","Epoch 25/150 | Batch 3/198 | Loss: 1.935805320739746\n","Epoch 25/150 | Batch 4/198 | Loss: 1.932496190071106\n","Epoch 25/150 | Batch 5/198 | Loss: 2.1039745807647705\n","Epoch 25/150 | Batch 6/198 | Loss: 1.8526992797851562\n","Epoch 25/150 | Batch 7/198 | Loss: 1.9323129653930664\n","Epoch 25/150 | Batch 8/198 | Loss: 2.196779251098633\n","Epoch 25/150 | Batch 9/198 | Loss: 2.095362901687622\n","Epoch 25/150 | Batch 10/198 | Loss: 1.932713270187378\n","Epoch 25/150 | Batch 11/198 | Loss: 1.966024398803711\n","Epoch 25/150 | Batch 12/198 | Loss: 1.90909743309021\n","Epoch 25/150 | Batch 13/198 | Loss: 1.9493852853775024\n","Epoch 25/150 | Batch 14/198 | Loss: 2.048809289932251\n","Epoch 25/150 | Batch 15/198 | Loss: 1.9182292222976685\n","Epoch 25/150 | Batch 16/198 | Loss: 2.356154203414917\n","Epoch 25/150 | Batch 17/198 | Loss: 1.9159069061279297\n","Epoch 25/150 | Batch 18/198 | Loss: 1.4420615434646606\n","Epoch 25/150 | Batch 19/198 | Loss: 1.8101348876953125\n","Epoch 25/150 | Batch 20/198 | Loss: 2.041840076446533\n","Epoch 25/150 | Batch 21/198 | Loss: 2.1358163356781006\n","Epoch 25/150 | Batch 22/198 | Loss: 2.0377674102783203\n","Epoch 25/150 | Batch 23/198 | Loss: 1.5796290636062622\n","Epoch 25/150 | Batch 24/198 | Loss: 1.8874958753585815\n","Epoch 25/150 | Batch 25/198 | Loss: 2.1394786834716797\n","Epoch 25/150 | Batch 26/198 | Loss: 2.102604866027832\n","Epoch 25/150 | Batch 27/198 | Loss: 1.7438225746154785\n","Epoch 25/150 | Batch 28/198 | Loss: 2.2281339168548584\n","Epoch 25/150 | Batch 29/198 | Loss: 1.7845840454101562\n","Epoch 25/150 | Batch 30/198 | Loss: 1.6763073205947876\n","Epoch 25/150 | Batch 31/198 | Loss: 1.7354884147644043\n","Epoch 25/150 | Batch 32/198 | Loss: 1.867857575416565\n","Epoch 25/150 | Batch 33/198 | Loss: 1.84949791431427\n","Epoch 25/150 | Batch 34/198 | Loss: 2.0387203693389893\n","Epoch 25/150 | Batch 35/198 | Loss: 2.2950239181518555\n","Epoch 25/150 | Batch 36/198 | Loss: 2.1986093521118164\n","Epoch 25/150 | Batch 37/198 | Loss: 1.943293571472168\n","Epoch 25/150 | Batch 38/198 | Loss: 1.7653406858444214\n","Epoch 25/150 | Batch 39/198 | Loss: 1.8697116374969482\n","Epoch 25/150 | Batch 40/198 | Loss: 1.942649006843567\n","Epoch 25/150 | Batch 41/198 | Loss: 1.839525818824768\n","Epoch 25/150 | Batch 42/198 | Loss: 1.9755886793136597\n","Epoch 25/150 | Batch 43/198 | Loss: 1.9909484386444092\n","Epoch 25/150 | Batch 44/198 | Loss: 1.575003981590271\n","Epoch 25/150 | Batch 45/198 | Loss: 1.7898950576782227\n","Epoch 25/150 | Batch 46/198 | Loss: 2.020009994506836\n","Epoch 25/150 | Batch 47/198 | Loss: 1.9144580364227295\n","Epoch 25/150 | Batch 48/198 | Loss: 1.6430860757827759\n","Epoch 25/150 | Batch 49/198 | Loss: 1.88822603225708\n","Epoch 25/150 | Batch 50/198 | Loss: 1.9731619358062744\n","Epoch 25/150 | Batch 51/198 | Loss: 1.968624234199524\n","Epoch 25/150 | Batch 52/198 | Loss: 1.7701873779296875\n","Epoch 25/150 | Batch 53/198 | Loss: 1.805819034576416\n","Epoch 25/150 | Batch 54/198 | Loss: 1.889127492904663\n","Epoch 25/150 | Batch 55/198 | Loss: 1.7784377336502075\n","Epoch 25/150 | Batch 56/198 | Loss: 1.8833762407302856\n","Epoch 25/150 | Batch 57/198 | Loss: 1.9364396333694458\n","Epoch 25/150 | Batch 58/198 | Loss: 1.9159680604934692\n","Epoch 25/150 | Batch 59/198 | Loss: 1.9978771209716797\n","Epoch 25/150 | Batch 60/198 | Loss: 1.893341064453125\n","Epoch 25/150 | Batch 61/198 | Loss: 1.905230164527893\n","Epoch 25/150 | Batch 62/198 | Loss: 1.9366672039031982\n","Epoch 25/150 | Batch 63/198 | Loss: 1.9824142456054688\n","Epoch 25/150 | Batch 64/198 | Loss: 1.8704828023910522\n","Epoch 25/150 | Batch 65/198 | Loss: 1.7483487129211426\n","Epoch 25/150 | Batch 66/198 | Loss: 2.0008161067962646\n","Epoch 25/150 | Batch 67/198 | Loss: 1.860935091972351\n","Epoch 25/150 | Batch 68/198 | Loss: 2.0216710567474365\n","Epoch 25/150 | Batch 69/198 | Loss: 1.6521542072296143\n","Epoch 25/150 | Batch 70/198 | Loss: 1.9606761932373047\n","Epoch 25/150 | Batch 71/198 | Loss: 1.7138110399246216\n","Epoch 25/150 | Batch 72/198 | Loss: 1.797020435333252\n","Epoch 25/150 | Batch 73/198 | Loss: 1.4977251291275024\n","Epoch 25/150 | Batch 74/198 | Loss: 1.7889504432678223\n","Epoch 25/150 | Batch 75/198 | Loss: 1.8040101528167725\n","Epoch 25/150 | Batch 76/198 | Loss: 1.959518313407898\n","Epoch 25/150 | Batch 77/198 | Loss: 1.8600976467132568\n","Epoch 25/150 | Batch 78/198 | Loss: 1.924910068511963\n","Epoch 25/150 | Batch 79/198 | Loss: 1.8767293691635132\n","Epoch 25/150 | Batch 80/198 | Loss: 1.8123739957809448\n","Epoch 25/150 | Batch 81/198 | Loss: 2.0562174320220947\n","Epoch 25/150 | Batch 82/198 | Loss: 1.9883531332015991\n","Epoch 25/150 | Batch 83/198 | Loss: 2.016221284866333\n","Epoch 25/150 | Batch 84/198 | Loss: 2.0997676849365234\n","Epoch 25/150 | Batch 85/198 | Loss: 1.910973072052002\n","Epoch 25/150 | Batch 86/198 | Loss: 1.8359941244125366\n","Epoch 25/150 | Batch 87/198 | Loss: 1.8714826107025146\n","Epoch 25/150 | Batch 88/198 | Loss: 1.91725492477417\n","Epoch 25/150 | Batch 89/198 | Loss: 1.7750142812728882\n","Epoch 25/150 | Batch 90/198 | Loss: 1.6352856159210205\n","Epoch 25/150 | Batch 91/198 | Loss: 1.9478867053985596\n","Epoch 25/150 | Batch 92/198 | Loss: 1.8975751399993896\n","Epoch 25/150 | Batch 93/198 | Loss: 1.7306193113327026\n","Epoch 25/150 | Batch 94/198 | Loss: 2.1620359420776367\n","Epoch 25/150 | Batch 95/198 | Loss: 1.7069882154464722\n","Epoch 25/150 | Batch 96/198 | Loss: 1.8232648372650146\n","Epoch 25/150 | Batch 97/198 | Loss: 1.8767948150634766\n","Epoch 25/150 | Batch 98/198 | Loss: 1.7466135025024414\n","Epoch 25/150 | Batch 99/198 | Loss: 1.8019726276397705\n","Epoch 25/150 | Batch 100/198 | Loss: 1.8825913667678833\n","Epoch 25/150 | Batch 101/198 | Loss: 1.763152003288269\n","Epoch 25/150 | Batch 102/198 | Loss: 1.7665085792541504\n","Epoch 25/150 | Batch 103/198 | Loss: 1.9856935739517212\n","Epoch 25/150 | Batch 104/198 | Loss: 2.078019618988037\n","Epoch 25/150 | Batch 105/198 | Loss: 1.8753559589385986\n","Epoch 25/150 | Batch 106/198 | Loss: 1.7258315086364746\n","Epoch 25/150 | Batch 107/198 | Loss: 1.8357739448547363\n","Epoch 25/150 | Batch 108/198 | Loss: 1.9399793148040771\n","Epoch 25/150 | Batch 109/198 | Loss: 2.0800485610961914\n","Epoch 25/150 | Batch 110/198 | Loss: 1.8453540802001953\n","Epoch 25/150 | Batch 111/198 | Loss: 1.631072998046875\n","Epoch 25/150 | Batch 112/198 | Loss: 1.8308119773864746\n","Epoch 25/150 | Batch 113/198 | Loss: 2.1754071712493896\n","Epoch 25/150 | Batch 114/198 | Loss: 1.635650634765625\n","Epoch 25/150 | Batch 115/198 | Loss: 1.730460286140442\n","Epoch 25/150 | Batch 116/198 | Loss: 1.7901813983917236\n","Epoch 25/150 | Batch 117/198 | Loss: 2.0854177474975586\n","Epoch 25/150 | Batch 118/198 | Loss: 1.7670258283615112\n","Epoch 25/150 | Batch 119/198 | Loss: 1.9692946672439575\n","Epoch 25/150 | Batch 120/198 | Loss: 1.938640832901001\n","Epoch 25/150 | Batch 121/198 | Loss: 2.009655475616455\n","Epoch 25/150 | Batch 122/198 | Loss: 1.9965343475341797\n","Epoch 25/150 | Batch 123/198 | Loss: 1.530825138092041\n","Epoch 25/150 | Batch 124/198 | Loss: 1.923985242843628\n","Epoch 25/150 | Batch 125/198 | Loss: 1.7783962488174438\n","Epoch 25/150 | Batch 126/198 | Loss: 2.011575937271118\n","Epoch 25/150 | Batch 127/198 | Loss: 1.8866158723831177\n","Epoch 25/150 | Batch 128/198 | Loss: 1.799580454826355\n","Epoch 25/150 | Batch 129/198 | Loss: 1.9647307395935059\n","Epoch 25/150 | Batch 130/198 | Loss: 1.6569188833236694\n","Epoch 25/150 | Batch 131/198 | Loss: 1.9202862977981567\n","Epoch 25/150 | Batch 132/198 | Loss: 1.9477864503860474\n","Epoch 25/150 | Batch 133/198 | Loss: 1.6753391027450562\n","Epoch 25/150 | Batch 134/198 | Loss: 2.023869037628174\n","Epoch 25/150 | Batch 135/198 | Loss: 2.0176661014556885\n","Epoch 25/150 | Batch 136/198 | Loss: 1.9658770561218262\n","Epoch 25/150 | Batch 137/198 | Loss: 1.6845948696136475\n","Epoch 25/150 | Batch 138/198 | Loss: 2.0474464893341064\n","Epoch 25/150 | Batch 139/198 | Loss: 1.822698950767517\n","Epoch 25/150 | Batch 140/198 | Loss: 1.712245225906372\n","Epoch 25/150 | Batch 141/198 | Loss: 1.8423388004302979\n","Epoch 25/150 | Batch 142/198 | Loss: 1.9329603910446167\n","Epoch 25/150 | Batch 143/198 | Loss: 1.8420639038085938\n","Epoch 25/150 | Batch 144/198 | Loss: 2.0008902549743652\n","Epoch 25/150 | Batch 145/198 | Loss: 1.8072835206985474\n","Epoch 25/150 | Batch 146/198 | Loss: 2.0579140186309814\n","Epoch 25/150 | Batch 147/198 | Loss: 2.099668264389038\n","Epoch 25/150 | Batch 148/198 | Loss: 2.1039445400238037\n","Epoch 25/150 | Batch 149/198 | Loss: 1.9481145143508911\n","Epoch 25/150 | Batch 150/198 | Loss: 1.949432611465454\n","Epoch 25/150 | Batch 151/198 | Loss: 1.8912373781204224\n","Epoch 25/150 | Batch 152/198 | Loss: 2.075885772705078\n","Epoch 25/150 | Batch 153/198 | Loss: 2.0299603939056396\n","Epoch 25/150 | Batch 154/198 | Loss: 2.234069585800171\n","Epoch 25/150 | Batch 155/198 | Loss: 1.8756693601608276\n","Epoch 25/150 | Batch 156/198 | Loss: 2.1190803050994873\n","Epoch 25/150 | Batch 157/198 | Loss: 2.15440034866333\n","Epoch 25/150 | Batch 158/198 | Loss: 1.7605146169662476\n","Epoch 25/150 | Batch 159/198 | Loss: 1.8462817668914795\n","Epoch 25/150 | Batch 160/198 | Loss: 2.038381576538086\n","Epoch 25/150 | Batch 161/198 | Loss: 1.7592869997024536\n","Epoch 25/150 | Batch 162/198 | Loss: 1.9866385459899902\n","Epoch 25/150 | Batch 163/198 | Loss: 1.7409183979034424\n","Epoch 25/150 | Batch 164/198 | Loss: 1.8448822498321533\n","Epoch 25/150 | Batch 165/198 | Loss: 1.7717519998550415\n","Epoch 25/150 | Batch 166/198 | Loss: 1.862151026725769\n","Epoch 25/150 | Batch 167/198 | Loss: 2.0710721015930176\n","Epoch 25/150 | Batch 168/198 | Loss: 1.959378957748413\n","Epoch 25/150 | Batch 169/198 | Loss: 1.952646017074585\n","Epoch 25/150 | Batch 170/198 | Loss: 2.0040736198425293\n","Epoch 25/150 | Batch 171/198 | Loss: 1.5967093706130981\n","Epoch 25/150 | Batch 172/198 | Loss: 1.9396787881851196\n","Epoch 25/150 | Batch 173/198 | Loss: 1.9681516885757446\n","Epoch 25/150 | Batch 174/198 | Loss: 1.9972589015960693\n","Epoch 25/150 | Batch 175/198 | Loss: 2.0753955841064453\n","Epoch 25/150 | Batch 176/198 | Loss: 2.085181951522827\n","Epoch 25/150 | Batch 177/198 | Loss: 1.776663064956665\n","Epoch 25/150 | Batch 178/198 | Loss: 1.7852683067321777\n","Epoch 25/150 | Batch 179/198 | Loss: 1.5632281303405762\n","Epoch 25/150 | Batch 180/198 | Loss: 2.299710988998413\n","Epoch 25/150 | Batch 181/198 | Loss: 2.089167356491089\n","Epoch 25/150 | Batch 182/198 | Loss: 2.069953680038452\n","Epoch 25/150 | Batch 183/198 | Loss: 1.8062622547149658\n","Epoch 25/150 | Batch 184/198 | Loss: 2.0197741985321045\n","Epoch 25/150 | Batch 185/198 | Loss: 1.7077611684799194\n","Epoch 25/150 | Batch 186/198 | Loss: 1.877002239227295\n","Epoch 25/150 | Batch 187/198 | Loss: 1.8307170867919922\n","Epoch 25/150 | Batch 188/198 | Loss: 1.8475655317306519\n","Epoch 25/150 | Batch 189/198 | Loss: 1.9160444736480713\n","Epoch 25/150 | Batch 190/198 | Loss: 2.1807332038879395\n","Epoch 25/150 | Batch 191/198 | Loss: 1.8251981735229492\n","Epoch 25/150 | Batch 192/198 | Loss: 2.0436577796936035\n","Epoch 25/150 | Batch 193/198 | Loss: 1.7425599098205566\n","Epoch 25/150 | Batch 194/198 | Loss: 1.79292631149292\n","Epoch 25/150 | Batch 195/198 | Loss: 1.741389274597168\n","Epoch 25/150 | Batch 196/198 | Loss: 1.8002831935882568\n","Epoch 25/150 | Batch 197/198 | Loss: 2.267622232437134\n","Epoch 25/150 | Batch 198/198 | Loss: 1.8983112573623657\n","Epoch 25/150 | Average Loss: 1.9027632836139563\n","Epoch 26/150 | Batch 1/198 | Loss: 1.7545548677444458\n","Epoch 26/150 | Batch 2/198 | Loss: 1.7861270904541016\n","Epoch 26/150 | Batch 3/198 | Loss: 1.8781561851501465\n","Epoch 26/150 | Batch 4/198 | Loss: 1.69911789894104\n","Epoch 26/150 | Batch 5/198 | Loss: 1.9877512454986572\n","Epoch 26/150 | Batch 6/198 | Loss: 1.9357681274414062\n","Epoch 26/150 | Batch 7/198 | Loss: 2.1343135833740234\n","Epoch 26/150 | Batch 8/198 | Loss: 1.726503610610962\n","Epoch 26/150 | Batch 9/198 | Loss: 1.6631834506988525\n","Epoch 26/150 | Batch 10/198 | Loss: 1.9534549713134766\n","Epoch 26/150 | Batch 11/198 | Loss: 1.8415485620498657\n","Epoch 26/150 | Batch 12/198 | Loss: 2.0451135635375977\n","Epoch 26/150 | Batch 13/198 | Loss: 1.9303743839263916\n","Epoch 26/150 | Batch 14/198 | Loss: 1.7124569416046143\n","Epoch 26/150 | Batch 15/198 | Loss: 1.8366124629974365\n","Epoch 26/150 | Batch 16/198 | Loss: 1.9535707235336304\n","Epoch 26/150 | Batch 17/198 | Loss: 1.804207444190979\n","Epoch 26/150 | Batch 18/198 | Loss: 1.9324713945388794\n","Epoch 26/150 | Batch 19/198 | Loss: 1.729150414466858\n","Epoch 26/150 | Batch 20/198 | Loss: 1.7520328760147095\n","Epoch 26/150 | Batch 21/198 | Loss: 1.748124599456787\n","Epoch 26/150 | Batch 22/198 | Loss: 1.7386747598648071\n","Epoch 26/150 | Batch 23/198 | Loss: 1.7928143739700317\n","Epoch 26/150 | Batch 24/198 | Loss: 1.7414429187774658\n","Epoch 26/150 | Batch 25/198 | Loss: 2.1111135482788086\n","Epoch 26/150 | Batch 26/198 | Loss: 1.823827862739563\n","Epoch 26/150 | Batch 27/198 | Loss: 1.8388807773590088\n","Epoch 26/150 | Batch 28/198 | Loss: 1.979964256286621\n","Epoch 26/150 | Batch 29/198 | Loss: 1.7803902626037598\n","Epoch 26/150 | Batch 30/198 | Loss: 2.0256593227386475\n","Epoch 26/150 | Batch 31/198 | Loss: 1.9386693239212036\n","Epoch 26/150 | Batch 32/198 | Loss: 1.7409865856170654\n","Epoch 26/150 | Batch 33/198 | Loss: 1.7940102815628052\n","Epoch 26/150 | Batch 34/198 | Loss: 1.9126614332199097\n","Epoch 26/150 | Batch 35/198 | Loss: 1.996714472770691\n","Epoch 26/150 | Batch 36/198 | Loss: 2.0876240730285645\n","Epoch 26/150 | Batch 37/198 | Loss: 1.8806825876235962\n","Epoch 26/150 | Batch 38/198 | Loss: 1.9231723546981812\n","Epoch 26/150 | Batch 39/198 | Loss: 2.055757999420166\n","Epoch 26/150 | Batch 40/198 | Loss: 1.6957639455795288\n","Epoch 26/150 | Batch 41/198 | Loss: 1.9068282842636108\n","Epoch 26/150 | Batch 42/198 | Loss: 1.776503562927246\n","Epoch 26/150 | Batch 43/198 | Loss: 1.7416424751281738\n","Epoch 26/150 | Batch 44/198 | Loss: 1.8743222951889038\n","Epoch 26/150 | Batch 45/198 | Loss: 1.7193866968154907\n","Epoch 26/150 | Batch 46/198 | Loss: 1.7873554229736328\n","Epoch 26/150 | Batch 47/198 | Loss: 1.8799617290496826\n","Epoch 26/150 | Batch 48/198 | Loss: 1.7960278987884521\n","Epoch 26/150 | Batch 49/198 | Loss: 1.7825900316238403\n","Epoch 26/150 | Batch 50/198 | Loss: 1.8959828615188599\n","Epoch 26/150 | Batch 51/198 | Loss: 1.8087061643600464\n","Epoch 26/150 | Batch 52/198 | Loss: 1.7782471179962158\n","Epoch 26/150 | Batch 53/198 | Loss: 1.6889903545379639\n","Epoch 26/150 | Batch 54/198 | Loss: 1.71868097782135\n","Epoch 26/150 | Batch 55/198 | Loss: 1.909301519393921\n","Epoch 26/150 | Batch 56/198 | Loss: 1.898720383644104\n","Epoch 26/150 | Batch 57/198 | Loss: 1.7952666282653809\n","Epoch 26/150 | Batch 58/198 | Loss: 1.631061315536499\n","Epoch 26/150 | Batch 59/198 | Loss: 1.8928834199905396\n","Epoch 26/150 | Batch 60/198 | Loss: 1.759916067123413\n","Epoch 26/150 | Batch 61/198 | Loss: 1.8199503421783447\n","Epoch 26/150 | Batch 62/198 | Loss: 1.8075168132781982\n","Epoch 26/150 | Batch 63/198 | Loss: 1.8690091371536255\n","Epoch 26/150 | Batch 64/198 | Loss: 2.027740478515625\n","Epoch 26/150 | Batch 65/198 | Loss: 1.9825011491775513\n","Epoch 26/150 | Batch 66/198 | Loss: 1.7518999576568604\n","Epoch 26/150 | Batch 67/198 | Loss: 2.023662805557251\n","Epoch 26/150 | Batch 68/198 | Loss: 1.8628240823745728\n","Epoch 26/150 | Batch 69/198 | Loss: 1.6671741008758545\n","Epoch 26/150 | Batch 70/198 | Loss: 1.591073751449585\n","Epoch 26/150 | Batch 71/198 | Loss: 1.9103500843048096\n","Epoch 26/150 | Batch 72/198 | Loss: 2.1923046112060547\n","Epoch 26/150 | Batch 73/198 | Loss: 1.839477300643921\n","Epoch 26/150 | Batch 74/198 | Loss: 2.0429890155792236\n","Epoch 26/150 | Batch 75/198 | Loss: 1.949154019355774\n","Epoch 26/150 | Batch 76/198 | Loss: 1.8783130645751953\n","Epoch 26/150 | Batch 77/198 | Loss: 1.8545031547546387\n","Epoch 26/150 | Batch 78/198 | Loss: 1.867363452911377\n","Epoch 26/150 | Batch 79/198 | Loss: 1.6203685998916626\n","Epoch 26/150 | Batch 80/198 | Loss: 1.7240965366363525\n","Epoch 26/150 | Batch 81/198 | Loss: 1.6790069341659546\n","Epoch 26/150 | Batch 82/198 | Loss: 1.699243187904358\n","Epoch 26/150 | Batch 83/198 | Loss: 1.8179655075073242\n","Epoch 26/150 | Batch 84/198 | Loss: 1.5995936393737793\n","Epoch 26/150 | Batch 85/198 | Loss: 1.911050796508789\n","Epoch 26/150 | Batch 86/198 | Loss: 1.7012585401535034\n","Epoch 26/150 | Batch 87/198 | Loss: 1.7329137325286865\n","Epoch 26/150 | Batch 88/198 | Loss: 1.705088496208191\n","Epoch 26/150 | Batch 89/198 | Loss: 1.955056071281433\n","Epoch 26/150 | Batch 90/198 | Loss: 1.662379503250122\n","Epoch 26/150 | Batch 91/198 | Loss: 1.6929277181625366\n","Epoch 26/150 | Batch 92/198 | Loss: 1.9376637935638428\n","Epoch 26/150 | Batch 93/198 | Loss: 1.894234538078308\n","Epoch 26/150 | Batch 94/198 | Loss: 1.8754754066467285\n","Epoch 26/150 | Batch 95/198 | Loss: 1.912701964378357\n","Epoch 26/150 | Batch 96/198 | Loss: 1.4755300283432007\n","Epoch 26/150 | Batch 97/198 | Loss: 2.137575387954712\n","Epoch 26/150 | Batch 98/198 | Loss: 1.596125602722168\n","Epoch 26/150 | Batch 99/198 | Loss: 1.9447855949401855\n","Epoch 26/150 | Batch 100/198 | Loss: 2.0369975566864014\n","Epoch 26/150 | Batch 101/198 | Loss: 1.4755429029464722\n","Epoch 26/150 | Batch 102/198 | Loss: 1.550024390220642\n","Epoch 26/150 | Batch 103/198 | Loss: 1.9061814546585083\n","Epoch 26/150 | Batch 104/198 | Loss: 1.826621413230896\n","Epoch 26/150 | Batch 105/198 | Loss: 1.896410584449768\n","Epoch 26/150 | Batch 106/198 | Loss: 1.925135612487793\n","Epoch 26/150 | Batch 107/198 | Loss: 1.8341807126998901\n","Epoch 26/150 | Batch 108/198 | Loss: 1.9593617916107178\n","Epoch 26/150 | Batch 109/198 | Loss: 1.9895267486572266\n","Epoch 26/150 | Batch 110/198 | Loss: 1.7450437545776367\n","Epoch 26/150 | Batch 111/198 | Loss: 1.7617460489273071\n","Epoch 26/150 | Batch 112/198 | Loss: 1.853164553642273\n","Epoch 26/150 | Batch 113/198 | Loss: 1.9222019910812378\n","Epoch 26/150 | Batch 114/198 | Loss: 1.8269498348236084\n","Epoch 26/150 | Batch 115/198 | Loss: 1.8080358505249023\n","Epoch 26/150 | Batch 116/198 | Loss: 1.8165936470031738\n","Epoch 26/150 | Batch 117/198 | Loss: 1.713287353515625\n","Epoch 26/150 | Batch 118/198 | Loss: 2.003990411758423\n","Epoch 26/150 | Batch 119/198 | Loss: 2.0117928981781006\n","Epoch 26/150 | Batch 120/198 | Loss: 1.5538252592086792\n","Epoch 26/150 | Batch 121/198 | Loss: 1.7412471771240234\n","Epoch 26/150 | Batch 122/198 | Loss: 1.7488921880722046\n","Epoch 26/150 | Batch 123/198 | Loss: 1.821216344833374\n","Epoch 26/150 | Batch 124/198 | Loss: 1.692327618598938\n","Epoch 26/150 | Batch 125/198 | Loss: 1.6208916902542114\n","Epoch 26/150 | Batch 126/198 | Loss: 1.771152377128601\n","Epoch 26/150 | Batch 127/198 | Loss: 1.9683902263641357\n","Epoch 26/150 | Batch 128/198 | Loss: 2.0092761516571045\n","Epoch 26/150 | Batch 129/198 | Loss: 1.8731436729431152\n","Epoch 26/150 | Batch 130/198 | Loss: 1.9024664163589478\n","Epoch 26/150 | Batch 131/198 | Loss: 1.865807056427002\n","Epoch 26/150 | Batch 132/198 | Loss: 2.080641746520996\n","Epoch 26/150 | Batch 133/198 | Loss: 2.143779754638672\n","Epoch 26/150 | Batch 134/198 | Loss: 1.8908593654632568\n","Epoch 26/150 | Batch 135/198 | Loss: 1.637635350227356\n","Epoch 26/150 | Batch 136/198 | Loss: 1.8852993249893188\n","Epoch 26/150 | Batch 137/198 | Loss: 2.085178852081299\n","Epoch 26/150 | Batch 138/198 | Loss: 1.8564990758895874\n","Epoch 26/150 | Batch 139/198 | Loss: 2.027987241744995\n","Epoch 26/150 | Batch 140/198 | Loss: 1.8101427555084229\n","Epoch 26/150 | Batch 141/198 | Loss: 1.6126433610916138\n","Epoch 26/150 | Batch 142/198 | Loss: 2.127385377883911\n","Epoch 26/150 | Batch 143/198 | Loss: 2.0237550735473633\n","Epoch 26/150 | Batch 144/198 | Loss: 1.8126689195632935\n","Epoch 26/150 | Batch 145/198 | Loss: 1.7536696195602417\n","Epoch 26/150 | Batch 146/198 | Loss: 1.794827938079834\n","Epoch 26/150 | Batch 147/198 | Loss: 2.078310012817383\n","Epoch 26/150 | Batch 148/198 | Loss: 1.8145874738693237\n","Epoch 26/150 | Batch 149/198 | Loss: 1.906408429145813\n","Epoch 26/150 | Batch 150/198 | Loss: 1.793579339981079\n","Epoch 26/150 | Batch 151/198 | Loss: 1.6597830057144165\n","Epoch 26/150 | Batch 152/198 | Loss: 1.9007158279418945\n","Epoch 26/150 | Batch 153/198 | Loss: 2.0883443355560303\n","Epoch 26/150 | Batch 154/198 | Loss: 1.9646753072738647\n","Epoch 26/150 | Batch 155/198 | Loss: 1.8134452104568481\n","Epoch 26/150 | Batch 156/198 | Loss: 2.029512405395508\n","Epoch 26/150 | Batch 157/198 | Loss: 1.6939127445220947\n","Epoch 26/150 | Batch 158/198 | Loss: 1.7689213752746582\n","Epoch 26/150 | Batch 159/198 | Loss: 1.7671809196472168\n","Epoch 26/150 | Batch 160/198 | Loss: 2.124234199523926\n","Epoch 26/150 | Batch 161/198 | Loss: 2.269078493118286\n","Epoch 26/150 | Batch 162/198 | Loss: 2.045044183731079\n","Epoch 26/150 | Batch 163/198 | Loss: 1.9306061267852783\n","Epoch 26/150 | Batch 164/198 | Loss: 1.7520442008972168\n","Epoch 26/150 | Batch 165/198 | Loss: 1.7499064207077026\n","Epoch 26/150 | Batch 166/198 | Loss: 1.592722773551941\n","Epoch 26/150 | Batch 167/198 | Loss: 1.963719367980957\n","Epoch 26/150 | Batch 168/198 | Loss: 1.9018856287002563\n","Epoch 26/150 | Batch 169/198 | Loss: 1.893057107925415\n","Epoch 26/150 | Batch 170/198 | Loss: 1.9259871244430542\n","Epoch 26/150 | Batch 171/198 | Loss: 1.753044605255127\n","Epoch 26/150 | Batch 172/198 | Loss: 2.1585311889648438\n","Epoch 26/150 | Batch 173/198 | Loss: 1.8850317001342773\n","Epoch 26/150 | Batch 174/198 | Loss: 1.7609184980392456\n","Epoch 26/150 | Batch 175/198 | Loss: 1.8051809072494507\n","Epoch 26/150 | Batch 176/198 | Loss: 1.8617497682571411\n","Epoch 26/150 | Batch 177/198 | Loss: 1.8706544637680054\n","Epoch 26/150 | Batch 178/198 | Loss: 1.9565296173095703\n","Epoch 26/150 | Batch 179/198 | Loss: 1.82270085811615\n","Epoch 26/150 | Batch 180/198 | Loss: 2.099628448486328\n","Epoch 26/150 | Batch 181/198 | Loss: 1.9289313554763794\n","Epoch 26/150 | Batch 182/198 | Loss: 1.845463514328003\n","Epoch 26/150 | Batch 183/198 | Loss: 2.0695960521698\n","Epoch 26/150 | Batch 184/198 | Loss: 1.9863865375518799\n","Epoch 26/150 | Batch 185/198 | Loss: 1.6784892082214355\n","Epoch 26/150 | Batch 186/198 | Loss: 2.1536335945129395\n","Epoch 26/150 | Batch 187/198 | Loss: 1.7029485702514648\n","Epoch 26/150 | Batch 188/198 | Loss: 2.074831962585449\n","Epoch 26/150 | Batch 189/198 | Loss: 2.0150723457336426\n","Epoch 26/150 | Batch 190/198 | Loss: 1.867916464805603\n","Epoch 26/150 | Batch 191/198 | Loss: 2.1751320362091064\n","Epoch 26/150 | Batch 192/198 | Loss: 1.839086651802063\n","Epoch 26/150 | Batch 193/198 | Loss: 1.9702426195144653\n","Epoch 26/150 | Batch 194/198 | Loss: 1.8346959352493286\n","Epoch 26/150 | Batch 195/198 | Loss: 1.7714980840682983\n","Epoch 26/150 | Batch 196/198 | Loss: 1.6818917989730835\n","Epoch 26/150 | Batch 197/198 | Loss: 1.7650574445724487\n","Epoch 26/150 | Batch 198/198 | Loss: 1.5643504858016968\n","Epoch 26/150 | Average Loss: 1.8546458880106609\n","Epoch 27/150 | Batch 1/198 | Loss: 1.7911869287490845\n","Epoch 27/150 | Batch 2/198 | Loss: 1.8752813339233398\n","Epoch 27/150 | Batch 3/198 | Loss: 1.8197218179702759\n","Epoch 27/150 | Batch 4/198 | Loss: 1.6472591161727905\n","Epoch 27/150 | Batch 5/198 | Loss: 1.785157322883606\n","Epoch 27/150 | Batch 6/198 | Loss: 1.7235573530197144\n","Epoch 27/150 | Batch 7/198 | Loss: 1.7230303287506104\n","Epoch 27/150 | Batch 8/198 | Loss: 1.7392133474349976\n","Epoch 27/150 | Batch 9/198 | Loss: 2.0045089721679688\n","Epoch 27/150 | Batch 10/198 | Loss: 1.8015961647033691\n","Epoch 27/150 | Batch 11/198 | Loss: 1.7023427486419678\n","Epoch 27/150 | Batch 12/198 | Loss: 1.8357285261154175\n","Epoch 27/150 | Batch 13/198 | Loss: 1.4644756317138672\n","Epoch 27/150 | Batch 14/198 | Loss: 1.6417893171310425\n","Epoch 27/150 | Batch 15/198 | Loss: 1.8958219289779663\n","Epoch 27/150 | Batch 16/198 | Loss: 1.7585667371749878\n","Epoch 27/150 | Batch 17/198 | Loss: 1.7278205156326294\n","Epoch 27/150 | Batch 18/198 | Loss: 1.8502095937728882\n","Epoch 27/150 | Batch 19/198 | Loss: 1.9387400150299072\n","Epoch 27/150 | Batch 20/198 | Loss: 1.6702823638916016\n","Epoch 27/150 | Batch 21/198 | Loss: 1.7655285596847534\n","Epoch 27/150 | Batch 22/198 | Loss: 1.7073206901550293\n","Epoch 27/150 | Batch 23/198 | Loss: 1.875576376914978\n","Epoch 27/150 | Batch 24/198 | Loss: 2.104734182357788\n","Epoch 27/150 | Batch 25/198 | Loss: 1.8889141082763672\n","Epoch 27/150 | Batch 26/198 | Loss: 1.9629414081573486\n","Epoch 27/150 | Batch 27/198 | Loss: 1.7209982872009277\n","Epoch 27/150 | Batch 28/198 | Loss: 1.6470799446105957\n","Epoch 27/150 | Batch 29/198 | Loss: 1.7706201076507568\n","Epoch 27/150 | Batch 30/198 | Loss: 2.020232915878296\n","Epoch 27/150 | Batch 31/198 | Loss: 1.7902090549468994\n","Epoch 27/150 | Batch 32/198 | Loss: 1.6872698068618774\n","Epoch 27/150 | Batch 33/198 | Loss: 1.934884786605835\n","Epoch 27/150 | Batch 34/198 | Loss: 1.8695964813232422\n","Epoch 27/150 | Batch 35/198 | Loss: 1.8634874820709229\n","Epoch 27/150 | Batch 36/198 | Loss: 1.9333182573318481\n","Epoch 27/150 | Batch 37/198 | Loss: 1.7554467916488647\n","Epoch 27/150 | Batch 38/198 | Loss: 2.012209177017212\n","Epoch 27/150 | Batch 39/198 | Loss: 1.6513564586639404\n","Epoch 27/150 | Batch 40/198 | Loss: 1.8729106187820435\n","Epoch 27/150 | Batch 41/198 | Loss: 1.6748712062835693\n","Epoch 27/150 | Batch 42/198 | Loss: 1.7643442153930664\n","Epoch 27/150 | Batch 43/198 | Loss: 2.007620096206665\n","Epoch 27/150 | Batch 44/198 | Loss: 1.636265754699707\n","Epoch 27/150 | Batch 45/198 | Loss: 1.7144159078598022\n","Epoch 27/150 | Batch 46/198 | Loss: 1.826099157333374\n","Epoch 27/150 | Batch 47/198 | Loss: 1.7609845399856567\n","Epoch 27/150 | Batch 48/198 | Loss: 1.7754884958267212\n","Epoch 27/150 | Batch 49/198 | Loss: 1.8581212759017944\n","Epoch 27/150 | Batch 50/198 | Loss: 1.6943200826644897\n","Epoch 27/150 | Batch 51/198 | Loss: 1.9503377676010132\n","Epoch 27/150 | Batch 52/198 | Loss: 1.8585829734802246\n","Epoch 27/150 | Batch 53/198 | Loss: 1.6234309673309326\n","Epoch 27/150 | Batch 54/198 | Loss: 1.718112826347351\n","Epoch 27/150 | Batch 55/198 | Loss: 1.6468390226364136\n","Epoch 27/150 | Batch 56/198 | Loss: 1.735107421875\n","Epoch 27/150 | Batch 57/198 | Loss: 2.0636801719665527\n","Epoch 27/150 | Batch 58/198 | Loss: 1.7747994661331177\n","Epoch 27/150 | Batch 59/198 | Loss: 1.8741666078567505\n","Epoch 27/150 | Batch 60/198 | Loss: 1.6023858785629272\n","Epoch 27/150 | Batch 61/198 | Loss: 1.7074953317642212\n","Epoch 27/150 | Batch 62/198 | Loss: 1.8495938777923584\n","Epoch 27/150 | Batch 63/198 | Loss: 1.74752676486969\n","Epoch 27/150 | Batch 64/198 | Loss: 1.8235185146331787\n","Epoch 27/150 | Batch 65/198 | Loss: 1.7176635265350342\n","Epoch 27/150 | Batch 66/198 | Loss: 1.8735684156417847\n","Epoch 27/150 | Batch 67/198 | Loss: 1.5932612419128418\n","Epoch 27/150 | Batch 68/198 | Loss: 2.0584921836853027\n","Epoch 27/150 | Batch 69/198 | Loss: 1.7867627143859863\n","Epoch 27/150 | Batch 70/198 | Loss: 1.7633817195892334\n","Epoch 27/150 | Batch 71/198 | Loss: 1.6848934888839722\n","Epoch 27/150 | Batch 72/198 | Loss: 1.8229687213897705\n","Epoch 27/150 | Batch 73/198 | Loss: 1.9330899715423584\n","Epoch 27/150 | Batch 74/198 | Loss: 1.8202284574508667\n","Epoch 27/150 | Batch 75/198 | Loss: 1.932021975517273\n","Epoch 27/150 | Batch 76/198 | Loss: 1.6545277833938599\n","Epoch 27/150 | Batch 77/198 | Loss: 1.8590714931488037\n","Epoch 27/150 | Batch 78/198 | Loss: 1.8929414749145508\n","Epoch 27/150 | Batch 79/198 | Loss: 1.8776001930236816\n","Epoch 27/150 | Batch 80/198 | Loss: 1.978755235671997\n","Epoch 27/150 | Batch 81/198 | Loss: 1.8486003875732422\n","Epoch 27/150 | Batch 82/198 | Loss: 1.5860579013824463\n","Epoch 27/150 | Batch 83/198 | Loss: 1.6839663982391357\n","Epoch 27/150 | Batch 84/198 | Loss: 1.6466237306594849\n","Epoch 27/150 | Batch 85/198 | Loss: 2.0085842609405518\n","Epoch 27/150 | Batch 86/198 | Loss: 2.1325488090515137\n","Epoch 27/150 | Batch 87/198 | Loss: 1.8334397077560425\n","Epoch 27/150 | Batch 88/198 | Loss: 1.812414526939392\n","Epoch 27/150 | Batch 89/198 | Loss: 2.0458498001098633\n","Epoch 27/150 | Batch 90/198 | Loss: 1.7916386127471924\n","Epoch 27/150 | Batch 91/198 | Loss: 1.4788018465042114\n","Epoch 27/150 | Batch 92/198 | Loss: 1.726035475730896\n","Epoch 27/150 | Batch 93/198 | Loss: 1.5976239442825317\n","Epoch 27/150 | Batch 94/198 | Loss: 1.8190888166427612\n","Epoch 27/150 | Batch 95/198 | Loss: 1.7816762924194336\n","Epoch 27/150 | Batch 96/198 | Loss: 1.9937663078308105\n","Epoch 27/150 | Batch 97/198 | Loss: 1.8173940181732178\n","Epoch 27/150 | Batch 98/198 | Loss: 1.8050737380981445\n","Epoch 27/150 | Batch 99/198 | Loss: 1.573714256286621\n","Epoch 27/150 | Batch 100/198 | Loss: 1.8166910409927368\n","Epoch 27/150 | Batch 101/198 | Loss: 1.6303982734680176\n","Epoch 27/150 | Batch 102/198 | Loss: 2.06499981880188\n","Epoch 27/150 | Batch 103/198 | Loss: 1.6349270343780518\n","Epoch 27/150 | Batch 104/198 | Loss: 1.9230501651763916\n","Epoch 27/150 | Batch 105/198 | Loss: 1.8536577224731445\n","Epoch 27/150 | Batch 106/198 | Loss: 1.677403211593628\n","Epoch 27/150 | Batch 107/198 | Loss: 1.7259806394577026\n","Epoch 27/150 | Batch 108/198 | Loss: 1.805609107017517\n","Epoch 27/150 | Batch 109/198 | Loss: 1.9079651832580566\n","Epoch 27/150 | Batch 110/198 | Loss: 2.01170015335083\n","Epoch 27/150 | Batch 111/198 | Loss: 1.7885552644729614\n","Epoch 27/150 | Batch 112/198 | Loss: 1.8984133005142212\n","Epoch 27/150 | Batch 113/198 | Loss: 1.4863033294677734\n","Epoch 27/150 | Batch 114/198 | Loss: 1.6699779033660889\n","Epoch 27/150 | Batch 115/198 | Loss: 1.614462971687317\n","Epoch 27/150 | Batch 116/198 | Loss: 1.5637285709381104\n","Epoch 27/150 | Batch 117/198 | Loss: 1.6374719142913818\n","Epoch 27/150 | Batch 118/198 | Loss: 1.685817837715149\n","Epoch 27/150 | Batch 119/198 | Loss: 1.630053997039795\n","Epoch 27/150 | Batch 120/198 | Loss: 1.7595001459121704\n","Epoch 27/150 | Batch 121/198 | Loss: 2.0266520977020264\n","Epoch 27/150 | Batch 122/198 | Loss: 1.4944162368774414\n","Epoch 27/150 | Batch 123/198 | Loss: 1.8568452596664429\n","Epoch 27/150 | Batch 124/198 | Loss: 1.8360190391540527\n","Epoch 27/150 | Batch 125/198 | Loss: 1.9162893295288086\n","Epoch 27/150 | Batch 126/198 | Loss: 1.9457215070724487\n","Epoch 27/150 | Batch 127/198 | Loss: 2.052056312561035\n","Epoch 27/150 | Batch 128/198 | Loss: 2.0004494190216064\n","Epoch 27/150 | Batch 129/198 | Loss: 1.6667802333831787\n","Epoch 27/150 | Batch 130/198 | Loss: 1.919222354888916\n","Epoch 27/150 | Batch 131/198 | Loss: 1.843423843383789\n","Epoch 27/150 | Batch 132/198 | Loss: 2.038982629776001\n","Epoch 27/150 | Batch 133/198 | Loss: 1.7271499633789062\n","Epoch 27/150 | Batch 134/198 | Loss: 1.8500734567642212\n","Epoch 27/150 | Batch 135/198 | Loss: 1.7133607864379883\n","Epoch 27/150 | Batch 136/198 | Loss: 2.0345308780670166\n","Epoch 27/150 | Batch 137/198 | Loss: 1.6896371841430664\n","Epoch 27/150 | Batch 138/198 | Loss: 1.6638532876968384\n","Epoch 27/150 | Batch 139/198 | Loss: 1.8785861730575562\n","Epoch 27/150 | Batch 140/198 | Loss: 1.885227084159851\n","Epoch 27/150 | Batch 141/198 | Loss: 1.6940494775772095\n","Epoch 27/150 | Batch 142/198 | Loss: 1.819907546043396\n","Epoch 27/150 | Batch 143/198 | Loss: 1.8008525371551514\n","Epoch 27/150 | Batch 144/198 | Loss: 2.014577865600586\n","Epoch 27/150 | Batch 145/198 | Loss: 1.8113394975662231\n","Epoch 27/150 | Batch 146/198 | Loss: 1.888990879058838\n","Epoch 27/150 | Batch 147/198 | Loss: 1.9529739618301392\n","Epoch 27/150 | Batch 148/198 | Loss: 1.6904780864715576\n","Epoch 27/150 | Batch 149/198 | Loss: 1.83775794506073\n","Epoch 27/150 | Batch 150/198 | Loss: 1.6104542016983032\n","Epoch 27/150 | Batch 151/198 | Loss: 1.858957290649414\n","Epoch 27/150 | Batch 152/198 | Loss: 1.776732325553894\n","Epoch 27/150 | Batch 153/198 | Loss: 1.6449528932571411\n","Epoch 27/150 | Batch 154/198 | Loss: 1.966762900352478\n","Epoch 27/150 | Batch 155/198 | Loss: 1.482261300086975\n","Epoch 27/150 | Batch 156/198 | Loss: 2.0733158588409424\n","Epoch 27/150 | Batch 157/198 | Loss: 1.828078269958496\n","Epoch 27/150 | Batch 158/198 | Loss: 1.9787142276763916\n","Epoch 27/150 | Batch 159/198 | Loss: 1.8762308359146118\n","Epoch 27/150 | Batch 160/198 | Loss: 1.907349705696106\n","Epoch 27/150 | Batch 161/198 | Loss: 1.8948899507522583\n","Epoch 27/150 | Batch 162/198 | Loss: 1.7688630819320679\n","Epoch 27/150 | Batch 163/198 | Loss: 1.9834173917770386\n","Epoch 27/150 | Batch 164/198 | Loss: 1.662249207496643\n","Epoch 27/150 | Batch 165/198 | Loss: 1.949239730834961\n","Epoch 27/150 | Batch 166/198 | Loss: 1.8305974006652832\n","Epoch 27/150 | Batch 167/198 | Loss: 1.8055071830749512\n","Epoch 27/150 | Batch 168/198 | Loss: 1.7708169221878052\n","Epoch 27/150 | Batch 169/198 | Loss: 1.8404858112335205\n","Epoch 27/150 | Batch 170/198 | Loss: 1.5180171728134155\n","Epoch 27/150 | Batch 171/198 | Loss: 1.67365562915802\n","Epoch 27/150 | Batch 172/198 | Loss: 1.8447312116622925\n","Epoch 27/150 | Batch 173/198 | Loss: 2.168104887008667\n","Epoch 27/150 | Batch 174/198 | Loss: 1.8767635822296143\n","Epoch 27/150 | Batch 175/198 | Loss: 2.05885648727417\n","Epoch 27/150 | Batch 176/198 | Loss: 1.7784539461135864\n","Epoch 27/150 | Batch 177/198 | Loss: 1.6833288669586182\n","Epoch 27/150 | Batch 178/198 | Loss: 1.9113402366638184\n","Epoch 27/150 | Batch 179/198 | Loss: 1.8176618814468384\n","Epoch 27/150 | Batch 180/198 | Loss: 1.7601218223571777\n","Epoch 27/150 | Batch 181/198 | Loss: 1.8809362649917603\n","Epoch 27/150 | Batch 182/198 | Loss: 1.7698415517807007\n","Epoch 27/150 | Batch 183/198 | Loss: 2.0336294174194336\n","Epoch 27/150 | Batch 184/198 | Loss: 1.6939122676849365\n","Epoch 27/150 | Batch 185/198 | Loss: 2.1274538040161133\n","Epoch 27/150 | Batch 186/198 | Loss: 1.8546751737594604\n","Epoch 27/150 | Batch 187/198 | Loss: 1.6728227138519287\n","Epoch 27/150 | Batch 188/198 | Loss: 1.820365071296692\n","Epoch 27/150 | Batch 189/198 | Loss: 1.8050674200057983\n","Epoch 27/150 | Batch 190/198 | Loss: 2.0058372020721436\n","Epoch 27/150 | Batch 191/198 | Loss: 1.8689228296279907\n","Epoch 27/150 | Batch 192/198 | Loss: 1.9806559085845947\n","Epoch 27/150 | Batch 193/198 | Loss: 1.3802469968795776\n","Epoch 27/150 | Batch 194/198 | Loss: 1.9149543046951294\n","Epoch 27/150 | Batch 195/198 | Loss: 2.0734217166900635\n","Epoch 27/150 | Batch 196/198 | Loss: 1.8980157375335693\n","Epoch 27/150 | Batch 197/198 | Loss: 2.0480592250823975\n","Epoch 27/150 | Batch 198/198 | Loss: 1.8057806491851807\n","Epoch 27/150 | Average Loss: 1.8117350719191812\n","Epoch 28/150 | Batch 1/198 | Loss: 1.8656452894210815\n","Epoch 28/150 | Batch 2/198 | Loss: 1.7244899272918701\n","Epoch 28/150 | Batch 3/198 | Loss: 1.5604586601257324\n","Epoch 28/150 | Batch 4/198 | Loss: 2.0960633754730225\n","Epoch 28/150 | Batch 5/198 | Loss: 1.7688777446746826\n","Epoch 28/150 | Batch 6/198 | Loss: 1.7197902202606201\n","Epoch 28/150 | Batch 7/198 | Loss: 1.745005488395691\n","Epoch 28/150 | Batch 8/198 | Loss: 1.7601776123046875\n","Epoch 28/150 | Batch 9/198 | Loss: 1.958831548690796\n","Epoch 28/150 | Batch 10/198 | Loss: 1.6385362148284912\n","Epoch 28/150 | Batch 11/198 | Loss: 2.0025174617767334\n","Epoch 28/150 | Batch 12/198 | Loss: 1.665114402770996\n","Epoch 28/150 | Batch 13/198 | Loss: 1.5886168479919434\n","Epoch 28/150 | Batch 14/198 | Loss: 1.7413511276245117\n","Epoch 28/150 | Batch 15/198 | Loss: 1.9372397661209106\n","Epoch 28/150 | Batch 16/198 | Loss: 1.8048537969589233\n","Epoch 28/150 | Batch 17/198 | Loss: 1.622941017150879\n","Epoch 28/150 | Batch 18/198 | Loss: 2.071591377258301\n","Epoch 28/150 | Batch 19/198 | Loss: 1.9012761116027832\n","Epoch 28/150 | Batch 20/198 | Loss: 1.4512948989868164\n","Epoch 28/150 | Batch 21/198 | Loss: 1.7221612930297852\n","Epoch 28/150 | Batch 22/198 | Loss: 2.0502874851226807\n","Epoch 28/150 | Batch 23/198 | Loss: 1.6397385597229004\n","Epoch 28/150 | Batch 24/198 | Loss: 1.9181917905807495\n","Epoch 28/150 | Batch 25/198 | Loss: 2.1203808784484863\n","Epoch 28/150 | Batch 26/198 | Loss: 1.7900956869125366\n","Epoch 28/150 | Batch 27/198 | Loss: 1.936483383178711\n","Epoch 28/150 | Batch 28/198 | Loss: 1.8300145864486694\n","Epoch 28/150 | Batch 29/198 | Loss: 1.809556245803833\n","Epoch 28/150 | Batch 30/198 | Loss: 1.9243746995925903\n","Epoch 28/150 | Batch 31/198 | Loss: 1.6047413349151611\n","Epoch 28/150 | Batch 32/198 | Loss: 1.8137896060943604\n","Epoch 28/150 | Batch 33/198 | Loss: 1.6422730684280396\n","Epoch 28/150 | Batch 34/198 | Loss: 1.4329415559768677\n","Epoch 28/150 | Batch 35/198 | Loss: 1.6789157390594482\n","Epoch 28/150 | Batch 36/198 | Loss: 1.6676437854766846\n","Epoch 28/150 | Batch 37/198 | Loss: 1.811947226524353\n","Epoch 28/150 | Batch 38/198 | Loss: 1.8200980424880981\n","Epoch 28/150 | Batch 39/198 | Loss: 1.8401530981063843\n","Epoch 28/150 | Batch 40/198 | Loss: 1.8946917057037354\n","Epoch 28/150 | Batch 41/198 | Loss: 1.8607197999954224\n","Epoch 28/150 | Batch 42/198 | Loss: 1.9679515361785889\n","Epoch 28/150 | Batch 43/198 | Loss: 1.912054419517517\n","Epoch 28/150 | Batch 44/198 | Loss: 1.8547987937927246\n","Epoch 28/150 | Batch 45/198 | Loss: 1.7839720249176025\n","Epoch 28/150 | Batch 46/198 | Loss: 1.6319904327392578\n","Epoch 28/150 | Batch 47/198 | Loss: 1.6658543348312378\n","Epoch 28/150 | Batch 48/198 | Loss: 1.7274165153503418\n","Epoch 28/150 | Batch 49/198 | Loss: 1.8927786350250244\n","Epoch 28/150 | Batch 50/198 | Loss: 1.7805204391479492\n","Epoch 28/150 | Batch 51/198 | Loss: 1.8011068105697632\n","Epoch 28/150 | Batch 52/198 | Loss: 1.567525863647461\n","Epoch 28/150 | Batch 53/198 | Loss: 1.9484691619873047\n","Epoch 28/150 | Batch 54/198 | Loss: 1.713059902191162\n","Epoch 28/150 | Batch 55/198 | Loss: 1.6302286386489868\n","Epoch 28/150 | Batch 56/198 | Loss: 1.824831485748291\n","Epoch 28/150 | Batch 57/198 | Loss: 1.5931695699691772\n","Epoch 28/150 | Batch 58/198 | Loss: 1.6261545419692993\n","Epoch 28/150 | Batch 59/198 | Loss: 1.6727228164672852\n","Epoch 28/150 | Batch 60/198 | Loss: 1.7339692115783691\n","Epoch 28/150 | Batch 61/198 | Loss: 1.6170034408569336\n","Epoch 28/150 | Batch 62/198 | Loss: 1.628331184387207\n","Epoch 28/150 | Batch 63/198 | Loss: 1.49509596824646\n","Epoch 28/150 | Batch 64/198 | Loss: 1.717728853225708\n","Epoch 28/150 | Batch 65/198 | Loss: 1.7838054895401\n","Epoch 28/150 | Batch 66/198 | Loss: 1.664212703704834\n","Epoch 28/150 | Batch 67/198 | Loss: 1.9169433116912842\n","Epoch 28/150 | Batch 68/198 | Loss: 1.883023977279663\n","Epoch 28/150 | Batch 69/198 | Loss: 1.6159800291061401\n","Epoch 28/150 | Batch 70/198 | Loss: 1.6689867973327637\n","Epoch 28/150 | Batch 71/198 | Loss: 1.8642172813415527\n","Epoch 28/150 | Batch 72/198 | Loss: 1.624110460281372\n","Epoch 28/150 | Batch 73/198 | Loss: 1.6198577880859375\n","Epoch 28/150 | Batch 74/198 | Loss: 1.5177431106567383\n","Epoch 28/150 | Batch 75/198 | Loss: 1.8742506504058838\n","Epoch 28/150 | Batch 76/198 | Loss: 1.533266544342041\n","Epoch 28/150 | Batch 77/198 | Loss: 1.8377233743667603\n","Epoch 28/150 | Batch 78/198 | Loss: 1.5915964841842651\n","Epoch 28/150 | Batch 79/198 | Loss: 1.5787129402160645\n","Epoch 28/150 | Batch 80/198 | Loss: 1.669777512550354\n","Epoch 28/150 | Batch 81/198 | Loss: 1.798887014389038\n","Epoch 28/150 | Batch 82/198 | Loss: 1.7199039459228516\n","Epoch 28/150 | Batch 83/198 | Loss: 1.720849633216858\n","Epoch 28/150 | Batch 84/198 | Loss: 1.7681454420089722\n","Epoch 28/150 | Batch 85/198 | Loss: 1.3863338232040405\n","Epoch 28/150 | Batch 86/198 | Loss: 1.8453856706619263\n","Epoch 28/150 | Batch 87/198 | Loss: 1.8030564785003662\n","Epoch 28/150 | Batch 88/198 | Loss: 1.8309111595153809\n","Epoch 28/150 | Batch 89/198 | Loss: 1.9266923666000366\n","Epoch 28/150 | Batch 90/198 | Loss: 1.8404968976974487\n","Epoch 28/150 | Batch 91/198 | Loss: 1.6954714059829712\n","Epoch 28/150 | Batch 92/198 | Loss: 1.6996561288833618\n","Epoch 28/150 | Batch 93/198 | Loss: 1.7445616722106934\n","Epoch 28/150 | Batch 94/198 | Loss: 1.9534157514572144\n","Epoch 28/150 | Batch 95/198 | Loss: 1.8467034101486206\n","Epoch 28/150 | Batch 96/198 | Loss: 1.620335340499878\n","Epoch 28/150 | Batch 97/198 | Loss: 1.9071458578109741\n","Epoch 28/150 | Batch 98/198 | Loss: 1.8062100410461426\n","Epoch 28/150 | Batch 99/198 | Loss: 1.8268290758132935\n","Epoch 28/150 | Batch 100/198 | Loss: 1.8925327062606812\n","Epoch 28/150 | Batch 101/198 | Loss: 1.7075673341751099\n","Epoch 28/150 | Batch 102/198 | Loss: 1.6108413934707642\n","Epoch 28/150 | Batch 103/198 | Loss: 1.7726603746414185\n","Epoch 28/150 | Batch 104/198 | Loss: 1.8466620445251465\n","Epoch 28/150 | Batch 105/198 | Loss: 1.7662476301193237\n","Epoch 28/150 | Batch 106/198 | Loss: 1.5694752931594849\n","Epoch 28/150 | Batch 107/198 | Loss: 1.895155429840088\n","Epoch 28/150 | Batch 108/198 | Loss: 1.9441043138504028\n","Epoch 28/150 | Batch 109/198 | Loss: 1.6297413110733032\n","Epoch 28/150 | Batch 110/198 | Loss: 1.7560937404632568\n","Epoch 28/150 | Batch 111/198 | Loss: 1.581912636756897\n","Epoch 28/150 | Batch 112/198 | Loss: 1.4906492233276367\n","Epoch 28/150 | Batch 113/198 | Loss: 1.6455460786819458\n","Epoch 28/150 | Batch 114/198 | Loss: 1.8255817890167236\n","Epoch 28/150 | Batch 115/198 | Loss: 1.880366563796997\n","Epoch 28/150 | Batch 116/198 | Loss: 1.7638564109802246\n","Epoch 28/150 | Batch 117/198 | Loss: 1.7199079990386963\n","Epoch 28/150 | Batch 118/198 | Loss: 1.9171675443649292\n","Epoch 28/150 | Batch 119/198 | Loss: 1.801288366317749\n","Epoch 28/150 | Batch 120/198 | Loss: 1.7712044715881348\n","Epoch 28/150 | Batch 121/198 | Loss: 1.7404325008392334\n","Epoch 28/150 | Batch 122/198 | Loss: 1.9052430391311646\n","Epoch 28/150 | Batch 123/198 | Loss: 1.8116565942764282\n","Epoch 28/150 | Batch 124/198 | Loss: 1.68168306350708\n","Epoch 28/150 | Batch 125/198 | Loss: 2.0293257236480713\n","Epoch 28/150 | Batch 126/198 | Loss: 1.7716633081436157\n","Epoch 28/150 | Batch 127/198 | Loss: 1.8773287534713745\n","Epoch 28/150 | Batch 128/198 | Loss: 1.9070454835891724\n","Epoch 28/150 | Batch 129/198 | Loss: 1.7169666290283203\n","Epoch 28/150 | Batch 130/198 | Loss: 1.6970030069351196\n","Epoch 28/150 | Batch 131/198 | Loss: 1.7564209699630737\n","Epoch 28/150 | Batch 132/198 | Loss: 2.029608726501465\n","Epoch 28/150 | Batch 133/198 | Loss: 1.657580852508545\n","Epoch 28/150 | Batch 134/198 | Loss: 1.8179917335510254\n","Epoch 28/150 | Batch 135/198 | Loss: 1.7699087858200073\n","Epoch 28/150 | Batch 136/198 | Loss: 1.6173827648162842\n","Epoch 28/150 | Batch 137/198 | Loss: 1.408628225326538\n","Epoch 28/150 | Batch 138/198 | Loss: 1.9126404523849487\n","Epoch 28/150 | Batch 139/198 | Loss: 1.810722827911377\n","Epoch 28/150 | Batch 140/198 | Loss: 1.8433884382247925\n","Epoch 28/150 | Batch 141/198 | Loss: 1.723488450050354\n","Epoch 28/150 | Batch 142/198 | Loss: 1.5011265277862549\n","Epoch 28/150 | Batch 143/198 | Loss: 1.7943834066390991\n","Epoch 28/150 | Batch 144/198 | Loss: 2.0367774963378906\n","Epoch 28/150 | Batch 145/198 | Loss: 1.7673085927963257\n","Epoch 28/150 | Batch 146/198 | Loss: 1.6833163499832153\n","Epoch 28/150 | Batch 147/198 | Loss: 1.5848698616027832\n","Epoch 28/150 | Batch 148/198 | Loss: 1.9273749589920044\n","Epoch 28/150 | Batch 149/198 | Loss: 2.0121853351593018\n","Epoch 28/150 | Batch 150/198 | Loss: 1.6691783666610718\n","Epoch 28/150 | Batch 151/198 | Loss: 1.8179361820220947\n","Epoch 28/150 | Batch 152/198 | Loss: 1.8489596843719482\n","Epoch 28/150 | Batch 153/198 | Loss: 2.0361828804016113\n","Epoch 28/150 | Batch 154/198 | Loss: 1.9486210346221924\n","Epoch 28/150 | Batch 155/198 | Loss: 2.0798439979553223\n","Epoch 28/150 | Batch 156/198 | Loss: 1.5871542692184448\n","Epoch 28/150 | Batch 157/198 | Loss: 1.9161453247070312\n","Epoch 28/150 | Batch 158/198 | Loss: 1.8577313423156738\n","Epoch 28/150 | Batch 159/198 | Loss: 1.959065556526184\n","Epoch 28/150 | Batch 160/198 | Loss: 1.7222965955734253\n","Epoch 28/150 | Batch 161/198 | Loss: 1.596336007118225\n","Epoch 28/150 | Batch 162/198 | Loss: 1.8769749402999878\n","Epoch 28/150 | Batch 163/198 | Loss: 1.7961766719818115\n","Epoch 28/150 | Batch 164/198 | Loss: 1.7872543334960938\n","Epoch 28/150 | Batch 165/198 | Loss: 1.6796133518218994\n","Epoch 28/150 | Batch 166/198 | Loss: 1.95842707157135\n","Epoch 28/150 | Batch 167/198 | Loss: 1.8805980682373047\n","Epoch 28/150 | Batch 168/198 | Loss: 1.7271555662155151\n","Epoch 28/150 | Batch 169/198 | Loss: 2.016291379928589\n","Epoch 28/150 | Batch 170/198 | Loss: 1.7306870222091675\n","Epoch 28/150 | Batch 171/198 | Loss: 1.9886003732681274\n","Epoch 28/150 | Batch 172/198 | Loss: 1.5917214155197144\n","Epoch 28/150 | Batch 173/198 | Loss: 1.8648483753204346\n","Epoch 28/150 | Batch 174/198 | Loss: 1.6118961572647095\n","Epoch 28/150 | Batch 175/198 | Loss: 1.762797236442566\n","Epoch 28/150 | Batch 176/198 | Loss: 1.685697078704834\n","Epoch 28/150 | Batch 177/198 | Loss: 1.8907215595245361\n","Epoch 28/150 | Batch 178/198 | Loss: 1.6427019834518433\n","Epoch 28/150 | Batch 179/198 | Loss: 1.9002243280410767\n","Epoch 28/150 | Batch 180/198 | Loss: 1.8427984714508057\n","Epoch 28/150 | Batch 181/198 | Loss: 1.7216523885726929\n","Epoch 28/150 | Batch 182/198 | Loss: 1.843724012374878\n","Epoch 28/150 | Batch 183/198 | Loss: 1.8856091499328613\n","Epoch 28/150 | Batch 184/198 | Loss: 1.67633855342865\n","Epoch 28/150 | Batch 185/198 | Loss: 1.9816622734069824\n","Epoch 28/150 | Batch 186/198 | Loss: 1.7741979360580444\n","Epoch 28/150 | Batch 187/198 | Loss: 1.6111891269683838\n","Epoch 28/150 | Batch 188/198 | Loss: 1.7865417003631592\n","Epoch 28/150 | Batch 189/198 | Loss: 2.066526412963867\n","Epoch 28/150 | Batch 190/198 | Loss: 1.5907715559005737\n","Epoch 28/150 | Batch 191/198 | Loss: 1.633969783782959\n","Epoch 28/150 | Batch 192/198 | Loss: 1.6996746063232422\n","Epoch 28/150 | Batch 193/198 | Loss: 1.6925510168075562\n","Epoch 28/150 | Batch 194/198 | Loss: 1.8252793550491333\n","Epoch 28/150 | Batch 195/198 | Loss: 1.6816773414611816\n","Epoch 28/150 | Batch 196/198 | Loss: 1.8734612464904785\n","Epoch 28/150 | Batch 197/198 | Loss: 1.8963193893432617\n","Epoch 28/150 | Batch 198/198 | Loss: 1.7422951459884644\n","Epoch 28/150 | Average Loss: 1.772959089640415\n","Epoch 29/150 | Batch 1/198 | Loss: 1.73365318775177\n","Epoch 29/150 | Batch 2/198 | Loss: 1.8054276704788208\n","Epoch 29/150 | Batch 3/198 | Loss: 1.6082903146743774\n","Epoch 29/150 | Batch 4/198 | Loss: 1.617864966392517\n","Epoch 29/150 | Batch 5/198 | Loss: 1.5870134830474854\n","Epoch 29/150 | Batch 6/198 | Loss: 1.8117092847824097\n","Epoch 29/150 | Batch 7/198 | Loss: 1.4806671142578125\n","Epoch 29/150 | Batch 8/198 | Loss: 1.7777734994888306\n","Epoch 29/150 | Batch 9/198 | Loss: 1.4754815101623535\n","Epoch 29/150 | Batch 10/198 | Loss: 1.6334503889083862\n","Epoch 29/150 | Batch 11/198 | Loss: 1.6804255247116089\n","Epoch 29/150 | Batch 12/198 | Loss: 1.7532576322555542\n","Epoch 29/150 | Batch 13/198 | Loss: 1.7027205228805542\n","Epoch 29/150 | Batch 14/198 | Loss: 1.702177882194519\n","Epoch 29/150 | Batch 15/198 | Loss: 1.5508555173873901\n","Epoch 29/150 | Batch 16/198 | Loss: 2.0774989128112793\n","Epoch 29/150 | Batch 17/198 | Loss: 1.7635637521743774\n","Epoch 29/150 | Batch 18/198 | Loss: 1.9128599166870117\n","Epoch 29/150 | Batch 19/198 | Loss: 1.3561315536499023\n","Epoch 29/150 | Batch 20/198 | Loss: 1.8112330436706543\n","Epoch 29/150 | Batch 21/198 | Loss: 1.6248422861099243\n","Epoch 29/150 | Batch 22/198 | Loss: 1.62871253490448\n","Epoch 29/150 | Batch 23/198 | Loss: 1.7110041379928589\n","Epoch 29/150 | Batch 24/198 | Loss: 1.798418641090393\n","Epoch 29/150 | Batch 25/198 | Loss: 1.7873119115829468\n","Epoch 29/150 | Batch 26/198 | Loss: 1.4363666772842407\n","Epoch 29/150 | Batch 27/198 | Loss: 1.7812860012054443\n","Epoch 29/150 | Batch 28/198 | Loss: 1.8820298910140991\n","Epoch 29/150 | Batch 29/198 | Loss: 1.6841994524002075\n","Epoch 29/150 | Batch 30/198 | Loss: 1.766668438911438\n","Epoch 29/150 | Batch 31/198 | Loss: 1.4610050916671753\n","Epoch 29/150 | Batch 32/198 | Loss: 2.013704299926758\n","Epoch 29/150 | Batch 33/198 | Loss: 1.8320603370666504\n","Epoch 29/150 | Batch 34/198 | Loss: 1.6414458751678467\n","Epoch 29/150 | Batch 35/198 | Loss: 1.8061590194702148\n","Epoch 29/150 | Batch 36/198 | Loss: 1.681209921836853\n","Epoch 29/150 | Batch 37/198 | Loss: 1.7314236164093018\n","Epoch 29/150 | Batch 38/198 | Loss: 1.9093306064605713\n","Epoch 29/150 | Batch 39/198 | Loss: 1.8438383340835571\n","Epoch 29/150 | Batch 40/198 | Loss: 1.684572696685791\n","Epoch 29/150 | Batch 41/198 | Loss: 1.9481511116027832\n","Epoch 29/150 | Batch 42/198 | Loss: 1.6335537433624268\n","Epoch 29/150 | Batch 43/198 | Loss: 1.9561727046966553\n","Epoch 29/150 | Batch 44/198 | Loss: 1.5318169593811035\n","Epoch 29/150 | Batch 45/198 | Loss: 1.984192967414856\n","Epoch 29/150 | Batch 46/198 | Loss: 1.9045344591140747\n","Epoch 29/150 | Batch 47/198 | Loss: 1.5255428552627563\n","Epoch 29/150 | Batch 48/198 | Loss: 1.8069192171096802\n","Epoch 29/150 | Batch 49/198 | Loss: 1.7283581495285034\n","Epoch 29/150 | Batch 50/198 | Loss: 1.7104443311691284\n","Epoch 29/150 | Batch 51/198 | Loss: 1.9396083354949951\n","Epoch 29/150 | Batch 52/198 | Loss: 1.6556302309036255\n","Epoch 29/150 | Batch 53/198 | Loss: 1.5191541910171509\n","Epoch 29/150 | Batch 54/198 | Loss: 1.8345153331756592\n","Epoch 29/150 | Batch 55/198 | Loss: 1.9710956811904907\n","Epoch 29/150 | Batch 56/198 | Loss: 1.7744187116622925\n","Epoch 29/150 | Batch 57/198 | Loss: 1.5892213582992554\n","Epoch 29/150 | Batch 58/198 | Loss: 1.6179052591323853\n","Epoch 29/150 | Batch 59/198 | Loss: 2.054476261138916\n","Epoch 29/150 | Batch 60/198 | Loss: 1.7458449602127075\n","Epoch 29/150 | Batch 61/198 | Loss: 1.7335922718048096\n","Epoch 29/150 | Batch 62/198 | Loss: 1.6999380588531494\n","Epoch 29/150 | Batch 63/198 | Loss: 1.8029301166534424\n","Epoch 29/150 | Batch 64/198 | Loss: 1.7281622886657715\n","Epoch 29/150 | Batch 65/198 | Loss: 1.8837177753448486\n","Epoch 29/150 | Batch 66/198 | Loss: 1.8145558834075928\n","Epoch 29/150 | Batch 67/198 | Loss: 1.5706286430358887\n","Epoch 29/150 | Batch 68/198 | Loss: 1.7132965326309204\n","Epoch 29/150 | Batch 69/198 | Loss: 1.7426888942718506\n","Epoch 29/150 | Batch 70/198 | Loss: 1.6185694932937622\n","Epoch 29/150 | Batch 71/198 | Loss: 1.5849450826644897\n","Epoch 29/150 | Batch 72/198 | Loss: 1.6046370267868042\n","Epoch 29/150 | Batch 73/198 | Loss: 1.547928810119629\n","Epoch 29/150 | Batch 74/198 | Loss: 1.643447995185852\n","Epoch 29/150 | Batch 75/198 | Loss: 1.4763845205307007\n","Epoch 29/150 | Batch 76/198 | Loss: 1.6640124320983887\n","Epoch 29/150 | Batch 77/198 | Loss: 1.5732731819152832\n","Epoch 29/150 | Batch 78/198 | Loss: 1.659132719039917\n","Epoch 29/150 | Batch 79/198 | Loss: 1.8313043117523193\n","Epoch 29/150 | Batch 80/198 | Loss: 1.60468590259552\n","Epoch 29/150 | Batch 81/198 | Loss: 1.6715887784957886\n","Epoch 29/150 | Batch 82/198 | Loss: 1.8315860033035278\n","Epoch 29/150 | Batch 83/198 | Loss: 1.8412141799926758\n","Epoch 29/150 | Batch 84/198 | Loss: 1.8697000741958618\n","Epoch 29/150 | Batch 85/198 | Loss: 1.503835678100586\n","Epoch 29/150 | Batch 86/198 | Loss: 1.4921236038208008\n","Epoch 29/150 | Batch 87/198 | Loss: 1.7733473777770996\n","Epoch 29/150 | Batch 88/198 | Loss: 1.9037150144577026\n","Epoch 29/150 | Batch 89/198 | Loss: 1.4985759258270264\n","Epoch 29/150 | Batch 90/198 | Loss: 1.8742650747299194\n","Epoch 29/150 | Batch 91/198 | Loss: 1.6115235090255737\n","Epoch 29/150 | Batch 92/198 | Loss: 1.8014057874679565\n","Epoch 29/150 | Batch 93/198 | Loss: 1.7417666912078857\n","Epoch 29/150 | Batch 94/198 | Loss: 1.8666080236434937\n","Epoch 29/150 | Batch 95/198 | Loss: 1.8854162693023682\n","Epoch 29/150 | Batch 96/198 | Loss: 1.8419269323349\n","Epoch 29/150 | Batch 97/198 | Loss: 1.9260541200637817\n","Epoch 29/150 | Batch 98/198 | Loss: 1.6674185991287231\n","Epoch 29/150 | Batch 99/198 | Loss: 1.5125963687896729\n","Epoch 29/150 | Batch 100/198 | Loss: 1.8233904838562012\n","Epoch 29/150 | Batch 101/198 | Loss: 1.9653798341751099\n","Epoch 29/150 | Batch 102/198 | Loss: 1.6899040937423706\n","Epoch 29/150 | Batch 103/198 | Loss: 1.9387288093566895\n","Epoch 29/150 | Batch 104/198 | Loss: 1.777242660522461\n","Epoch 29/150 | Batch 105/198 | Loss: 1.8381377458572388\n","Epoch 29/150 | Batch 106/198 | Loss: 1.9234471321105957\n","Epoch 29/150 | Batch 107/198 | Loss: 1.5329875946044922\n","Epoch 29/150 | Batch 108/198 | Loss: 1.8739376068115234\n","Epoch 29/150 | Batch 109/198 | Loss: 1.5399951934814453\n","Epoch 29/150 | Batch 110/198 | Loss: 1.6638944149017334\n","Epoch 29/150 | Batch 111/198 | Loss: 1.9189199209213257\n","Epoch 29/150 | Batch 112/198 | Loss: 1.6985580921173096\n","Epoch 29/150 | Batch 113/198 | Loss: 1.8258699178695679\n","Epoch 29/150 | Batch 114/198 | Loss: 1.7702100276947021\n","Epoch 29/150 | Batch 115/198 | Loss: 1.7095117568969727\n","Epoch 29/150 | Batch 116/198 | Loss: 1.889362096786499\n","Epoch 29/150 | Batch 117/198 | Loss: 2.0311367511749268\n","Epoch 29/150 | Batch 118/198 | Loss: 1.7928013801574707\n","Epoch 29/150 | Batch 119/198 | Loss: 2.068563938140869\n","Epoch 29/150 | Batch 120/198 | Loss: 1.8705605268478394\n","Epoch 29/150 | Batch 121/198 | Loss: 1.734474539756775\n","Epoch 29/150 | Batch 122/198 | Loss: 1.7086976766586304\n","Epoch 29/150 | Batch 123/198 | Loss: 1.6418951749801636\n","Epoch 29/150 | Batch 124/198 | Loss: 1.7545254230499268\n","Epoch 29/150 | Batch 125/198 | Loss: 1.8762154579162598\n","Epoch 29/150 | Batch 126/198 | Loss: 1.5274205207824707\n","Epoch 29/150 | Batch 127/198 | Loss: 1.5802923440933228\n","Epoch 29/150 | Batch 128/198 | Loss: 1.8260575532913208\n","Epoch 29/150 | Batch 129/198 | Loss: 1.79437255859375\n","Epoch 29/150 | Batch 130/198 | Loss: 1.5470229387283325\n","Epoch 29/150 | Batch 131/198 | Loss: 1.6587053537368774\n","Epoch 29/150 | Batch 132/198 | Loss: 1.8414270877838135\n","Epoch 29/150 | Batch 133/198 | Loss: 1.6262465715408325\n","Epoch 29/150 | Batch 134/198 | Loss: 1.7222546339035034\n","Epoch 29/150 | Batch 135/198 | Loss: 1.6224247217178345\n","Epoch 29/150 | Batch 136/198 | Loss: 1.5769344568252563\n","Epoch 29/150 | Batch 137/198 | Loss: 1.7250219583511353\n","Epoch 29/150 | Batch 138/198 | Loss: 1.843796968460083\n","Epoch 29/150 | Batch 139/198 | Loss: 1.5992677211761475\n","Epoch 29/150 | Batch 140/198 | Loss: 1.6380343437194824\n","Epoch 29/150 | Batch 141/198 | Loss: 1.8968275785446167\n","Epoch 29/150 | Batch 142/198 | Loss: 1.7201485633850098\n","Epoch 29/150 | Batch 143/198 | Loss: 1.7961957454681396\n","Epoch 29/150 | Batch 144/198 | Loss: 1.8741428852081299\n","Epoch 29/150 | Batch 145/198 | Loss: 1.7092317342758179\n","Epoch 29/150 | Batch 146/198 | Loss: 1.6495989561080933\n","Epoch 29/150 | Batch 147/198 | Loss: 1.7082327604293823\n","Epoch 29/150 | Batch 148/198 | Loss: 1.8503830432891846\n","Epoch 29/150 | Batch 149/198 | Loss: 1.628343105316162\n","Epoch 29/150 | Batch 150/198 | Loss: 1.765914797782898\n","Epoch 29/150 | Batch 151/198 | Loss: 1.7246947288513184\n","Epoch 29/150 | Batch 152/198 | Loss: 1.806818962097168\n","Epoch 29/150 | Batch 153/198 | Loss: 1.5908712148666382\n","Epoch 29/150 | Batch 154/198 | Loss: 1.7453655004501343\n","Epoch 29/150 | Batch 155/198 | Loss: 1.6907507181167603\n","Epoch 29/150 | Batch 156/198 | Loss: 1.697696566581726\n","Epoch 29/150 | Batch 157/198 | Loss: 1.6735750436782837\n","Epoch 29/150 | Batch 158/198 | Loss: 2.033473491668701\n","Epoch 29/150 | Batch 159/198 | Loss: 2.1776607036590576\n","Epoch 29/150 | Batch 160/198 | Loss: 2.001509189605713\n","Epoch 29/150 | Batch 161/198 | Loss: 1.780531644821167\n","Epoch 29/150 | Batch 162/198 | Loss: 1.6509487628936768\n","Epoch 29/150 | Batch 163/198 | Loss: 1.6639529466629028\n","Epoch 29/150 | Batch 164/198 | Loss: 1.910892367362976\n","Epoch 29/150 | Batch 165/198 | Loss: 1.5048695802688599\n","Epoch 29/150 | Batch 166/198 | Loss: 1.7785136699676514\n","Epoch 29/150 | Batch 167/198 | Loss: 1.6705824136734009\n","Epoch 29/150 | Batch 168/198 | Loss: 1.4759169816970825\n","Epoch 29/150 | Batch 169/198 | Loss: 1.5927163362503052\n","Epoch 29/150 | Batch 170/198 | Loss: 1.7845255136489868\n","Epoch 29/150 | Batch 171/198 | Loss: 1.6105570793151855\n","Epoch 29/150 | Batch 172/198 | Loss: 1.8207212686538696\n","Epoch 29/150 | Batch 173/198 | Loss: 1.699429988861084\n","Epoch 29/150 | Batch 174/198 | Loss: 1.9647873640060425\n","Epoch 29/150 | Batch 175/198 | Loss: 1.6605802774429321\n","Epoch 29/150 | Batch 176/198 | Loss: 1.5548605918884277\n","Epoch 29/150 | Batch 177/198 | Loss: 1.6119600534439087\n","Epoch 29/150 | Batch 178/198 | Loss: 1.6945809125900269\n","Epoch 29/150 | Batch 179/198 | Loss: 1.7391451597213745\n","Epoch 29/150 | Batch 180/198 | Loss: 1.654833436012268\n","Epoch 29/150 | Batch 181/198 | Loss: 1.7574355602264404\n","Epoch 29/150 | Batch 182/198 | Loss: 1.8953553438186646\n","Epoch 29/150 | Batch 183/198 | Loss: 1.7713114023208618\n","Epoch 29/150 | Batch 184/198 | Loss: 1.7101929187774658\n","Epoch 29/150 | Batch 185/198 | Loss: 1.7500978708267212\n","Epoch 29/150 | Batch 186/198 | Loss: 1.9081752300262451\n","Epoch 29/150 | Batch 187/198 | Loss: 1.6084047555923462\n","Epoch 29/150 | Batch 188/198 | Loss: 1.6414798498153687\n","Epoch 29/150 | Batch 189/198 | Loss: 1.711538314819336\n","Epoch 29/150 | Batch 190/198 | Loss: 1.6536524295806885\n","Epoch 29/150 | Batch 191/198 | Loss: 1.919778823852539\n","Epoch 29/150 | Batch 192/198 | Loss: 1.618941307067871\n","Epoch 29/150 | Batch 193/198 | Loss: 1.9592727422714233\n","Epoch 29/150 | Batch 194/198 | Loss: 2.0794730186462402\n","Epoch 29/150 | Batch 195/198 | Loss: 1.6093324422836304\n","Epoch 29/150 | Batch 196/198 | Loss: 1.9742134809494019\n","Epoch 29/150 | Batch 197/198 | Loss: 1.6187747716903687\n","Epoch 29/150 | Batch 198/198 | Loss: 1.9524060487747192\n","Epoch 29/150 | Average Loss: 1.7388156977566807\n","Epoch 30/150 | Batch 1/198 | Loss: 1.505060076713562\n","Epoch 30/150 | Batch 2/198 | Loss: 1.7907357215881348\n","Epoch 30/150 | Batch 3/198 | Loss: 1.9550883769989014\n","Epoch 30/150 | Batch 4/198 | Loss: 1.7785712480545044\n","Epoch 30/150 | Batch 5/198 | Loss: 1.58079993724823\n","Epoch 30/150 | Batch 6/198 | Loss: 1.721653938293457\n","Epoch 30/150 | Batch 7/198 | Loss: 1.6761689186096191\n","Epoch 30/150 | Batch 8/198 | Loss: 1.6716864109039307\n","Epoch 30/150 | Batch 9/198 | Loss: 1.774784803390503\n","Epoch 30/150 | Batch 10/198 | Loss: 1.6146198511123657\n","Epoch 30/150 | Batch 11/198 | Loss: 1.7564043998718262\n","Epoch 30/150 | Batch 12/198 | Loss: 1.7612336874008179\n","Epoch 30/150 | Batch 13/198 | Loss: 1.7414697408676147\n","Epoch 30/150 | Batch 14/198 | Loss: 1.6880981922149658\n","Epoch 30/150 | Batch 15/198 | Loss: 1.7987288236618042\n","Epoch 30/150 | Batch 16/198 | Loss: 1.8186116218566895\n","Epoch 30/150 | Batch 17/198 | Loss: 1.9471361637115479\n","Epoch 30/150 | Batch 18/198 | Loss: 1.8258872032165527\n","Epoch 30/150 | Batch 19/198 | Loss: 1.9036632776260376\n","Epoch 30/150 | Batch 20/198 | Loss: 1.4930981397628784\n","Epoch 30/150 | Batch 21/198 | Loss: 1.5903512239456177\n","Epoch 30/150 | Batch 22/198 | Loss: 1.6748226881027222\n","Epoch 30/150 | Batch 23/198 | Loss: 1.8266665935516357\n","Epoch 30/150 | Batch 24/198 | Loss: 1.7182146310806274\n","Epoch 30/150 | Batch 25/198 | Loss: 1.8573302030563354\n","Epoch 30/150 | Batch 26/198 | Loss: 1.7360700368881226\n","Epoch 30/150 | Batch 27/198 | Loss: 1.7786710262298584\n","Epoch 30/150 | Batch 28/198 | Loss: 1.6468347311019897\n","Epoch 30/150 | Batch 29/198 | Loss: 1.4716967344284058\n","Epoch 30/150 | Batch 30/198 | Loss: 1.6051735877990723\n","Epoch 30/150 | Batch 31/198 | Loss: 1.6232659816741943\n","Epoch 30/150 | Batch 32/198 | Loss: 1.481863021850586\n","Epoch 30/150 | Batch 33/198 | Loss: 1.4888592958450317\n","Epoch 30/150 | Batch 34/198 | Loss: 1.5816441774368286\n","Epoch 30/150 | Batch 35/198 | Loss: 1.5910576581954956\n","Epoch 30/150 | Batch 36/198 | Loss: 1.731449842453003\n","Epoch 30/150 | Batch 37/198 | Loss: 1.7842856645584106\n","Epoch 30/150 | Batch 38/198 | Loss: 1.783281922340393\n","Epoch 30/150 | Batch 39/198 | Loss: 1.8968884944915771\n","Epoch 30/150 | Batch 40/198 | Loss: 1.7702817916870117\n","Epoch 30/150 | Batch 41/198 | Loss: 1.8321489095687866\n","Epoch 30/150 | Batch 42/198 | Loss: 1.936295509338379\n","Epoch 30/150 | Batch 43/198 | Loss: 1.819234013557434\n","Epoch 30/150 | Batch 44/198 | Loss: 1.59490168094635\n","Epoch 30/150 | Batch 45/198 | Loss: 1.6649014949798584\n","Epoch 30/150 | Batch 46/198 | Loss: 1.5154352188110352\n","Epoch 30/150 | Batch 47/198 | Loss: 1.8624272346496582\n","Epoch 30/150 | Batch 48/198 | Loss: 1.7810490131378174\n","Epoch 30/150 | Batch 49/198 | Loss: 1.5180004835128784\n","Epoch 30/150 | Batch 50/198 | Loss: 1.6168440580368042\n","Epoch 30/150 | Batch 51/198 | Loss: 1.9083385467529297\n","Epoch 30/150 | Batch 52/198 | Loss: 1.8357797861099243\n","Epoch 30/150 | Batch 53/198 | Loss: 1.7310149669647217\n","Epoch 30/150 | Batch 54/198 | Loss: 1.5593633651733398\n","Epoch 30/150 | Batch 55/198 | Loss: 1.7759352922439575\n","Epoch 30/150 | Batch 56/198 | Loss: 1.707886815071106\n","Epoch 30/150 | Batch 57/198 | Loss: 1.8153736591339111\n","Epoch 30/150 | Batch 58/198 | Loss: 1.6897883415222168\n","Epoch 30/150 | Batch 59/198 | Loss: 1.6998438835144043\n","Epoch 30/150 | Batch 60/198 | Loss: 1.5581852197647095\n","Epoch 30/150 | Batch 61/198 | Loss: 1.7833342552185059\n","Epoch 30/150 | Batch 62/198 | Loss: 1.6636743545532227\n","Epoch 30/150 | Batch 63/198 | Loss: 1.5816861391067505\n","Epoch 30/150 | Batch 64/198 | Loss: 2.040905237197876\n","Epoch 30/150 | Batch 65/198 | Loss: 1.805172324180603\n","Epoch 30/150 | Batch 66/198 | Loss: 1.8173506259918213\n","Epoch 30/150 | Batch 67/198 | Loss: 1.955177903175354\n","Epoch 30/150 | Batch 68/198 | Loss: 1.9414827823638916\n","Epoch 30/150 | Batch 69/198 | Loss: 1.746445894241333\n","Epoch 30/150 | Batch 70/198 | Loss: 1.7773137092590332\n","Epoch 30/150 | Batch 71/198 | Loss: 1.7378103733062744\n","Epoch 30/150 | Batch 72/198 | Loss: 1.7032992839813232\n","Epoch 30/150 | Batch 73/198 | Loss: 1.5109598636627197\n","Epoch 30/150 | Batch 74/198 | Loss: 1.5464756488800049\n","Epoch 30/150 | Batch 75/198 | Loss: 1.6420565843582153\n","Epoch 30/150 | Batch 76/198 | Loss: 1.732143759727478\n","Epoch 30/150 | Batch 77/198 | Loss: 1.7425333261489868\n","Epoch 30/150 | Batch 78/198 | Loss: 1.7165067195892334\n","Epoch 30/150 | Batch 79/198 | Loss: 1.592357873916626\n","Epoch 30/150 | Batch 80/198 | Loss: 1.7009795904159546\n","Epoch 30/150 | Batch 81/198 | Loss: 1.7759501934051514\n","Epoch 30/150 | Batch 82/198 | Loss: 1.3934972286224365\n","Epoch 30/150 | Batch 83/198 | Loss: 1.8310976028442383\n","Epoch 30/150 | Batch 84/198 | Loss: 1.735019326210022\n","Epoch 30/150 | Batch 85/198 | Loss: 1.860548973083496\n","Epoch 30/150 | Batch 86/198 | Loss: 1.6051273345947266\n","Epoch 30/150 | Batch 87/198 | Loss: 1.467623233795166\n","Epoch 30/150 | Batch 88/198 | Loss: 1.527220368385315\n","Epoch 30/150 | Batch 89/198 | Loss: 1.6401838064193726\n","Epoch 30/150 | Batch 90/198 | Loss: 1.8433195352554321\n","Epoch 30/150 | Batch 91/198 | Loss: 1.879459023475647\n","Epoch 30/150 | Batch 92/198 | Loss: 1.5636729001998901\n","Epoch 30/150 | Batch 93/198 | Loss: 1.8243839740753174\n","Epoch 30/150 | Batch 94/198 | Loss: 1.6781961917877197\n","Epoch 30/150 | Batch 95/198 | Loss: 1.6178168058395386\n","Epoch 30/150 | Batch 96/198 | Loss: 1.7453356981277466\n","Epoch 30/150 | Batch 97/198 | Loss: 1.7355215549468994\n","Epoch 30/150 | Batch 98/198 | Loss: 1.9985430240631104\n","Epoch 30/150 | Batch 99/198 | Loss: 1.4864481687545776\n","Epoch 30/150 | Batch 100/198 | Loss: 1.7134878635406494\n","Epoch 30/150 | Batch 101/198 | Loss: 1.8285787105560303\n","Epoch 30/150 | Batch 102/198 | Loss: 1.5039583444595337\n","Epoch 30/150 | Batch 103/198 | Loss: 1.7514711618423462\n","Epoch 30/150 | Batch 104/198 | Loss: 1.7740352153778076\n","Epoch 30/150 | Batch 105/198 | Loss: 1.4700357913970947\n","Epoch 30/150 | Batch 106/198 | Loss: 1.546251893043518\n","Epoch 30/150 | Batch 107/198 | Loss: 1.5983304977416992\n","Epoch 30/150 | Batch 108/198 | Loss: 1.6201955080032349\n","Epoch 30/150 | Batch 109/198 | Loss: 1.7880991697311401\n","Epoch 30/150 | Batch 110/198 | Loss: 1.8473401069641113\n","Epoch 30/150 | Batch 111/198 | Loss: 1.825807809829712\n","Epoch 30/150 | Batch 112/198 | Loss: 1.7127002477645874\n","Epoch 30/150 | Batch 113/198 | Loss: 1.7804498672485352\n","Epoch 30/150 | Batch 114/198 | Loss: 1.6611863374710083\n","Epoch 30/150 | Batch 115/198 | Loss: 2.0622804164886475\n","Epoch 30/150 | Batch 116/198 | Loss: 1.6663663387298584\n","Epoch 30/150 | Batch 117/198 | Loss: 1.6361963748931885\n","Epoch 30/150 | Batch 118/198 | Loss: 1.8838170766830444\n","Epoch 30/150 | Batch 119/198 | Loss: 1.2117053270339966\n","Epoch 30/150 | Batch 120/198 | Loss: 1.797284483909607\n","Epoch 30/150 | Batch 121/198 | Loss: 1.6207685470581055\n","Epoch 30/150 | Batch 122/198 | Loss: 1.6502727270126343\n","Epoch 30/150 | Batch 123/198 | Loss: 1.8194987773895264\n","Epoch 30/150 | Batch 124/198 | Loss: 1.6936994791030884\n","Epoch 30/150 | Batch 125/198 | Loss: 1.889716625213623\n","Epoch 30/150 | Batch 126/198 | Loss: 1.771515965461731\n","Epoch 30/150 | Batch 127/198 | Loss: 1.5720160007476807\n","Epoch 30/150 | Batch 128/198 | Loss: 1.6568914651870728\n","Epoch 30/150 | Batch 129/198 | Loss: 1.6474074125289917\n","Epoch 30/150 | Batch 130/198 | Loss: 1.8033547401428223\n","Epoch 30/150 | Batch 131/198 | Loss: 1.793455719947815\n","Epoch 30/150 | Batch 132/198 | Loss: 1.6637405157089233\n","Epoch 30/150 | Batch 133/198 | Loss: 1.5720057487487793\n","Epoch 30/150 | Batch 134/198 | Loss: 1.4765573740005493\n","Epoch 30/150 | Batch 135/198 | Loss: 1.5731801986694336\n","Epoch 30/150 | Batch 136/198 | Loss: 1.7379343509674072\n","Epoch 30/150 | Batch 137/198 | Loss: 1.427764892578125\n","Epoch 30/150 | Batch 138/198 | Loss: 1.7522116899490356\n","Epoch 30/150 | Batch 139/198 | Loss: 1.655286192893982\n","Epoch 30/150 | Batch 140/198 | Loss: 1.7418959140777588\n","Epoch 30/150 | Batch 141/198 | Loss: 1.6453241109848022\n","Epoch 30/150 | Batch 142/198 | Loss: 1.6892634630203247\n","Epoch 30/150 | Batch 143/198 | Loss: 1.6983587741851807\n","Epoch 30/150 | Batch 144/198 | Loss: 1.4682832956314087\n","Epoch 30/150 | Batch 145/198 | Loss: 1.5920672416687012\n","Epoch 30/150 | Batch 146/198 | Loss: 1.6256760358810425\n","Epoch 30/150 | Batch 147/198 | Loss: 1.968163013458252\n","Epoch 30/150 | Batch 148/198 | Loss: 1.7754355669021606\n","Epoch 30/150 | Batch 149/198 | Loss: 1.7125552892684937\n","Epoch 30/150 | Batch 150/198 | Loss: 1.8852874040603638\n","Epoch 30/150 | Batch 151/198 | Loss: 1.6580530405044556\n","Epoch 30/150 | Batch 152/198 | Loss: 1.929485559463501\n","Epoch 30/150 | Batch 153/198 | Loss: 1.8964869976043701\n","Epoch 30/150 | Batch 154/198 | Loss: 1.6093860864639282\n","Epoch 30/150 | Batch 155/198 | Loss: 1.6606954336166382\n","Epoch 30/150 | Batch 156/198 | Loss: 1.7348521947860718\n","Epoch 30/150 | Batch 157/198 | Loss: 1.6898332834243774\n","Epoch 30/150 | Batch 158/198 | Loss: 1.6578938961029053\n","Epoch 30/150 | Batch 159/198 | Loss: 1.7358829975128174\n","Epoch 30/150 | Batch 160/198 | Loss: 1.4848828315734863\n","Epoch 30/150 | Batch 161/198 | Loss: 1.6094927787780762\n","Epoch 30/150 | Batch 162/198 | Loss: 1.8419686555862427\n","Epoch 30/150 | Batch 163/198 | Loss: 1.6559338569641113\n","Epoch 30/150 | Batch 164/198 | Loss: 1.6051805019378662\n","Epoch 30/150 | Batch 165/198 | Loss: 1.8572938442230225\n","Epoch 30/150 | Batch 166/198 | Loss: 1.8354467153549194\n","Epoch 30/150 | Batch 167/198 | Loss: 1.5628727674484253\n","Epoch 30/150 | Batch 168/198 | Loss: 1.832277774810791\n","Epoch 30/150 | Batch 169/198 | Loss: 1.6887767314910889\n","Epoch 30/150 | Batch 170/198 | Loss: 1.525118112564087\n","Epoch 30/150 | Batch 171/198 | Loss: 1.5699158906936646\n","Epoch 30/150 | Batch 172/198 | Loss: 1.7787402868270874\n","Epoch 30/150 | Batch 173/198 | Loss: 1.6797977685928345\n","Epoch 30/150 | Batch 174/198 | Loss: 1.870840072631836\n","Epoch 30/150 | Batch 175/198 | Loss: 1.6513798236846924\n","Epoch 30/150 | Batch 176/198 | Loss: 1.5069063901901245\n","Epoch 30/150 | Batch 177/198 | Loss: 1.6290582418441772\n","Epoch 30/150 | Batch 178/198 | Loss: 1.87796950340271\n","Epoch 30/150 | Batch 179/198 | Loss: 1.7557647228240967\n","Epoch 30/150 | Batch 180/198 | Loss: 1.5836244821548462\n","Epoch 30/150 | Batch 181/198 | Loss: 1.8832128047943115\n","Epoch 30/150 | Batch 182/198 | Loss: 1.9024194478988647\n","Epoch 30/150 | Batch 183/198 | Loss: 1.5933964252471924\n","Epoch 30/150 | Batch 184/198 | Loss: 1.7565107345581055\n","Epoch 30/150 | Batch 185/198 | Loss: 1.5345326662063599\n","Epoch 30/150 | Batch 186/198 | Loss: 1.8227545022964478\n","Epoch 30/150 | Batch 187/198 | Loss: 1.853111743927002\n","Epoch 30/150 | Batch 188/198 | Loss: 1.668222188949585\n","Epoch 30/150 | Batch 189/198 | Loss: 1.908402681350708\n","Epoch 30/150 | Batch 190/198 | Loss: 1.5242277383804321\n","Epoch 30/150 | Batch 191/198 | Loss: 1.7700047492980957\n","Epoch 30/150 | Batch 192/198 | Loss: 1.834986686706543\n","Epoch 30/150 | Batch 193/198 | Loss: 1.6246026754379272\n","Epoch 30/150 | Batch 194/198 | Loss: 1.7711795568466187\n","Epoch 30/150 | Batch 195/198 | Loss: 1.6635891199111938\n","Epoch 30/150 | Batch 196/198 | Loss: 1.6777094602584839\n","Epoch 30/150 | Batch 197/198 | Loss: 1.7283843755722046\n","Epoch 30/150 | Batch 198/198 | Loss: 1.497451663017273\n","Epoch 30/150 | Average Loss: 1.7073801316396156\n","Epoch 31/150 | Batch 1/198 | Loss: 1.7398431301116943\n","Epoch 31/150 | Batch 2/198 | Loss: 1.7054471969604492\n","Epoch 31/150 | Batch 3/198 | Loss: 1.6206258535385132\n","Epoch 31/150 | Batch 4/198 | Loss: 1.8935576677322388\n","Epoch 31/150 | Batch 5/198 | Loss: 1.8006277084350586\n","Epoch 31/150 | Batch 6/198 | Loss: 1.6999108791351318\n","Epoch 31/150 | Batch 7/198 | Loss: 1.7544798851013184\n","Epoch 31/150 | Batch 8/198 | Loss: 1.7470190525054932\n","Epoch 31/150 | Batch 9/198 | Loss: 1.5680880546569824\n","Epoch 31/150 | Batch 10/198 | Loss: 1.7089985609054565\n","Epoch 31/150 | Batch 11/198 | Loss: 1.6657801866531372\n","Epoch 31/150 | Batch 12/198 | Loss: 1.5823335647583008\n","Epoch 31/150 | Batch 13/198 | Loss: 1.36909818649292\n","Epoch 31/150 | Batch 14/198 | Loss: 1.571260690689087\n","Epoch 31/150 | Batch 15/198 | Loss: 1.8308765888214111\n","Epoch 31/150 | Batch 16/198 | Loss: 1.8268872499465942\n","Epoch 31/150 | Batch 17/198 | Loss: 1.830006718635559\n","Epoch 31/150 | Batch 18/198 | Loss: 1.4065197706222534\n","Epoch 31/150 | Batch 19/198 | Loss: 1.6775844097137451\n","Epoch 31/150 | Batch 20/198 | Loss: 1.6724640130996704\n","Epoch 31/150 | Batch 21/198 | Loss: 1.6651859283447266\n","Epoch 31/150 | Batch 22/198 | Loss: 1.898499608039856\n","Epoch 31/150 | Batch 23/198 | Loss: 1.685816764831543\n","Epoch 31/150 | Batch 24/198 | Loss: 1.7128076553344727\n","Epoch 31/150 | Batch 25/198 | Loss: 1.5945165157318115\n","Epoch 31/150 | Batch 26/198 | Loss: 1.5253137350082397\n","Epoch 31/150 | Batch 27/198 | Loss: 1.6324583292007446\n","Epoch 31/150 | Batch 28/198 | Loss: 1.6049113273620605\n","Epoch 31/150 | Batch 29/198 | Loss: 1.6279728412628174\n","Epoch 31/150 | Batch 30/198 | Loss: 1.648438811302185\n","Epoch 31/150 | Batch 31/198 | Loss: 1.9946507215499878\n","Epoch 31/150 | Batch 32/198 | Loss: 1.6887437105178833\n","Epoch 31/150 | Batch 33/198 | Loss: 1.8624449968338013\n","Epoch 31/150 | Batch 34/198 | Loss: 1.6854528188705444\n","Epoch 31/150 | Batch 35/198 | Loss: 1.7218778133392334\n","Epoch 31/150 | Batch 36/198 | Loss: 1.727400541305542\n","Epoch 31/150 | Batch 37/198 | Loss: 1.8282501697540283\n","Epoch 31/150 | Batch 38/198 | Loss: 1.3913969993591309\n","Epoch 31/150 | Batch 39/198 | Loss: 1.5344820022583008\n","Epoch 31/150 | Batch 40/198 | Loss: 1.7215453386306763\n","Epoch 31/150 | Batch 41/198 | Loss: 1.629516839981079\n","Epoch 31/150 | Batch 42/198 | Loss: 1.6836209297180176\n","Epoch 31/150 | Batch 43/198 | Loss: 1.7932331562042236\n","Epoch 31/150 | Batch 44/198 | Loss: 1.7184207439422607\n","Epoch 31/150 | Batch 45/198 | Loss: 1.5581027269363403\n","Epoch 31/150 | Batch 46/198 | Loss: 1.532413363456726\n","Epoch 31/150 | Batch 47/198 | Loss: 1.4882274866104126\n","Epoch 31/150 | Batch 48/198 | Loss: 1.5165399312973022\n","Epoch 31/150 | Batch 49/198 | Loss: 1.9543670415878296\n","Epoch 31/150 | Batch 50/198 | Loss: 1.5000405311584473\n","Epoch 31/150 | Batch 51/198 | Loss: 1.633002758026123\n","Epoch 31/150 | Batch 52/198 | Loss: 1.734157919883728\n","Epoch 31/150 | Batch 53/198 | Loss: 1.5513463020324707\n","Epoch 31/150 | Batch 54/198 | Loss: 1.6095281839370728\n","Epoch 31/150 | Batch 55/198 | Loss: 1.6674585342407227\n","Epoch 31/150 | Batch 56/198 | Loss: 1.6793938875198364\n","Epoch 31/150 | Batch 57/198 | Loss: 1.570938229560852\n","Epoch 31/150 | Batch 58/198 | Loss: 1.7034389972686768\n","Epoch 31/150 | Batch 59/198 | Loss: 1.5186611413955688\n","Epoch 31/150 | Batch 60/198 | Loss: 1.5814019441604614\n","Epoch 31/150 | Batch 61/198 | Loss: 1.8860273361206055\n","Epoch 31/150 | Batch 62/198 | Loss: 2.0106313228607178\n","Epoch 31/150 | Batch 63/198 | Loss: 1.4374327659606934\n","Epoch 31/150 | Batch 64/198 | Loss: 1.7778409719467163\n","Epoch 31/150 | Batch 65/198 | Loss: 1.603348970413208\n","Epoch 31/150 | Batch 66/198 | Loss: 1.7375388145446777\n","Epoch 31/150 | Batch 67/198 | Loss: 1.5902668237686157\n","Epoch 31/150 | Batch 68/198 | Loss: 1.792258858680725\n","Epoch 31/150 | Batch 69/198 | Loss: 1.6569715738296509\n","Epoch 31/150 | Batch 70/198 | Loss: 1.702003002166748\n","Epoch 31/150 | Batch 71/198 | Loss: 1.733328938484192\n","Epoch 31/150 | Batch 72/198 | Loss: 1.7821534872055054\n","Epoch 31/150 | Batch 73/198 | Loss: 1.677370548248291\n","Epoch 31/150 | Batch 74/198 | Loss: 1.72398042678833\n","Epoch 31/150 | Batch 75/198 | Loss: 1.645119309425354\n","Epoch 31/150 | Batch 76/198 | Loss: 1.6982377767562866\n","Epoch 31/150 | Batch 77/198 | Loss: 1.9976394176483154\n","Epoch 31/150 | Batch 78/198 | Loss: 1.80863618850708\n","Epoch 31/150 | Batch 79/198 | Loss: 1.6151976585388184\n","Epoch 31/150 | Batch 80/198 | Loss: 1.6913875341415405\n","Epoch 31/150 | Batch 81/198 | Loss: 1.4462406635284424\n","Epoch 31/150 | Batch 82/198 | Loss: 1.872090458869934\n","Epoch 31/150 | Batch 83/198 | Loss: 1.8501763343811035\n","Epoch 31/150 | Batch 84/198 | Loss: 1.4903291463851929\n","Epoch 31/150 | Batch 85/198 | Loss: 1.7411986589431763\n","Epoch 31/150 | Batch 86/198 | Loss: 1.8050129413604736\n","Epoch 31/150 | Batch 87/198 | Loss: 1.6112357378005981\n","Epoch 31/150 | Batch 88/198 | Loss: 1.752974510192871\n","Epoch 31/150 | Batch 89/198 | Loss: 1.2297717332839966\n","Epoch 31/150 | Batch 90/198 | Loss: 1.4846264123916626\n","Epoch 31/150 | Batch 91/198 | Loss: 1.6341840028762817\n","Epoch 31/150 | Batch 92/198 | Loss: 1.769951343536377\n","Epoch 31/150 | Batch 93/198 | Loss: 1.637595534324646\n","Epoch 31/150 | Batch 94/198 | Loss: 1.6704894304275513\n","Epoch 31/150 | Batch 95/198 | Loss: 1.821601390838623\n","Epoch 31/150 | Batch 96/198 | Loss: 1.5789433717727661\n","Epoch 31/150 | Batch 97/198 | Loss: 1.6745221614837646\n","Epoch 31/150 | Batch 98/198 | Loss: 1.7213937044143677\n","Epoch 31/150 | Batch 99/198 | Loss: 1.613054871559143\n","Epoch 31/150 | Batch 100/198 | Loss: 1.7878180742263794\n","Epoch 31/150 | Batch 101/198 | Loss: 1.4970368146896362\n","Epoch 31/150 | Batch 102/198 | Loss: 1.7598415613174438\n","Epoch 31/150 | Batch 103/198 | Loss: 1.699796438217163\n","Epoch 31/150 | Batch 104/198 | Loss: 1.545050024986267\n","Epoch 31/150 | Batch 105/198 | Loss: 1.7106643915176392\n","Epoch 31/150 | Batch 106/198 | Loss: 1.7412220239639282\n","Epoch 31/150 | Batch 107/198 | Loss: 1.5606883764266968\n","Epoch 31/150 | Batch 108/198 | Loss: 1.7186734676361084\n","Epoch 31/150 | Batch 109/198 | Loss: 1.9683901071548462\n","Epoch 31/150 | Batch 110/198 | Loss: 1.7515020370483398\n","Epoch 31/150 | Batch 111/198 | Loss: 1.8132787942886353\n","Epoch 31/150 | Batch 112/198 | Loss: 1.3388009071350098\n","Epoch 31/150 | Batch 113/198 | Loss: 1.7875354290008545\n","Epoch 31/150 | Batch 114/198 | Loss: 1.6747792959213257\n","Epoch 31/150 | Batch 115/198 | Loss: 1.5652681589126587\n","Epoch 31/150 | Batch 116/198 | Loss: 1.6337268352508545\n","Epoch 31/150 | Batch 117/198 | Loss: 1.5509933233261108\n","Epoch 31/150 | Batch 118/198 | Loss: 1.8001378774642944\n","Epoch 31/150 | Batch 119/198 | Loss: 1.505312442779541\n","Epoch 31/150 | Batch 120/198 | Loss: 1.586837887763977\n","Epoch 31/150 | Batch 121/198 | Loss: 1.9039292335510254\n","Epoch 31/150 | Batch 122/198 | Loss: 1.795452356338501\n","Epoch 31/150 | Batch 123/198 | Loss: 1.693694829940796\n","Epoch 31/150 | Batch 124/198 | Loss: 1.5264936685562134\n","Epoch 31/150 | Batch 125/198 | Loss: 1.7770851850509644\n","Epoch 31/150 | Batch 126/198 | Loss: 1.9081745147705078\n","Epoch 31/150 | Batch 127/198 | Loss: 1.444109559059143\n","Epoch 31/150 | Batch 128/198 | Loss: 1.6468654870986938\n","Epoch 31/150 | Batch 129/198 | Loss: 1.6866956949234009\n","Epoch 31/150 | Batch 130/198 | Loss: 1.7748972177505493\n","Epoch 31/150 | Batch 131/198 | Loss: 1.6756089925765991\n","Epoch 31/150 | Batch 132/198 | Loss: 1.603615641593933\n","Epoch 31/150 | Batch 133/198 | Loss: 1.6976897716522217\n","Epoch 31/150 | Batch 134/198 | Loss: 1.4089710712432861\n","Epoch 31/150 | Batch 135/198 | Loss: 1.5704580545425415\n","Epoch 31/150 | Batch 136/198 | Loss: 1.6149410009384155\n","Epoch 31/150 | Batch 137/198 | Loss: 1.5515004396438599\n","Epoch 31/150 | Batch 138/198 | Loss: 1.628605842590332\n","Epoch 31/150 | Batch 139/198 | Loss: 1.6268677711486816\n","Epoch 31/150 | Batch 140/198 | Loss: 1.6410378217697144\n","Epoch 31/150 | Batch 141/198 | Loss: 1.5123614072799683\n","Epoch 31/150 | Batch 142/198 | Loss: 1.76189386844635\n","Epoch 31/150 | Batch 143/198 | Loss: 1.5451380014419556\n","Epoch 31/150 | Batch 144/198 | Loss: 1.787454605102539\n","Epoch 31/150 | Batch 145/198 | Loss: 1.7389805316925049\n","Epoch 31/150 | Batch 146/198 | Loss: 1.7775760889053345\n","Epoch 31/150 | Batch 147/198 | Loss: 1.5638291835784912\n","Epoch 31/150 | Batch 148/198 | Loss: 1.7605040073394775\n","Epoch 31/150 | Batch 149/198 | Loss: 1.5650557279586792\n","Epoch 31/150 | Batch 150/198 | Loss: 2.0187251567840576\n","Epoch 31/150 | Batch 151/198 | Loss: 1.7303446531295776\n","Epoch 31/150 | Batch 152/198 | Loss: 1.6683577299118042\n","Epoch 31/150 | Batch 153/198 | Loss: 1.5459775924682617\n","Epoch 31/150 | Batch 154/198 | Loss: 1.5030791759490967\n","Epoch 31/150 | Batch 155/198 | Loss: 1.6663315296173096\n","Epoch 31/150 | Batch 156/198 | Loss: 1.5892282724380493\n","Epoch 31/150 | Batch 157/198 | Loss: 1.8521056175231934\n","Epoch 31/150 | Batch 158/198 | Loss: 1.3855098485946655\n","Epoch 31/150 | Batch 159/198 | Loss: 1.674363613128662\n","Epoch 31/150 | Batch 160/198 | Loss: 1.7155053615570068\n","Epoch 31/150 | Batch 161/198 | Loss: 1.792236089706421\n","Epoch 31/150 | Batch 162/198 | Loss: 1.677162528038025\n","Epoch 31/150 | Batch 163/198 | Loss: 1.5653454065322876\n","Epoch 31/150 | Batch 164/198 | Loss: 1.5151063203811646\n","Epoch 31/150 | Batch 165/198 | Loss: 1.757792353630066\n","Epoch 31/150 | Batch 166/198 | Loss: 1.610464096069336\n","Epoch 31/150 | Batch 167/198 | Loss: 1.7652510404586792\n","Epoch 31/150 | Batch 168/198 | Loss: 1.8194469213485718\n","Epoch 31/150 | Batch 169/198 | Loss: 1.547735571861267\n","Epoch 31/150 | Batch 170/198 | Loss: 1.7517050504684448\n","Epoch 31/150 | Batch 171/198 | Loss: 1.7454771995544434\n","Epoch 31/150 | Batch 172/198 | Loss: 1.7987048625946045\n","Epoch 31/150 | Batch 173/198 | Loss: 1.7429180145263672\n","Epoch 31/150 | Batch 174/198 | Loss: 1.550166130065918\n","Epoch 31/150 | Batch 175/198 | Loss: 1.4848229885101318\n","Epoch 31/150 | Batch 176/198 | Loss: 1.7010438442230225\n","Epoch 31/150 | Batch 177/198 | Loss: 1.7480015754699707\n","Epoch 31/150 | Batch 178/198 | Loss: 1.9769865274429321\n","Epoch 31/150 | Batch 179/198 | Loss: 1.8022381067276\n","Epoch 31/150 | Batch 180/198 | Loss: 1.7771693468093872\n","Epoch 31/150 | Batch 181/198 | Loss: 1.9690245389938354\n","Epoch 31/150 | Batch 182/198 | Loss: 1.668045163154602\n","Epoch 31/150 | Batch 183/198 | Loss: 1.497162103652954\n","Epoch 31/150 | Batch 184/198 | Loss: 1.6781549453735352\n","Epoch 31/150 | Batch 185/198 | Loss: 1.7666000127792358\n","Epoch 31/150 | Batch 186/198 | Loss: 1.7403823137283325\n","Epoch 31/150 | Batch 187/198 | Loss: 1.8563414812088013\n","Epoch 31/150 | Batch 188/198 | Loss: 1.4479882717132568\n","Epoch 31/150 | Batch 189/198 | Loss: 1.5999963283538818\n","Epoch 31/150 | Batch 190/198 | Loss: 1.579435110092163\n","Epoch 31/150 | Batch 191/198 | Loss: 1.7414121627807617\n","Epoch 31/150 | Batch 192/198 | Loss: 1.590634822845459\n","Epoch 31/150 | Batch 193/198 | Loss: 1.812345266342163\n","Epoch 31/150 | Batch 194/198 | Loss: 1.9844021797180176\n","Epoch 31/150 | Batch 195/198 | Loss: 1.4493616819381714\n","Epoch 31/150 | Batch 196/198 | Loss: 1.7809197902679443\n","Epoch 31/150 | Batch 197/198 | Loss: 1.8430860042572021\n","Epoch 31/150 | Batch 198/198 | Loss: 1.9368704557418823\n","Epoch 31/150 | Average Loss: 1.6799417059830946\n","Epoch 32/150 | Batch 1/198 | Loss: 1.7511740922927856\n","Epoch 32/150 | Batch 2/198 | Loss: 1.686812400817871\n","Epoch 32/150 | Batch 3/198 | Loss: 1.6897597312927246\n","Epoch 32/150 | Batch 4/198 | Loss: 1.5619237422943115\n","Epoch 32/150 | Batch 5/198 | Loss: 1.7829214334487915\n","Epoch 32/150 | Batch 6/198 | Loss: 1.815635085105896\n","Epoch 32/150 | Batch 7/198 | Loss: 1.5884495973587036\n","Epoch 32/150 | Batch 8/198 | Loss: 1.7087815999984741\n","Epoch 32/150 | Batch 9/198 | Loss: 1.3452883958816528\n","Epoch 32/150 | Batch 10/198 | Loss: 1.744485855102539\n","Epoch 32/150 | Batch 11/198 | Loss: 1.6794174909591675\n","Epoch 32/150 | Batch 12/198 | Loss: 1.591408133506775\n","Epoch 32/150 | Batch 13/198 | Loss: 1.556884527206421\n","Epoch 32/150 | Batch 14/198 | Loss: 1.711549162864685\n","Epoch 32/150 | Batch 15/198 | Loss: 1.6642247438430786\n","Epoch 32/150 | Batch 16/198 | Loss: 1.6607941389083862\n","Epoch 32/150 | Batch 17/198 | Loss: 1.76008141040802\n","Epoch 32/150 | Batch 18/198 | Loss: 1.2995166778564453\n","Epoch 32/150 | Batch 19/198 | Loss: 1.5100338459014893\n","Epoch 32/150 | Batch 20/198 | Loss: 1.5916246175765991\n","Epoch 32/150 | Batch 21/198 | Loss: 1.6873723268508911\n","Epoch 32/150 | Batch 22/198 | Loss: 1.6779998540878296\n","Epoch 32/150 | Batch 23/198 | Loss: 1.6204118728637695\n","Epoch 32/150 | Batch 24/198 | Loss: 1.5080088376998901\n","Epoch 32/150 | Batch 25/198 | Loss: 1.49894380569458\n","Epoch 32/150 | Batch 26/198 | Loss: 1.593632459640503\n","Epoch 32/150 | Batch 27/198 | Loss: 1.5307661294937134\n","Epoch 32/150 | Batch 28/198 | Loss: 1.8332841396331787\n","Epoch 32/150 | Batch 29/198 | Loss: 1.8780890703201294\n","Epoch 32/150 | Batch 30/198 | Loss: 1.4200464487075806\n","Epoch 32/150 | Batch 31/198 | Loss: 1.5931370258331299\n","Epoch 32/150 | Batch 32/198 | Loss: 1.7206352949142456\n","Epoch 32/150 | Batch 33/198 | Loss: 1.5185821056365967\n","Epoch 32/150 | Batch 34/198 | Loss: 1.7117857933044434\n","Epoch 32/150 | Batch 35/198 | Loss: 1.5257216691970825\n","Epoch 32/150 | Batch 36/198 | Loss: 1.7446614503860474\n","Epoch 32/150 | Batch 37/198 | Loss: 1.5862224102020264\n","Epoch 32/150 | Batch 38/198 | Loss: 1.8436115980148315\n","Epoch 32/150 | Batch 39/198 | Loss: 1.625970721244812\n","Epoch 32/150 | Batch 40/198 | Loss: 1.872732400894165\n","Epoch 32/150 | Batch 41/198 | Loss: 1.7771726846694946\n","Epoch 32/150 | Batch 42/198 | Loss: 1.7249889373779297\n","Epoch 32/150 | Batch 43/198 | Loss: 1.6962170600891113\n","Epoch 32/150 | Batch 44/198 | Loss: 1.778349757194519\n","Epoch 32/150 | Batch 45/198 | Loss: 1.6764968633651733\n","Epoch 32/150 | Batch 46/198 | Loss: 1.5558724403381348\n","Epoch 32/150 | Batch 47/198 | Loss: 1.6145182847976685\n","Epoch 32/150 | Batch 48/198 | Loss: 1.5879217386245728\n","Epoch 32/150 | Batch 49/198 | Loss: 1.6168409585952759\n","Epoch 32/150 | Batch 50/198 | Loss: 1.4771488904953003\n","Epoch 32/150 | Batch 51/198 | Loss: 1.5169165134429932\n","Epoch 32/150 | Batch 52/198 | Loss: 1.4821118116378784\n","Epoch 32/150 | Batch 53/198 | Loss: 1.6798006296157837\n","Epoch 32/150 | Batch 54/198 | Loss: 1.7404003143310547\n","Epoch 32/150 | Batch 55/198 | Loss: 1.692270278930664\n","Epoch 32/150 | Batch 56/198 | Loss: 1.7068302631378174\n","Epoch 32/150 | Batch 57/198 | Loss: 1.5175907611846924\n","Epoch 32/150 | Batch 58/198 | Loss: 1.6468169689178467\n","Epoch 32/150 | Batch 59/198 | Loss: 1.6156539916992188\n","Epoch 32/150 | Batch 60/198 | Loss: 1.497130036354065\n","Epoch 32/150 | Batch 61/198 | Loss: 1.657389760017395\n","Epoch 32/150 | Batch 62/198 | Loss: 1.9132176637649536\n","Epoch 32/150 | Batch 63/198 | Loss: 1.548558235168457\n","Epoch 32/150 | Batch 64/198 | Loss: 1.541823387145996\n","Epoch 32/150 | Batch 65/198 | Loss: 1.7386846542358398\n","Epoch 32/150 | Batch 66/198 | Loss: 1.5930119752883911\n","Epoch 32/150 | Batch 67/198 | Loss: 1.4993360042572021\n","Epoch 32/150 | Batch 68/198 | Loss: 1.738106608390808\n","Epoch 32/150 | Batch 69/198 | Loss: 1.7444771528244019\n","Epoch 32/150 | Batch 70/198 | Loss: 1.6780924797058105\n","Epoch 32/150 | Batch 71/198 | Loss: 1.7034265995025635\n","Epoch 32/150 | Batch 72/198 | Loss: 1.6494741439819336\n","Epoch 32/150 | Batch 73/198 | Loss: 1.5491342544555664\n","Epoch 32/150 | Batch 74/198 | Loss: 1.3611154556274414\n","Epoch 32/150 | Batch 75/198 | Loss: 1.611311674118042\n","Epoch 32/150 | Batch 76/198 | Loss: 1.592736840248108\n","Epoch 32/150 | Batch 77/198 | Loss: 1.4122470617294312\n","Epoch 32/150 | Batch 78/198 | Loss: 1.5733098983764648\n","Epoch 32/150 | Batch 79/198 | Loss: 1.585700511932373\n","Epoch 32/150 | Batch 80/198 | Loss: 1.7131010293960571\n","Epoch 32/150 | Batch 81/198 | Loss: 1.794259786605835\n","Epoch 32/150 | Batch 82/198 | Loss: 1.6384011507034302\n","Epoch 32/150 | Batch 83/198 | Loss: 1.7372541427612305\n","Epoch 32/150 | Batch 84/198 | Loss: 1.5681456327438354\n","Epoch 32/150 | Batch 85/198 | Loss: 1.9338079690933228\n","Epoch 32/150 | Batch 86/198 | Loss: 1.6414568424224854\n","Epoch 32/150 | Batch 87/198 | Loss: 1.437705636024475\n","Epoch 32/150 | Batch 88/198 | Loss: 1.8564026355743408\n","Epoch 32/150 | Batch 89/198 | Loss: 1.8404760360717773\n","Epoch 32/150 | Batch 90/198 | Loss: 1.9358224868774414\n","Epoch 32/150 | Batch 91/198 | Loss: 1.6827096939086914\n","Epoch 32/150 | Batch 92/198 | Loss: 1.828685998916626\n","Epoch 32/150 | Batch 93/198 | Loss: 1.5866962671279907\n","Epoch 32/150 | Batch 94/198 | Loss: 1.7471553087234497\n","Epoch 32/150 | Batch 95/198 | Loss: 1.684717059135437\n","Epoch 32/150 | Batch 96/198 | Loss: 1.6046595573425293\n","Epoch 32/150 | Batch 97/198 | Loss: 1.537760853767395\n","Epoch 32/150 | Batch 98/198 | Loss: 1.543276309967041\n","Epoch 32/150 | Batch 99/198 | Loss: 1.7456284761428833\n","Epoch 32/150 | Batch 100/198 | Loss: 1.5862993001937866\n","Epoch 32/150 | Batch 101/198 | Loss: 1.8306916952133179\n","Epoch 32/150 | Batch 102/198 | Loss: 1.6898573637008667\n","Epoch 32/150 | Batch 103/198 | Loss: 1.685622215270996\n","Epoch 32/150 | Batch 104/198 | Loss: 1.627893090248108\n","Epoch 32/150 | Batch 105/198 | Loss: 1.8182507753372192\n","Epoch 32/150 | Batch 106/198 | Loss: 1.5510027408599854\n","Epoch 32/150 | Batch 107/198 | Loss: 1.517836570739746\n","Epoch 32/150 | Batch 108/198 | Loss: 1.4178485870361328\n","Epoch 32/150 | Batch 109/198 | Loss: 1.7939397096633911\n","Epoch 32/150 | Batch 110/198 | Loss: 1.5848066806793213\n","Epoch 32/150 | Batch 111/198 | Loss: 1.292331337928772\n","Epoch 32/150 | Batch 112/198 | Loss: 1.658653736114502\n","Epoch 32/150 | Batch 113/198 | Loss: 1.727541208267212\n","Epoch 32/150 | Batch 114/198 | Loss: 1.5181323289871216\n","Epoch 32/150 | Batch 115/198 | Loss: 1.7871547937393188\n","Epoch 32/150 | Batch 116/198 | Loss: 1.707656979560852\n","Epoch 32/150 | Batch 117/198 | Loss: 1.5647733211517334\n","Epoch 32/150 | Batch 118/198 | Loss: 1.7960747480392456\n","Epoch 32/150 | Batch 119/198 | Loss: 1.8131625652313232\n","Epoch 32/150 | Batch 120/198 | Loss: 1.9192676544189453\n","Epoch 32/150 | Batch 121/198 | Loss: 1.880630373954773\n","Epoch 32/150 | Batch 122/198 | Loss: 1.795482873916626\n","Epoch 32/150 | Batch 123/198 | Loss: 1.52935791015625\n","Epoch 32/150 | Batch 124/198 | Loss: 1.8302987813949585\n","Epoch 32/150 | Batch 125/198 | Loss: 1.6672660112380981\n","Epoch 32/150 | Batch 126/198 | Loss: 1.2846986055374146\n","Epoch 32/150 | Batch 127/198 | Loss: 1.676253080368042\n","Epoch 32/150 | Batch 128/198 | Loss: 1.4981050491333008\n","Epoch 32/150 | Batch 129/198 | Loss: 1.5347535610198975\n","Epoch 32/150 | Batch 130/198 | Loss: 1.6364942789077759\n","Epoch 32/150 | Batch 131/198 | Loss: 1.559057354927063\n","Epoch 32/150 | Batch 132/198 | Loss: 1.5927953720092773\n","Epoch 32/150 | Batch 133/198 | Loss: 1.7399591207504272\n","Epoch 32/150 | Batch 134/198 | Loss: 1.5744516849517822\n","Epoch 32/150 | Batch 135/198 | Loss: 1.6334710121154785\n","Epoch 32/150 | Batch 136/198 | Loss: 1.7624150514602661\n","Epoch 32/150 | Batch 137/198 | Loss: 1.6166565418243408\n","Epoch 32/150 | Batch 138/198 | Loss: 1.6883372068405151\n","Epoch 32/150 | Batch 139/198 | Loss: 1.6533091068267822\n","Epoch 32/150 | Batch 140/198 | Loss: 1.5486993789672852\n","Epoch 32/150 | Batch 141/198 | Loss: 1.6895257234573364\n","Epoch 32/150 | Batch 142/198 | Loss: 1.4616259336471558\n","Epoch 32/150 | Batch 143/198 | Loss: 1.6236510276794434\n","Epoch 32/150 | Batch 144/198 | Loss: 1.7137959003448486\n","Epoch 32/150 | Batch 145/198 | Loss: 1.6613717079162598\n","Epoch 32/150 | Batch 146/198 | Loss: 1.5362396240234375\n","Epoch 32/150 | Batch 147/198 | Loss: 1.5713359117507935\n","Epoch 32/150 | Batch 148/198 | Loss: 1.686952829360962\n","Epoch 32/150 | Batch 149/198 | Loss: 1.2862787246704102\n","Epoch 32/150 | Batch 150/198 | Loss: 1.6521387100219727\n","Epoch 32/150 | Batch 151/198 | Loss: 1.8059874773025513\n","Epoch 32/150 | Batch 152/198 | Loss: 1.6004681587219238\n","Epoch 32/150 | Batch 153/198 | Loss: 1.754395604133606\n","Epoch 32/150 | Batch 154/198 | Loss: 1.694539189338684\n","Epoch 32/150 | Batch 155/198 | Loss: 1.673117995262146\n","Epoch 32/150 | Batch 156/198 | Loss: 1.8584564924240112\n","Epoch 32/150 | Batch 157/198 | Loss: 1.907659888267517\n","Epoch 32/150 | Batch 158/198 | Loss: 1.6314539909362793\n","Epoch 32/150 | Batch 159/198 | Loss: 1.6085925102233887\n","Epoch 32/150 | Batch 160/198 | Loss: 1.5456593036651611\n","Epoch 32/150 | Batch 161/198 | Loss: 1.7781550884246826\n","Epoch 32/150 | Batch 162/198 | Loss: 1.5042285919189453\n","Epoch 32/150 | Batch 163/198 | Loss: 1.5131115913391113\n","Epoch 32/150 | Batch 164/198 | Loss: 1.9995650053024292\n","Epoch 32/150 | Batch 165/198 | Loss: 1.5500613451004028\n","Epoch 32/150 | Batch 166/198 | Loss: 1.5152761936187744\n","Epoch 32/150 | Batch 167/198 | Loss: 1.6966074705123901\n","Epoch 32/150 | Batch 168/198 | Loss: 1.7154645919799805\n","Epoch 32/150 | Batch 169/198 | Loss: 1.6775271892547607\n","Epoch 32/150 | Batch 170/198 | Loss: 1.7000259160995483\n","Epoch 32/150 | Batch 171/198 | Loss: 1.7529174089431763\n","Epoch 32/150 | Batch 172/198 | Loss: 1.5934967994689941\n","Epoch 32/150 | Batch 173/198 | Loss: 1.7091524600982666\n","Epoch 32/150 | Batch 174/198 | Loss: 1.6678714752197266\n","Epoch 32/150 | Batch 175/198 | Loss: 1.8409911394119263\n","Epoch 32/150 | Batch 176/198 | Loss: 1.5724267959594727\n","Epoch 32/150 | Batch 177/198 | Loss: 1.9048115015029907\n","Epoch 32/150 | Batch 178/198 | Loss: 1.8650299310684204\n","Epoch 32/150 | Batch 179/198 | Loss: 1.7361727952957153\n","Epoch 32/150 | Batch 180/198 | Loss: 1.6639755964279175\n","Epoch 32/150 | Batch 181/198 | Loss: 1.6731256246566772\n","Epoch 32/150 | Batch 182/198 | Loss: 1.40742826461792\n","Epoch 32/150 | Batch 183/198 | Loss: 1.6137834787368774\n","Epoch 32/150 | Batch 184/198 | Loss: 1.6174155473709106\n","Epoch 32/150 | Batch 185/198 | Loss: 1.7378432750701904\n","Epoch 32/150 | Batch 186/198 | Loss: 1.7644217014312744\n","Epoch 32/150 | Batch 187/198 | Loss: 1.4684631824493408\n","Epoch 32/150 | Batch 188/198 | Loss: 1.5714187622070312\n","Epoch 32/150 | Batch 189/198 | Loss: 1.6344646215438843\n","Epoch 32/150 | Batch 190/198 | Loss: 1.6452124118804932\n","Epoch 32/150 | Batch 191/198 | Loss: 1.5812033414840698\n","Epoch 32/150 | Batch 192/198 | Loss: 1.6480106115341187\n","Epoch 32/150 | Batch 193/198 | Loss: 1.6732233762741089\n","Epoch 32/150 | Batch 194/198 | Loss: 2.014478921890259\n","Epoch 32/150 | Batch 195/198 | Loss: 1.74813973903656\n","Epoch 32/150 | Batch 196/198 | Loss: 1.9023168087005615\n","Epoch 32/150 | Batch 197/198 | Loss: 1.56910240650177\n","Epoch 32/150 | Batch 198/198 | Loss: 1.8259241580963135\n","Epoch 32/150 | Average Loss: 1.6547984348403082\n","Epoch 33/150 | Batch 1/198 | Loss: 1.4586414098739624\n","Epoch 33/150 | Batch 2/198 | Loss: 1.6068414449691772\n","Epoch 33/150 | Batch 3/198 | Loss: 1.6172043085098267\n","Epoch 33/150 | Batch 4/198 | Loss: 1.8381847143173218\n","Epoch 33/150 | Batch 5/198 | Loss: 1.6488170623779297\n","Epoch 33/150 | Batch 6/198 | Loss: 1.7012851238250732\n","Epoch 33/150 | Batch 7/198 | Loss: 1.6132681369781494\n","Epoch 33/150 | Batch 8/198 | Loss: 1.671970248222351\n","Epoch 33/150 | Batch 9/198 | Loss: 1.751944899559021\n","Epoch 33/150 | Batch 10/198 | Loss: 1.7356725931167603\n","Epoch 33/150 | Batch 11/198 | Loss: 1.7609530687332153\n","Epoch 33/150 | Batch 12/198 | Loss: 1.7301331758499146\n","Epoch 33/150 | Batch 13/198 | Loss: 1.6410725116729736\n","Epoch 33/150 | Batch 14/198 | Loss: 1.6598941087722778\n","Epoch 33/150 | Batch 15/198 | Loss: 1.6949111223220825\n","Epoch 33/150 | Batch 16/198 | Loss: 1.810090184211731\n","Epoch 33/150 | Batch 17/198 | Loss: 1.7449344396591187\n","Epoch 33/150 | Batch 18/198 | Loss: 1.6442300081253052\n","Epoch 33/150 | Batch 19/198 | Loss: 1.9243122339248657\n","Epoch 33/150 | Batch 20/198 | Loss: 1.5940701961517334\n","Epoch 33/150 | Batch 21/198 | Loss: 1.6374973058700562\n","Epoch 33/150 | Batch 22/198 | Loss: 1.684732437133789\n","Epoch 33/150 | Batch 23/198 | Loss: 1.6693717241287231\n","Epoch 33/150 | Batch 24/198 | Loss: 1.7097985744476318\n","Epoch 33/150 | Batch 25/198 | Loss: 1.7902805805206299\n","Epoch 33/150 | Batch 26/198 | Loss: 1.867391586303711\n","Epoch 33/150 | Batch 27/198 | Loss: 1.7700636386871338\n","Epoch 33/150 | Batch 28/198 | Loss: 1.55928373336792\n","Epoch 33/150 | Batch 29/198 | Loss: 1.7743521928787231\n","Epoch 33/150 | Batch 30/198 | Loss: 1.7452527284622192\n","Epoch 33/150 | Batch 31/198 | Loss: 1.5949088335037231\n","Epoch 33/150 | Batch 32/198 | Loss: 1.608987808227539\n","Epoch 33/150 | Batch 33/198 | Loss: 1.4579298496246338\n","Epoch 33/150 | Batch 34/198 | Loss: 1.670607089996338\n","Epoch 33/150 | Batch 35/198 | Loss: 1.6736077070236206\n","Epoch 33/150 | Batch 36/198 | Loss: 1.6772574186325073\n","Epoch 33/150 | Batch 37/198 | Loss: 1.9230023622512817\n","Epoch 33/150 | Batch 38/198 | Loss: 1.5150409936904907\n","Epoch 33/150 | Batch 39/198 | Loss: 1.4522857666015625\n","Epoch 33/150 | Batch 40/198 | Loss: 1.7866394519805908\n","Epoch 33/150 | Batch 41/198 | Loss: 1.5932129621505737\n","Epoch 33/150 | Batch 42/198 | Loss: 1.6601790189743042\n","Epoch 33/150 | Batch 43/198 | Loss: 1.676376461982727\n","Epoch 33/150 | Batch 44/198 | Loss: 1.7520873546600342\n","Epoch 33/150 | Batch 45/198 | Loss: 1.4531567096710205\n","Epoch 33/150 | Batch 46/198 | Loss: 1.4520624876022339\n","Epoch 33/150 | Batch 47/198 | Loss: 1.7449419498443604\n","Epoch 33/150 | Batch 48/198 | Loss: 1.52198326587677\n","Epoch 33/150 | Batch 49/198 | Loss: 1.5121545791625977\n","Epoch 33/150 | Batch 50/198 | Loss: 1.8087525367736816\n","Epoch 33/150 | Batch 51/198 | Loss: 1.5433963537216187\n","Epoch 33/150 | Batch 52/198 | Loss: 1.6684982776641846\n","Epoch 33/150 | Batch 53/198 | Loss: 1.4901959896087646\n","Epoch 33/150 | Batch 54/198 | Loss: 1.4949719905853271\n","Epoch 33/150 | Batch 55/198 | Loss: 1.84287428855896\n","Epoch 33/150 | Batch 56/198 | Loss: 1.6921366453170776\n","Epoch 33/150 | Batch 57/198 | Loss: 1.7076486349105835\n","Epoch 33/150 | Batch 58/198 | Loss: 1.5509856939315796\n","Epoch 33/150 | Batch 59/198 | Loss: 1.745327115058899\n","Epoch 33/150 | Batch 60/198 | Loss: 1.6449934244155884\n","Epoch 33/150 | Batch 61/198 | Loss: 1.4856812953948975\n","Epoch 33/150 | Batch 62/198 | Loss: 1.670345664024353\n","Epoch 33/150 | Batch 63/198 | Loss: 1.5733624696731567\n","Epoch 33/150 | Batch 64/198 | Loss: 1.5711649656295776\n","Epoch 33/150 | Batch 65/198 | Loss: 1.8052536249160767\n","Epoch 33/150 | Batch 66/198 | Loss: 1.578139066696167\n","Epoch 33/150 | Batch 67/198 | Loss: 1.6196179389953613\n","Epoch 33/150 | Batch 68/198 | Loss: 1.4497456550598145\n","Epoch 33/150 | Batch 69/198 | Loss: 1.3332151174545288\n","Epoch 33/150 | Batch 70/198 | Loss: 1.4051684141159058\n","Epoch 33/150 | Batch 71/198 | Loss: 1.5015062093734741\n","Epoch 33/150 | Batch 72/198 | Loss: 1.7210811376571655\n","Epoch 33/150 | Batch 73/198 | Loss: 1.6241086721420288\n","Epoch 33/150 | Batch 74/198 | Loss: 1.6419552564620972\n","Epoch 33/150 | Batch 75/198 | Loss: 1.5410563945770264\n","Epoch 33/150 | Batch 76/198 | Loss: 1.6383864879608154\n","Epoch 33/150 | Batch 77/198 | Loss: 1.8410334587097168\n","Epoch 33/150 | Batch 78/198 | Loss: 1.6694393157958984\n","Epoch 33/150 | Batch 79/198 | Loss: 1.5103518962860107\n","Epoch 33/150 | Batch 80/198 | Loss: 1.8044462203979492\n","Epoch 33/150 | Batch 81/198 | Loss: 1.8654203414916992\n","Epoch 33/150 | Batch 82/198 | Loss: 1.489109992980957\n","Epoch 33/150 | Batch 83/198 | Loss: 1.3862375020980835\n","Epoch 33/150 | Batch 84/198 | Loss: 1.532471776008606\n","Epoch 33/150 | Batch 85/198 | Loss: 1.8660627603530884\n","Epoch 33/150 | Batch 86/198 | Loss: 1.5337682962417603\n","Epoch 33/150 | Batch 87/198 | Loss: 1.7211648225784302\n","Epoch 33/150 | Batch 88/198 | Loss: 1.4423972368240356\n","Epoch 33/150 | Batch 89/198 | Loss: 1.605305552482605\n","Epoch 33/150 | Batch 90/198 | Loss: 1.683109998703003\n","Epoch 33/150 | Batch 91/198 | Loss: 1.5119379758834839\n","Epoch 33/150 | Batch 92/198 | Loss: 1.7450395822525024\n","Epoch 33/150 | Batch 93/198 | Loss: 1.5909534692764282\n","Epoch 33/150 | Batch 94/198 | Loss: 1.5080479383468628\n","Epoch 33/150 | Batch 95/198 | Loss: 1.6317920684814453\n","Epoch 33/150 | Batch 96/198 | Loss: 1.696823000907898\n","Epoch 33/150 | Batch 97/198 | Loss: 1.757670283317566\n","Epoch 33/150 | Batch 98/198 | Loss: 1.5791494846343994\n","Epoch 33/150 | Batch 99/198 | Loss: 1.5987614393234253\n","Epoch 33/150 | Batch 100/198 | Loss: 1.735582947731018\n","Epoch 33/150 | Batch 101/198 | Loss: 1.6440929174423218\n","Epoch 33/150 | Batch 102/198 | Loss: 1.607407569885254\n","Epoch 33/150 | Batch 103/198 | Loss: 1.5159205198287964\n","Epoch 33/150 | Batch 104/198 | Loss: 1.7496328353881836\n","Epoch 33/150 | Batch 105/198 | Loss: 1.567335605621338\n","Epoch 33/150 | Batch 106/198 | Loss: 1.546209454536438\n","Epoch 33/150 | Batch 107/198 | Loss: 1.554389476776123\n","Epoch 33/150 | Batch 108/198 | Loss: 1.4689950942993164\n","Epoch 33/150 | Batch 109/198 | Loss: 1.8429930210113525\n","Epoch 33/150 | Batch 110/198 | Loss: 1.5136666297912598\n","Epoch 33/150 | Batch 111/198 | Loss: 1.3686859607696533\n","Epoch 33/150 | Batch 112/198 | Loss: 1.6314284801483154\n","Epoch 33/150 | Batch 113/198 | Loss: 1.5554711818695068\n","Epoch 33/150 | Batch 114/198 | Loss: 1.8082607984542847\n","Epoch 33/150 | Batch 115/198 | Loss: 1.5272126197814941\n","Epoch 33/150 | Batch 116/198 | Loss: 1.5392825603485107\n","Epoch 33/150 | Batch 117/198 | Loss: 1.5432461500167847\n","Epoch 33/150 | Batch 118/198 | Loss: 1.4938329458236694\n","Epoch 33/150 | Batch 119/198 | Loss: 1.7064529657363892\n","Epoch 33/150 | Batch 120/198 | Loss: 1.7178717851638794\n","Epoch 33/150 | Batch 121/198 | Loss: 1.505643367767334\n","Epoch 33/150 | Batch 122/198 | Loss: 1.6103650331497192\n","Epoch 33/150 | Batch 123/198 | Loss: 1.5937360525131226\n","Epoch 33/150 | Batch 124/198 | Loss: 1.7335172891616821\n","Epoch 33/150 | Batch 125/198 | Loss: 1.543536901473999\n","Epoch 33/150 | Batch 126/198 | Loss: 1.3950163125991821\n","Epoch 33/150 | Batch 127/198 | Loss: 1.549336314201355\n","Epoch 33/150 | Batch 128/198 | Loss: 1.763596534729004\n","Epoch 33/150 | Batch 129/198 | Loss: 1.4891107082366943\n","Epoch 33/150 | Batch 130/198 | Loss: 1.9090296030044556\n","Epoch 33/150 | Batch 131/198 | Loss: 1.7649322748184204\n","Epoch 33/150 | Batch 132/198 | Loss: 1.4898942708969116\n","Epoch 33/150 | Batch 133/198 | Loss: 1.4315824508666992\n","Epoch 33/150 | Batch 134/198 | Loss: 1.4841129779815674\n","Epoch 33/150 | Batch 135/198 | Loss: 1.5718152523040771\n","Epoch 33/150 | Batch 136/198 | Loss: 1.6047641038894653\n","Epoch 33/150 | Batch 137/198 | Loss: 1.748711109161377\n","Epoch 33/150 | Batch 138/198 | Loss: 1.6881053447723389\n","Epoch 33/150 | Batch 139/198 | Loss: 1.655551552772522\n","Epoch 33/150 | Batch 140/198 | Loss: 1.3254445791244507\n","Epoch 33/150 | Batch 141/198 | Loss: 1.5655593872070312\n","Epoch 33/150 | Batch 142/198 | Loss: 1.639797568321228\n","Epoch 33/150 | Batch 143/198 | Loss: 1.7481430768966675\n","Epoch 33/150 | Batch 144/198 | Loss: 1.5402947664260864\n","Epoch 33/150 | Batch 145/198 | Loss: 1.7160247564315796\n","Epoch 33/150 | Batch 146/198 | Loss: 1.6952637434005737\n","Epoch 33/150 | Batch 147/198 | Loss: 1.7738157510757446\n","Epoch 33/150 | Batch 148/198 | Loss: 1.6619809865951538\n","Epoch 33/150 | Batch 149/198 | Loss: 1.8513610363006592\n","Epoch 33/150 | Batch 150/198 | Loss: 1.6124730110168457\n","Epoch 33/150 | Batch 151/198 | Loss: 1.650097131729126\n","Epoch 33/150 | Batch 152/198 | Loss: 1.572081446647644\n","Epoch 33/150 | Batch 153/198 | Loss: 1.9300222396850586\n","Epoch 33/150 | Batch 154/198 | Loss: 1.6336649656295776\n","Epoch 33/150 | Batch 155/198 | Loss: 1.5991898775100708\n","Epoch 33/150 | Batch 156/198 | Loss: 1.577149510383606\n","Epoch 33/150 | Batch 157/198 | Loss: 1.7788972854614258\n","Epoch 33/150 | Batch 158/198 | Loss: 1.604907751083374\n","Epoch 33/150 | Batch 159/198 | Loss: 1.606734037399292\n","Epoch 33/150 | Batch 160/198 | Loss: 1.7304188013076782\n","Epoch 33/150 | Batch 161/198 | Loss: 1.8015789985656738\n","Epoch 33/150 | Batch 162/198 | Loss: 1.4547858238220215\n","Epoch 33/150 | Batch 163/198 | Loss: 1.5595446825027466\n","Epoch 33/150 | Batch 164/198 | Loss: 1.658368468284607\n","Epoch 33/150 | Batch 165/198 | Loss: 1.656869649887085\n","Epoch 33/150 | Batch 166/198 | Loss: 1.485098958015442\n","Epoch 33/150 | Batch 167/198 | Loss: 1.4754204750061035\n","Epoch 33/150 | Batch 168/198 | Loss: 1.4840737581253052\n","Epoch 33/150 | Batch 169/198 | Loss: 1.6555562019348145\n","Epoch 33/150 | Batch 170/198 | Loss: 1.5771902799606323\n","Epoch 33/150 | Batch 171/198 | Loss: 1.6818422079086304\n","Epoch 33/150 | Batch 172/198 | Loss: 1.6826554536819458\n","Epoch 33/150 | Batch 173/198 | Loss: 1.6785188913345337\n","Epoch 33/150 | Batch 174/198 | Loss: 1.3126190900802612\n","Epoch 33/150 | Batch 175/198 | Loss: 1.7526953220367432\n","Epoch 33/150 | Batch 176/198 | Loss: 1.5989044904708862\n","Epoch 33/150 | Batch 177/198 | Loss: 1.5751863718032837\n","Epoch 33/150 | Batch 178/198 | Loss: 1.4799606800079346\n","Epoch 33/150 | Batch 179/198 | Loss: 1.4613105058670044\n","Epoch 33/150 | Batch 180/198 | Loss: 1.541396141052246\n","Epoch 33/150 | Batch 181/198 | Loss: 1.6253100633621216\n","Epoch 33/150 | Batch 182/198 | Loss: 1.8433202505111694\n","Epoch 33/150 | Batch 183/198 | Loss: 1.727937936782837\n","Epoch 33/150 | Batch 184/198 | Loss: 1.6551756858825684\n","Epoch 33/150 | Batch 185/198 | Loss: 1.5290844440460205\n","Epoch 33/150 | Batch 186/198 | Loss: 1.5565185546875\n","Epoch 33/150 | Batch 187/198 | Loss: 1.7944390773773193\n","Epoch 33/150 | Batch 188/198 | Loss: 1.6421984434127808\n","Epoch 33/150 | Batch 189/198 | Loss: 1.5446408987045288\n","Epoch 33/150 | Batch 190/198 | Loss: 1.7281081676483154\n","Epoch 33/150 | Batch 191/198 | Loss: 1.5145809650421143\n","Epoch 33/150 | Batch 192/198 | Loss: 1.7978121042251587\n","Epoch 33/150 | Batch 193/198 | Loss: 1.4185901880264282\n","Epoch 33/150 | Batch 194/198 | Loss: 1.7107315063476562\n","Epoch 33/150 | Batch 195/198 | Loss: 1.6864854097366333\n","Epoch 33/150 | Batch 196/198 | Loss: 1.5500763654708862\n","Epoch 33/150 | Batch 197/198 | Loss: 1.7587436437606812\n","Epoch 33/150 | Batch 198/198 | Loss: 1.7873797416687012\n","Epoch 33/150 | Average Loss: 1.6319762721206204\n","Epoch 34/150 | Batch 1/198 | Loss: 1.8107742071151733\n","Epoch 34/150 | Batch 2/198 | Loss: 1.39757239818573\n","Epoch 34/150 | Batch 3/198 | Loss: 1.7014251947402954\n","Epoch 34/150 | Batch 4/198 | Loss: 1.8027455806732178\n","Epoch 34/150 | Batch 5/198 | Loss: 1.328505277633667\n","Epoch 34/150 | Batch 6/198 | Loss: 1.4978864192962646\n","Epoch 34/150 | Batch 7/198 | Loss: 1.5636980533599854\n","Epoch 34/150 | Batch 8/198 | Loss: 1.7095717191696167\n","Epoch 34/150 | Batch 9/198 | Loss: 1.6640697717666626\n","Epoch 34/150 | Batch 10/198 | Loss: 1.688380241394043\n","Epoch 34/150 | Batch 11/198 | Loss: 1.486162781715393\n","Epoch 34/150 | Batch 12/198 | Loss: 1.674086093902588\n","Epoch 34/150 | Batch 13/198 | Loss: 1.5817633867263794\n","Epoch 34/150 | Batch 14/198 | Loss: 1.5176912546157837\n","Epoch 34/150 | Batch 15/198 | Loss: 1.365165114402771\n","Epoch 34/150 | Batch 16/198 | Loss: 1.7668704986572266\n","Epoch 34/150 | Batch 17/198 | Loss: 1.6445552110671997\n","Epoch 34/150 | Batch 18/198 | Loss: 1.5860642194747925\n","Epoch 34/150 | Batch 19/198 | Loss: 1.809915542602539\n","Epoch 34/150 | Batch 20/198 | Loss: 1.6334086656570435\n","Epoch 34/150 | Batch 21/198 | Loss: 1.5125011205673218\n","Epoch 34/150 | Batch 22/198 | Loss: 1.5360385179519653\n","Epoch 34/150 | Batch 23/198 | Loss: 1.645569920539856\n","Epoch 34/150 | Batch 24/198 | Loss: 1.5162726640701294\n","Epoch 34/150 | Batch 25/198 | Loss: 1.619923710823059\n","Epoch 34/150 | Batch 26/198 | Loss: 1.6478067636489868\n","Epoch 34/150 | Batch 27/198 | Loss: 1.456502079963684\n","Epoch 34/150 | Batch 28/198 | Loss: 1.5850400924682617\n","Epoch 34/150 | Batch 29/198 | Loss: 1.8370485305786133\n","Epoch 34/150 | Batch 30/198 | Loss: 1.7215750217437744\n","Epoch 34/150 | Batch 31/198 | Loss: 1.7015166282653809\n","Epoch 34/150 | Batch 32/198 | Loss: 1.5313739776611328\n","Epoch 34/150 | Batch 33/198 | Loss: 1.6215448379516602\n","Epoch 34/150 | Batch 34/198 | Loss: 1.6620944738388062\n","Epoch 34/150 | Batch 35/198 | Loss: 1.7167870998382568\n","Epoch 34/150 | Batch 36/198 | Loss: 1.5308934450149536\n","Epoch 34/150 | Batch 37/198 | Loss: 1.7195806503295898\n","Epoch 34/150 | Batch 38/198 | Loss: 1.4159671068191528\n","Epoch 34/150 | Batch 39/198 | Loss: 1.6650606393814087\n","Epoch 34/150 | Batch 40/198 | Loss: 1.658805012702942\n","Epoch 34/150 | Batch 41/198 | Loss: 1.7610355615615845\n","Epoch 34/150 | Batch 42/198 | Loss: 1.6877974271774292\n","Epoch 34/150 | Batch 43/198 | Loss: 1.395722508430481\n","Epoch 34/150 | Batch 44/198 | Loss: 1.3537570238113403\n","Epoch 34/150 | Batch 45/198 | Loss: 1.5429480075836182\n","Epoch 34/150 | Batch 46/198 | Loss: 1.8104801177978516\n","Epoch 34/150 | Batch 47/198 | Loss: 1.7342164516448975\n","Epoch 34/150 | Batch 48/198 | Loss: 1.2877514362335205\n","Epoch 34/150 | Batch 49/198 | Loss: 1.4581876993179321\n","Epoch 34/150 | Batch 50/198 | Loss: 1.642600178718567\n","Epoch 34/150 | Batch 51/198 | Loss: 1.8329534530639648\n","Epoch 34/150 | Batch 52/198 | Loss: 1.5167733430862427\n","Epoch 34/150 | Batch 53/198 | Loss: 1.5804898738861084\n","Epoch 34/150 | Batch 54/198 | Loss: 1.6621644496917725\n","Epoch 34/150 | Batch 55/198 | Loss: 1.6939178705215454\n","Epoch 34/150 | Batch 56/198 | Loss: 1.517773151397705\n","Epoch 34/150 | Batch 57/198 | Loss: 1.5437880754470825\n","Epoch 34/150 | Batch 58/198 | Loss: 1.5244793891906738\n","Epoch 34/150 | Batch 59/198 | Loss: 1.598267674446106\n","Epoch 34/150 | Batch 60/198 | Loss: 1.4870644807815552\n","Epoch 34/150 | Batch 61/198 | Loss: 1.5638195276260376\n","Epoch 34/150 | Batch 62/198 | Loss: 1.8144510984420776\n","Epoch 34/150 | Batch 63/198 | Loss: 1.5116313695907593\n","Epoch 34/150 | Batch 64/198 | Loss: 1.6764057874679565\n","Epoch 34/150 | Batch 65/198 | Loss: 1.6870903968811035\n","Epoch 34/150 | Batch 66/198 | Loss: 1.4808913469314575\n","Epoch 34/150 | Batch 67/198 | Loss: 1.4447581768035889\n","Epoch 34/150 | Batch 68/198 | Loss: 1.4438092708587646\n","Epoch 34/150 | Batch 69/198 | Loss: 1.6027475595474243\n","Epoch 34/150 | Batch 70/198 | Loss: 1.4450418949127197\n","Epoch 34/150 | Batch 71/198 | Loss: 1.4328571557998657\n","Epoch 34/150 | Batch 72/198 | Loss: 1.6951237916946411\n","Epoch 34/150 | Batch 73/198 | Loss: 1.60651433467865\n","Epoch 34/150 | Batch 74/198 | Loss: 1.5084657669067383\n","Epoch 34/150 | Batch 75/198 | Loss: 1.5777559280395508\n","Epoch 34/150 | Batch 76/198 | Loss: 1.4651976823806763\n","Epoch 34/150 | Batch 77/198 | Loss: 1.6399391889572144\n","Epoch 34/150 | Batch 78/198 | Loss: 1.685897946357727\n","Epoch 34/150 | Batch 79/198 | Loss: 1.40105140209198\n","Epoch 34/150 | Batch 80/198 | Loss: 1.5719865560531616\n","Epoch 34/150 | Batch 81/198 | Loss: 1.6891071796417236\n","Epoch 34/150 | Batch 82/198 | Loss: 1.6231732368469238\n","Epoch 34/150 | Batch 83/198 | Loss: 1.3915154933929443\n","Epoch 34/150 | Batch 84/198 | Loss: 1.6716595888137817\n","Epoch 34/150 | Batch 85/198 | Loss: 1.5683279037475586\n","Epoch 34/150 | Batch 86/198 | Loss: 1.6177440881729126\n","Epoch 34/150 | Batch 87/198 | Loss: 1.666732668876648\n","Epoch 34/150 | Batch 88/198 | Loss: 1.6626770496368408\n","Epoch 34/150 | Batch 89/198 | Loss: 1.5712658166885376\n","Epoch 34/150 | Batch 90/198 | Loss: 1.5417529344558716\n","Epoch 34/150 | Batch 91/198 | Loss: 1.654840350151062\n","Epoch 34/150 | Batch 92/198 | Loss: 1.5857574939727783\n","Epoch 34/150 | Batch 93/198 | Loss: 1.9644010066986084\n","Epoch 34/150 | Batch 94/198 | Loss: 1.624395489692688\n","Epoch 34/150 | Batch 95/198 | Loss: 1.762717604637146\n","Epoch 34/150 | Batch 96/198 | Loss: 1.8299504518508911\n","Epoch 34/150 | Batch 97/198 | Loss: 1.5954008102416992\n","Epoch 34/150 | Batch 98/198 | Loss: 1.6056019067764282\n","Epoch 34/150 | Batch 99/198 | Loss: 1.6310285329818726\n","Epoch 34/150 | Batch 100/198 | Loss: 1.6216096878051758\n","Epoch 34/150 | Batch 101/198 | Loss: 1.6844022274017334\n","Epoch 34/150 | Batch 102/198 | Loss: 1.6682008504867554\n","Epoch 34/150 | Batch 103/198 | Loss: 1.6174856424331665\n","Epoch 34/150 | Batch 104/198 | Loss: 1.738069772720337\n","Epoch 34/150 | Batch 105/198 | Loss: 1.6166530847549438\n","Epoch 34/150 | Batch 106/198 | Loss: 1.687961459159851\n","Epoch 34/150 | Batch 107/198 | Loss: 1.415465235710144\n","Epoch 34/150 | Batch 108/198 | Loss: 1.6601228713989258\n","Epoch 34/150 | Batch 109/198 | Loss: 1.6205142736434937\n","Epoch 34/150 | Batch 110/198 | Loss: 1.5063828229904175\n","Epoch 34/150 | Batch 111/198 | Loss: 1.8757119178771973\n","Epoch 34/150 | Batch 112/198 | Loss: 1.5051767826080322\n","Epoch 34/150 | Batch 113/198 | Loss: 1.5999394655227661\n","Epoch 34/150 | Batch 114/198 | Loss: 1.564441204071045\n","Epoch 34/150 | Batch 115/198 | Loss: 1.5294889211654663\n","Epoch 34/150 | Batch 116/198 | Loss: 1.4346189498901367\n","Epoch 34/150 | Batch 117/198 | Loss: 1.6426420211791992\n","Epoch 34/150 | Batch 118/198 | Loss: 1.5863401889801025\n","Epoch 34/150 | Batch 119/198 | Loss: 1.8164044618606567\n","Epoch 34/150 | Batch 120/198 | Loss: 1.7235485315322876\n","Epoch 34/150 | Batch 121/198 | Loss: 1.645371675491333\n","Epoch 34/150 | Batch 122/198 | Loss: 1.5246405601501465\n","Epoch 34/150 | Batch 123/198 | Loss: 1.6625030040740967\n","Epoch 34/150 | Batch 124/198 | Loss: 1.9946658611297607\n","Epoch 34/150 | Batch 125/198 | Loss: 1.8525524139404297\n","Epoch 34/150 | Batch 126/198 | Loss: 1.8577994108200073\n","Epoch 34/150 | Batch 127/198 | Loss: 1.4705984592437744\n","Epoch 34/150 | Batch 128/198 | Loss: 1.5379929542541504\n","Epoch 34/150 | Batch 129/198 | Loss: 1.8867733478546143\n","Epoch 34/150 | Batch 130/198 | Loss: 1.618330478668213\n","Epoch 34/150 | Batch 131/198 | Loss: 1.548506498336792\n","Epoch 34/150 | Batch 132/198 | Loss: 1.3628947734832764\n","Epoch 34/150 | Batch 133/198 | Loss: 1.8851416110992432\n","Epoch 34/150 | Batch 134/198 | Loss: 1.6114898920059204\n","Epoch 34/150 | Batch 135/198 | Loss: 1.6090654134750366\n","Epoch 34/150 | Batch 136/198 | Loss: 1.6538825035095215\n","Epoch 34/150 | Batch 137/198 | Loss: 1.5783441066741943\n","Epoch 34/150 | Batch 138/198 | Loss: 1.5282505750656128\n","Epoch 34/150 | Batch 139/198 | Loss: 1.629059910774231\n","Epoch 34/150 | Batch 140/198 | Loss: 1.533414363861084\n","Epoch 34/150 | Batch 141/198 | Loss: 1.251773715019226\n","Epoch 34/150 | Batch 142/198 | Loss: 1.4002472162246704\n","Epoch 34/150 | Batch 143/198 | Loss: 1.591625452041626\n","Epoch 34/150 | Batch 144/198 | Loss: 1.5633490085601807\n","Epoch 34/150 | Batch 145/198 | Loss: 1.6689825057983398\n","Epoch 34/150 | Batch 146/198 | Loss: 1.6462301015853882\n","Epoch 34/150 | Batch 147/198 | Loss: 1.7293556928634644\n","Epoch 34/150 | Batch 148/198 | Loss: 1.530922293663025\n","Epoch 34/150 | Batch 149/198 | Loss: 1.8400683403015137\n","Epoch 34/150 | Batch 150/198 | Loss: 1.6244109869003296\n","Epoch 34/150 | Batch 151/198 | Loss: 1.575958013534546\n","Epoch 34/150 | Batch 152/198 | Loss: 1.6173080205917358\n","Epoch 34/150 | Batch 153/198 | Loss: 1.5676071643829346\n","Epoch 34/150 | Batch 154/198 | Loss: 1.7752164602279663\n","Epoch 34/150 | Batch 155/198 | Loss: 1.6257649660110474\n","Epoch 34/150 | Batch 156/198 | Loss: 1.5926638841629028\n","Epoch 34/150 | Batch 157/198 | Loss: 1.812404751777649\n","Epoch 34/150 | Batch 158/198 | Loss: 1.6043574810028076\n","Epoch 34/150 | Batch 159/198 | Loss: 1.5372719764709473\n","Epoch 34/150 | Batch 160/198 | Loss: 1.5325947999954224\n","Epoch 34/150 | Batch 161/198 | Loss: 1.5842843055725098\n","Epoch 34/150 | Batch 162/198 | Loss: 1.8611054420471191\n","Epoch 34/150 | Batch 163/198 | Loss: 1.5806719064712524\n","Epoch 34/150 | Batch 164/198 | Loss: 1.5462002754211426\n","Epoch 34/150 | Batch 165/198 | Loss: 1.602344036102295\n","Epoch 34/150 | Batch 166/198 | Loss: 1.5405187606811523\n","Epoch 34/150 | Batch 167/198 | Loss: 1.6297388076782227\n","Epoch 34/150 | Batch 168/198 | Loss: 1.7833304405212402\n","Epoch 34/150 | Batch 169/198 | Loss: 1.6218150854110718\n","Epoch 34/150 | Batch 170/198 | Loss: 1.6271666288375854\n","Epoch 34/150 | Batch 171/198 | Loss: 1.6780054569244385\n","Epoch 34/150 | Batch 172/198 | Loss: 1.6898595094680786\n","Epoch 34/150 | Batch 173/198 | Loss: 1.5088083744049072\n","Epoch 34/150 | Batch 174/198 | Loss: 1.617538571357727\n","Epoch 34/150 | Batch 175/198 | Loss: 1.6140267848968506\n","Epoch 34/150 | Batch 176/198 | Loss: 1.6486808061599731\n","Epoch 34/150 | Batch 177/198 | Loss: 1.5347119569778442\n","Epoch 34/150 | Batch 178/198 | Loss: 1.65286123752594\n","Epoch 34/150 | Batch 179/198 | Loss: 1.4451202154159546\n","Epoch 34/150 | Batch 180/198 | Loss: 1.4758442640304565\n","Epoch 34/150 | Batch 181/198 | Loss: 1.5798500776290894\n","Epoch 34/150 | Batch 182/198 | Loss: 1.7397360801696777\n","Epoch 34/150 | Batch 183/198 | Loss: 1.6953778266906738\n","Epoch 34/150 | Batch 184/198 | Loss: 1.7996095418930054\n","Epoch 34/150 | Batch 185/198 | Loss: 1.673697590827942\n","Epoch 34/150 | Batch 186/198 | Loss: 1.5939009189605713\n","Epoch 34/150 | Batch 187/198 | Loss: 1.5175057649612427\n","Epoch 34/150 | Batch 188/198 | Loss: 1.5323209762573242\n","Epoch 34/150 | Batch 189/198 | Loss: 1.5577830076217651\n","Epoch 34/150 | Batch 190/198 | Loss: 1.412274718284607\n","Epoch 34/150 | Batch 191/198 | Loss: 1.419468879699707\n","Epoch 34/150 | Batch 192/198 | Loss: 1.7430367469787598\n","Epoch 34/150 | Batch 193/198 | Loss: 1.556901454925537\n","Epoch 34/150 | Batch 194/198 | Loss: 1.6884979009628296\n","Epoch 34/150 | Batch 195/198 | Loss: 1.5331547260284424\n","Epoch 34/150 | Batch 196/198 | Loss: 1.628499150276184\n","Epoch 34/150 | Batch 197/198 | Loss: 1.514495611190796\n","Epoch 34/150 | Batch 198/198 | Loss: 1.8106213808059692\n","Epoch 34/150 | Average Loss: 1.6116757784226927\n","Epoch 35/150 | Batch 1/198 | Loss: 1.36947762966156\n","Epoch 35/150 | Batch 2/198 | Loss: 1.6853255033493042\n","Epoch 35/150 | Batch 3/198 | Loss: 1.462549090385437\n","Epoch 35/150 | Batch 4/198 | Loss: 1.7298433780670166\n","Epoch 35/150 | Batch 5/198 | Loss: 1.5358357429504395\n","Epoch 35/150 | Batch 6/198 | Loss: 1.6291041374206543\n","Epoch 35/150 | Batch 7/198 | Loss: 1.5126501321792603\n","Epoch 35/150 | Batch 8/198 | Loss: 1.3007311820983887\n","Epoch 35/150 | Batch 9/198 | Loss: 1.7196686267852783\n","Epoch 35/150 | Batch 10/198 | Loss: 1.3170305490493774\n","Epoch 35/150 | Batch 11/198 | Loss: 1.7100368738174438\n","Epoch 35/150 | Batch 12/198 | Loss: 1.6502832174301147\n","Epoch 35/150 | Batch 13/198 | Loss: 1.5041276216506958\n","Epoch 35/150 | Batch 14/198 | Loss: 1.6715998649597168\n","Epoch 35/150 | Batch 15/198 | Loss: 1.4388295412063599\n","Epoch 35/150 | Batch 16/198 | Loss: 1.6997710466384888\n","Epoch 35/150 | Batch 17/198 | Loss: 1.617066740989685\n","Epoch 35/150 | Batch 18/198 | Loss: 1.5589969158172607\n","Epoch 35/150 | Batch 19/198 | Loss: 1.5321844816207886\n","Epoch 35/150 | Batch 20/198 | Loss: 1.3685866594314575\n","Epoch 35/150 | Batch 21/198 | Loss: 1.5317225456237793\n","Epoch 35/150 | Batch 22/198 | Loss: 1.713867425918579\n","Epoch 35/150 | Batch 23/198 | Loss: 1.3639845848083496\n","Epoch 35/150 | Batch 24/198 | Loss: 1.2417749166488647\n","Epoch 35/150 | Batch 25/198 | Loss: 1.5298655033111572\n","Epoch 35/150 | Batch 26/198 | Loss: 1.5026531219482422\n","Epoch 35/150 | Batch 27/198 | Loss: 1.7502528429031372\n","Epoch 35/150 | Batch 28/198 | Loss: 1.7680840492248535\n","Epoch 35/150 | Batch 29/198 | Loss: 1.900209903717041\n","Epoch 35/150 | Batch 30/198 | Loss: 1.3869655132293701\n","Epoch 35/150 | Batch 31/198 | Loss: 1.4637386798858643\n","Epoch 35/150 | Batch 32/198 | Loss: 1.579967975616455\n","Epoch 35/150 | Batch 33/198 | Loss: 1.3087941408157349\n","Epoch 35/150 | Batch 34/198 | Loss: 1.4618122577667236\n","Epoch 35/150 | Batch 35/198 | Loss: 1.4156233072280884\n","Epoch 35/150 | Batch 36/198 | Loss: 1.5239248275756836\n","Epoch 35/150 | Batch 37/198 | Loss: 1.6203362941741943\n","Epoch 35/150 | Batch 38/198 | Loss: 1.5926319360733032\n","Epoch 35/150 | Batch 39/198 | Loss: 1.418817162513733\n","Epoch 35/150 | Batch 40/198 | Loss: 1.5171442031860352\n","Epoch 35/150 | Batch 41/198 | Loss: 1.5648828744888306\n","Epoch 35/150 | Batch 42/198 | Loss: 1.6686451435089111\n","Epoch 35/150 | Batch 43/198 | Loss: 1.6051112413406372\n","Epoch 35/150 | Batch 44/198 | Loss: 1.8513455390930176\n","Epoch 35/150 | Batch 45/198 | Loss: 1.4764790534973145\n","Epoch 35/150 | Batch 46/198 | Loss: 1.6139590740203857\n","Epoch 35/150 | Batch 47/198 | Loss: 1.4056445360183716\n","Epoch 35/150 | Batch 48/198 | Loss: 1.604004979133606\n","Epoch 35/150 | Batch 49/198 | Loss: 1.4431347846984863\n","Epoch 35/150 | Batch 50/198 | Loss: 1.6775444746017456\n","Epoch 35/150 | Batch 51/198 | Loss: 1.6523957252502441\n","Epoch 35/150 | Batch 52/198 | Loss: 1.7856417894363403\n","Epoch 35/150 | Batch 53/198 | Loss: 1.765499472618103\n","Epoch 35/150 | Batch 54/198 | Loss: 1.6005197763442993\n","Epoch 35/150 | Batch 55/198 | Loss: 1.5733052492141724\n","Epoch 35/150 | Batch 56/198 | Loss: 1.483939290046692\n","Epoch 35/150 | Batch 57/198 | Loss: 1.837679386138916\n","Epoch 35/150 | Batch 58/198 | Loss: 1.4922995567321777\n","Epoch 35/150 | Batch 59/198 | Loss: 1.6968833208084106\n","Epoch 35/150 | Batch 60/198 | Loss: 1.7083626985549927\n","Epoch 35/150 | Batch 61/198 | Loss: 1.544838309288025\n","Epoch 35/150 | Batch 62/198 | Loss: 1.4106574058532715\n","Epoch 35/150 | Batch 63/198 | Loss: 1.6226602792739868\n","Epoch 35/150 | Batch 64/198 | Loss: 1.5397658348083496\n","Epoch 35/150 | Batch 65/198 | Loss: 1.5353617668151855\n","Epoch 35/150 | Batch 66/198 | Loss: 1.5443707704544067\n","Epoch 35/150 | Batch 67/198 | Loss: 1.4815337657928467\n","Epoch 35/150 | Batch 68/198 | Loss: 1.720646619796753\n","Epoch 35/150 | Batch 69/198 | Loss: 1.621313452720642\n","Epoch 35/150 | Batch 70/198 | Loss: 1.5419368743896484\n","Epoch 35/150 | Batch 71/198 | Loss: 1.468313217163086\n","Epoch 35/150 | Batch 72/198 | Loss: 1.652828335762024\n","Epoch 35/150 | Batch 73/198 | Loss: 1.7430833578109741\n","Epoch 35/150 | Batch 74/198 | Loss: 1.687263011932373\n","Epoch 35/150 | Batch 75/198 | Loss: 1.7489792108535767\n","Epoch 35/150 | Batch 76/198 | Loss: 1.5536178350448608\n","Epoch 35/150 | Batch 77/198 | Loss: 1.6407791376113892\n","Epoch 35/150 | Batch 78/198 | Loss: 1.5744866132736206\n","Epoch 35/150 | Batch 79/198 | Loss: 1.668046236038208\n","Epoch 35/150 | Batch 80/198 | Loss: 1.5696805715560913\n","Epoch 35/150 | Batch 81/198 | Loss: 1.719678282737732\n","Epoch 35/150 | Batch 82/198 | Loss: 1.7597105503082275\n","Epoch 35/150 | Batch 83/198 | Loss: 1.6997610330581665\n","Epoch 35/150 | Batch 84/198 | Loss: 1.5985827445983887\n","Epoch 35/150 | Batch 85/198 | Loss: 1.6181442737579346\n","Epoch 35/150 | Batch 86/198 | Loss: 1.7471376657485962\n","Epoch 35/150 | Batch 87/198 | Loss: 1.6067333221435547\n","Epoch 35/150 | Batch 88/198 | Loss: 1.810562252998352\n","Epoch 35/150 | Batch 89/198 | Loss: 1.6976914405822754\n","Epoch 35/150 | Batch 90/198 | Loss: 1.722612977027893\n","Epoch 35/150 | Batch 91/198 | Loss: 1.6138466596603394\n","Epoch 35/150 | Batch 92/198 | Loss: 1.410043716430664\n","Epoch 35/150 | Batch 93/198 | Loss: 1.6688599586486816\n","Epoch 35/150 | Batch 94/198 | Loss: 1.5709885358810425\n","Epoch 35/150 | Batch 95/198 | Loss: 1.6219303607940674\n","Epoch 35/150 | Batch 96/198 | Loss: 1.475604772567749\n","Epoch 35/150 | Batch 97/198 | Loss: 1.7257966995239258\n","Epoch 35/150 | Batch 98/198 | Loss: 1.4699534177780151\n","Epoch 35/150 | Batch 99/198 | Loss: 1.5221058130264282\n","Epoch 35/150 | Batch 100/198 | Loss: 1.6675342321395874\n","Epoch 35/150 | Batch 101/198 | Loss: 1.643151879310608\n","Epoch 35/150 | Batch 102/198 | Loss: 1.6270599365234375\n","Epoch 35/150 | Batch 103/198 | Loss: 1.4618875980377197\n","Epoch 35/150 | Batch 104/198 | Loss: 1.3407504558563232\n","Epoch 35/150 | Batch 105/198 | Loss: 1.687459111213684\n","Epoch 35/150 | Batch 106/198 | Loss: 1.5096982717514038\n","Epoch 35/150 | Batch 107/198 | Loss: 1.7691432237625122\n","Epoch 35/150 | Batch 108/198 | Loss: 1.5216976404190063\n","Epoch 35/150 | Batch 109/198 | Loss: 1.5407946109771729\n","Epoch 35/150 | Batch 110/198 | Loss: 1.6396551132202148\n","Epoch 35/150 | Batch 111/198 | Loss: 1.4809114933013916\n","Epoch 35/150 | Batch 112/198 | Loss: 1.5471282005310059\n","Epoch 35/150 | Batch 113/198 | Loss: 1.4383493661880493\n","Epoch 35/150 | Batch 114/198 | Loss: 1.5551629066467285\n","Epoch 35/150 | Batch 115/198 | Loss: 1.6098828315734863\n","Epoch 35/150 | Batch 116/198 | Loss: 1.776747465133667\n","Epoch 35/150 | Batch 117/198 | Loss: 1.6050288677215576\n","Epoch 35/150 | Batch 118/198 | Loss: 1.652402400970459\n","Epoch 35/150 | Batch 119/198 | Loss: 1.3779077529907227\n","Epoch 35/150 | Batch 120/198 | Loss: 1.3233346939086914\n","Epoch 35/150 | Batch 121/198 | Loss: 1.498214840888977\n","Epoch 35/150 | Batch 122/198 | Loss: 1.6145813465118408\n","Epoch 35/150 | Batch 123/198 | Loss: 1.607214093208313\n","Epoch 35/150 | Batch 124/198 | Loss: 1.3881529569625854\n","Epoch 35/150 | Batch 125/198 | Loss: 1.5095218420028687\n","Epoch 35/150 | Batch 126/198 | Loss: 1.4795873165130615\n","Epoch 35/150 | Batch 127/198 | Loss: 1.6286208629608154\n","Epoch 35/150 | Batch 128/198 | Loss: 1.780460238456726\n","Epoch 35/150 | Batch 129/198 | Loss: 1.6240251064300537\n","Epoch 35/150 | Batch 130/198 | Loss: 1.6258960962295532\n","Epoch 35/150 | Batch 131/198 | Loss: 1.3872214555740356\n","Epoch 35/150 | Batch 132/198 | Loss: 1.7393312454223633\n","Epoch 35/150 | Batch 133/198 | Loss: 1.542851209640503\n"]}]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/Books/trained_model'\n","\n","# Save the trained model\n","model.save_pretrained(path)\n","\n","# Save the tokenizer\n","tokenizer.save_pretrained(\"path\")\n","\n","# Save the model configuration\n","model.config.save_pretrained(path)\n","\n","# Save other relevant components\n","torch.save(optimizer.state_dict(),path + \"/optimizer.pth\")\n","torch.save(scheduler.state_dict(), path+\"/scheduler.pth\")"],"metadata":{"id":"hphyqgTn7DIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.save_pretrained(\"/content/drive/MyDrive/Books/trained_model\")"],"metadata":{"id":"CRz2zFrwQWmM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(0, len(average_loss_p)):\n","    loss = str(average_loss_p[i])\n","    print(\"For Epoch {} Avg loss is {}\".format(i + 1, loss))"],"metadata":{"id":"gKoQZRn_G3B2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Testing"],"metadata":{"id":"xO5IZDUg5L1a"}},{"cell_type":"code","source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load the trained model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained('trained_model')\n","tokenizer = GPT2Tokenizer.from_pretrained('trained_model')\n","\n","# Set the device for inference\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Set the maximum length of generated text\n","max_length = 50\n","\n","# Set the prompt or input text\n","prompt = \"thanks\"\n","\n","# Tokenize the prompt\n","input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n","\n","# Generate text\n","output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n","\n","# Decode and print the generated text\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(\"Generated Text:\", generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0gBOvFFYeq0","executionInfo":{"status":"ok","timestamp":1687930428438,"user_tz":-330,"elapsed":3264,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"ec0b420c-6e3d-469e-98a0-3f8a927c2558"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Generated Text: thanks. you always end stories\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load the trained model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained('trained_model')\n","tokenizer = GPT2Tokenizer.from_pretrained('trained_model')\n","\n","# Set the device for inference\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Set the maximum length of generated text\n","max_length = 50\n","\n","# Set the prompt or input text\n","prompt = \"chat with me\"\n","\n","# Tokenize the prompt\n","input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n","\n","# Generate text\n","output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n","\n","# Decode and print the generated text\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(\"Generated Text:\", generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lQZK56vsdM_K","executionInfo":{"status":"ok","timestamp":1687930526514,"user_tz":-330,"elapsed":3024,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"30075b9f-b695-4280-d9b6-03cf2addc678"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Generated Text: chat with me for doing!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load the trained model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained('trained_model')\n","tokenizer = GPT2Tokenizer.from_pretrained('trained_model')\n","\n","# Set the device for inference\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Set the maximum length of generated text\n","max_length = 50\n","\n","# Set the prompt or input text\n","prompt = \"do you have any sentiments to find\"\n","\n","# Tokenize the prompt\n","input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n","\n","# Generate text\n","output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n","\n","# Decode and print the generated text\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(\"Generated Text:\", generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UbxuAJ4ifskv","executionInfo":{"status":"ok","timestamp":1687930573640,"user_tz":-330,"elapsed":3765,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"c29bffae-20f4-447a-c2e4-64eaa6162c3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Generated Text: do you have any sentiments to find!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load the trained model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained('trained_model')\n","tokenizer = GPT2Tokenizer.from_pretrained('trained_model')\n","\n","# Set the device for inference\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Set the maximum length of generated text\n","max_length = 50\n","\n","# Set the prompt or input text\n","prompt = \"describe a story for me on any new topic\"\n","\n","# Tokenize the prompt\n","input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n","\n","# Generate text\n","output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n","\n","# Decode and print the generated text\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(\"Generated Text:\", generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4iLZR5P6gPpk","executionInfo":{"status":"ok","timestamp":1687930654978,"user_tz":-330,"elapsed":3266,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"1138e4fa-93ba-47d9-e302-05e665ddd363"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Generated Text: describe a story for me on any new topic you could too to the the the the a the the gets is no to no that the no on be no to the do to women doctor on what to to to wear to! for to draw a\n"]}]},{"cell_type":"markdown","source":["\n","\n","```\n","from transformers import GPT2LMHeadModel\n","\n","model_path = \"trained_model\"  # Path to the saved model directory\n","\n","# Load the pretrained model\n","model = GPT2LMHeadModel.from_pretrained(model_path)\n","\n","# Access the model configuration\n","config = model.config\n","\n","# Print the values\n","print(\"Layers:\", config.num_hidden_layers)\n","print(\"d_model:\", config.hidden_size)\n","print(\"head_dim:\", config.n_head)\n","print(\"Vocabulary:\", config.vocab_size)\n","print(\"Sequence length:\", config.max_position_embeddings)\n","\n","```\n","\n"],"metadata":{"id":"TnwNEpeR5DSc"}},{"cell_type":"markdown","source":[" the code loads a pretrained GPT-2 model and fine-tunes it using the provided dataset. The model is trained to generate appropriate replies given user inputs. The code follows a typical training loop, optimizing the model's parameters using the Adam optimizer and adjusting the learning rate during training"],"metadata":{"id":"9owplTXOkLEh"}},{"cell_type":"markdown","source":["#Comparing GPT2 Vs Trained Model"],"metadata":{"id":"cGXboB40hvf-"}},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","# Load the trained model and tokenizer\n","trained_model = GPT2LMHeadModel.from_pretrained(\"trained_model\")\n","trained_tokenizer = GPT2Tokenizer.from_pretrained(\"trained_model\")\n","\n","# Load the original GPT-2 model and tokenizer\n","gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# Set the device for inference\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Set the models to the desired device\n","trained_model = trained_model.to(device)\n","gpt2_model = gpt2_model.to(device)\n","\n","# Set the maximum length for generated responses\n","max_length = 100\n","\n","# Set the input prompt\n","input_prompt = \"Hello, how are you?\"\n","\n","# Generate response using the trained model\n","input_ids = trained_tokenizer.encode(input_prompt, return_tensors=\"pt\").to(device)\n","trained_output = trained_model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n","trained_response = trained_tokenizer.decode(trained_output[0], skip_special_tokens=True)\n","\n","# Generate response using the GPT-2 model\n","input_ids = gpt2_tokenizer.encode(input_prompt, return_tensors=\"pt\").to(device)\n","gpt2_output = gpt2_model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n","gpt2_response = gpt2_tokenizer.decode(gpt2_output[0], skip_special_tokens=True)\n","\n","# Print the generated responses\n","print(\"Trained Model Response:\", trained_response)\n","print(\"GPT-2 Model Response:\", gpt2_response)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UkNpn0RugC8u","executionInfo":{"status":"ok","timestamp":1687931704084,"user_tz":-330,"elapsed":6057,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"68cebba3-42ef-4775-9727-516a5a9a7a8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Trained Model Response: Hello, how are you?!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","GPT-2 Model Response: Hello, how are you?\n","\n","I'm a little bit of a nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd\n"]}]},{"cell_type":"markdown","source":["#Version 2 AI\n","with additional data reddit conv's and qa dataset"],"metadata":{"id":"b_J9C9SfsPPC"}},{"cell_type":"code","source":["reddit_conv = pd.read_csv('/content/drive/MyDrive/Books/Ai Writting/Conversation/fixed_reddit.csv')\n","data3 = reddit_conv"],"metadata":{"id":"THx_SXggomyM","executionInfo":{"status":"ok","timestamp":1688034168267,"user_tz":-330,"elapsed":405,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["data3 = data3.drop('Unnamed: 0',axis = 1)\n","data3.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"cSR-WjSgtNcx","executionInfo":{"status":"ok","timestamp":1688034169624,"user_tz":-330,"elapsed":7,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"607eb78f-d7e0-47db-dc43-466cd1b5397b"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            Question  \\\n","0            What kind of phone(s) do you guys have?   \n","1       Does it really charge all the way in 15 min?   \n","2  Samsung Galaxy J1. It's my first cell phone an...   \n","3            What kind of phone(s) do you guys have?   \n","4                My friend told me to kill myself :/   \n","5                My friend told me to kill myself :/   \n","6        I just won a state championship for debate.   \n","7                                               Huh?   \n","8               So I got a girlfriend the other day?   \n","9               So I got a girlfriend the other day?   \n","\n","                                               Reply  \n","0  I have a pixel. It's pretty great. Much better...  \n","1            What kind of phone(s) do you guys have?  \n","2  What do you think of it? Anything you don't like?  \n","3                    LG Optimus V. I know, it's old.  \n","4                            Don't kill yourself OP.  \n","5     Fuck those losers, I'll /we'll be your friend   \n","6                                     No you didn't!  \n","7                                     No you didn't!  \n","8  CONGRATS! Hope you are as happy as you could p...  \n","9     That's so lovely!! Basically my gay-ass goals.  "],"text/html":["\n","  <div id=\"df-a35946b6-b1fd-4d93-82a9-41e2b9a0f53e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Reply</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What kind of phone(s) do you guys have?</td>\n","      <td>I have a pixel. It's pretty great. Much better...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Does it really charge all the way in 15 min?</td>\n","      <td>What kind of phone(s) do you guys have?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Samsung Galaxy J1. It's my first cell phone an...</td>\n","      <td>What do you think of it? Anything you don't like?</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What kind of phone(s) do you guys have?</td>\n","      <td>LG Optimus V. I know, it's old.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>My friend told me to kill myself :/</td>\n","      <td>Don't kill yourself OP.</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>My friend told me to kill myself :/</td>\n","      <td>Fuck those losers, I'll /we'll be your friend</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>I just won a state championship for debate.</td>\n","      <td>No you didn't!</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Huh?</td>\n","      <td>No you didn't!</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>So I got a girlfriend the other day?</td>\n","      <td>CONGRATS! Hope you are as happy as you could p...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>So I got a girlfriend the other day?</td>\n","      <td>That's so lovely!! Basically my gay-ass goals.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a35946b6-b1fd-4d93-82a9-41e2b9a0f53e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a35946b6-b1fd-4d93-82a9-41e2b9a0f53e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a35946b6-b1fd-4d93-82a9-41e2b9a0f53e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["\n","\n","```\n","import pandas as pd\n","\n","# Read the dataset\n","data = data3\n","\n","# Fix the dataset columns\n","fixed_data = []\n","num_rows = len(data)\n","\n","\n","for i in range(len(data)-1):\n","    if i % 2 == 0:\n","        question = data.iloc[i, 0]\n","        if i+1 < len(data):\n","            reply = data.iloc[i+1, 0]\n","            fixed_data.append((question, reply))\n","        else:\n","            break\n","\n","# Create a new DataFrame with the fixed data\n","fixed_reddit = pd.DataFrame(fixed_data, columns=[\"Question\", \"Reply\"])\n","\n","fixed_reddit.head()\n","```\n","\n"],"metadata":{"id":"ABDE4WTnwidh"}},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","import re\n","\n","def clean_text(text):\n","    # Remove leading/trailing spaces\n","    text = text.strip()\n","    # Remove HTML tags\n","    text = re.sub(r\"<.*?>\", \"\", text)\n","    # Remove URLs\n","    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n","    # Remove special characters or symbols\n","    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n","    # Normalize whitespace\n","    text = re.sub(r\"\\s+\", \" \", text)\n","    # Replace multiple consecutive punctuation marks with a single one\n","    text = re.sub(r\"'[\\/:;-_+@&!?$()<>.,@#%^&*]'\",\"\", text)\n","    # Replace numbers with a special token\n","    text = re.sub(r\"\\d+\", \"NUM\", text)\n","\n","    return text"],"metadata":{"id":"pgGz3dS7tDeQ","executionInfo":{"status":"ok","timestamp":1688034175407,"user_tz":-330,"elapsed":2,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Select relevant columns from the datasets\n","data1_selected = data[[\"User\", \"Reply\"]]\n","data2_selected = data3[[\"Question\", \"Reply\"]]\n","\n","# Join User and Question, Reply and Reply into single columns\n","data1_selected[\"text\"] = data1_selected[\"User\"]\n","data1_selected.loc[len(data1_selected)-1, \"text\"] += \" \" + data2_selected[\"Question\"].values[0]\n","data1_selected[\"response\"] = data1_selected[\"Reply\"]\n","data1_selected.loc[len(data1_selected)-1, \"response\"] += \" \" + data2_selected[\"Reply\"].values[0]\n","\n","# Drop unnecessary columns\n","data1_selected = data1_selected[[\"text\", \"response\"]]\n","\n","# Concatenate the datasets\n","new_combined_data = pd.concat([data1_selected, data2_selected.iloc[1:]])\n","\n","# Reset the index of the combined dataset\n","new_combined_data.reset_index(drop=True, inplace=True)\n","\n","# Replace NaN values with empty string\n","new_combined_data.fillna(\"\", inplace=True)\n","\n","# Print the combined dataset\n","new_combined_data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"0Szs1bMFxu4I","executionInfo":{"status":"ok","timestamp":1688034215662,"user_tz":-330,"elapsed":854,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"5e5a29bd-ff6b-456d-f5cc-417bb9e01dcd"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                  text  \\\n","0               hi, how are you doing?   \n","1        i'm fine. how about yourself?   \n","2  i'm pretty good. thanks for asking.   \n","3    no problem. so how have you been?   \n","4     i've been great. what about you?   \n","\n","                                   response Question Reply  \n","0             i'm fine. how about yourself?                 \n","1       i'm pretty good. thanks for asking.                 \n","2         no problem. so how have you been?                 \n","3          i've been great. what about you?                 \n","4  i've been good. i'm in school right now.                 "],"text/html":["\n","  <div id=\"df-77311cf9-da92-4682-90d7-559f6038be0d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>response</th>\n","      <th>Question</th>\n","      <th>Reply</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hi, how are you doing?</td>\n","      <td>i'm fine. how about yourself?</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i'm fine. how about yourself?</td>\n","      <td>i'm pretty good. thanks for asking.</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>i'm pretty good. thanks for asking.</td>\n","      <td>no problem. so how have you been?</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>no problem. so how have you been?</td>\n","      <td>i've been great. what about you?</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i've been great. what about you?</td>\n","      <td>i've been good. i'm in school right now.</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77311cf9-da92-4682-90d7-559f6038be0d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-77311cf9-da92-4682-90d7-559f6038be0d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-77311cf9-da92-4682-90d7-559f6038be0d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["new_combined_data['User'] = new_combined_data['text'] + new_combined_data['Question']\n","new_combined_data['Replies'] = new_combined_data['response'] + new_combined_data['Reply']"],"metadata":{"id":"S9NPgHidyi0u","executionInfo":{"status":"ok","timestamp":1688034215663,"user_tz":-330,"elapsed":13,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["cols = [\"text\",\"response\",\"Question\" ,\"Reply\"]\n","new_combined_data = new_combined_data.drop(cols,axis=1)\n","new_combined_data.to_csv('check_now.csv')"],"metadata":{"id":"hslKFVyrz1W5","executionInfo":{"status":"ok","timestamp":1688034216645,"user_tz":-330,"elapsed":625,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["new_combined_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XYHhyYu60e7s","executionInfo":{"status":"ok","timestamp":1688034220865,"user_tz":-330,"elapsed":417,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"4e7bbb28-a492-4235-8879-183c74e193ee"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(40810, 2)"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["new_combined_data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"IIvqhcPVtcO3","executionInfo":{"status":"ok","timestamp":1688034727821,"user_tz":-330,"elapsed":815,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"461582c3-9225-4580-e8aa-6ac7049718c4"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                  User  \\\n","0               hi, how are you doing?   \n","1        i'm fine. how about yourself?   \n","2  i'm pretty good. thanks for asking.   \n","3    no problem. so how have you been?   \n","4     i've been great. what about you?   \n","\n","                                    Replies  \n","0             i'm fine. how about yourself?  \n","1       i'm pretty good. thanks for asking.  \n","2         no problem. so how have you been?  \n","3          i've been great. what about you?  \n","4  i've been good. i'm in school right now.  "],"text/html":["\n","  <div id=\"df-e3e2e891-466d-4137-b98b-c9026ead517e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User</th>\n","      <th>Replies</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hi, how are you doing?</td>\n","      <td>i'm fine. how about yourself?</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i'm fine. how about yourself?</td>\n","      <td>i'm pretty good. thanks for asking.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>i'm pretty good. thanks for asking.</td>\n","      <td>no problem. so how have you been?</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>no problem. so how have you been?</td>\n","      <td>i've been great. what about you?</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i've been great. what about you?</td>\n","      <td>i've been good. i'm in school right now.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3e2e891-466d-4137-b98b-c9026ead517e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e3e2e891-466d-4137-b98b-c9026ead517e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e3e2e891-466d-4137-b98b-c9026ead517e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Read the CSV\n","data4 = pd.read_csv('/content/drive/MyDrive/Books/Ai Writting/Conversation/oasst1-train.csv')\n"],"metadata":{"id":"OEUU4koV5X27","executionInfo":{"status":"ok","timestamp":1688034272864,"user_tz":-330,"elapsed":2148,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["colss = [\"message_id\",\"parent_id\",\"user_id\",\"created_date\",\"review_count\",\n","         \"review_result\",\"deleted\",\"rank\",\"synthetic\",\"model_name\",\n","         \"detoxify\",\"message_tree_id\",\"tree_state\",\"labels\",\"emojis\"]\n","data4 = data4.drop(colss,axis=1)\n","# Filter the DataFrame where lang is en\n","data4 = data4[data4['lang'] == 'en']"],"metadata":{"id":"nmGXA29X8_2G","executionInfo":{"status":"ok","timestamp":1688034272864,"user_tz":-330,"elapsed":11,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["data4 = data4.drop(\"lang\",axis=1)\n","data4.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"h28pV1vyBopS","executionInfo":{"status":"ok","timestamp":1688034272864,"user_tz":-330,"elapsed":11,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"ecf3d68a-4553-4212-ad83-b5010a46fdf8"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                text       role\n","0  Can you write a short introduction about the r...   prompter\n","1  \"Monopsony\" refers to a market structure where...  assistant\n","2                            Now explain it to a dog   prompter\n","3  Monopsony is a market structure in which there...  assistant\n","4  How can one fight back when a monospony had be...   prompter"],"text/html":["\n","  <div id=\"df-9d622f6e-6936-4c84-8a72-d49ff28c49be\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>role</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Can you write a short introduction about the r...</td>\n","      <td>prompter</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\"Monopsony\" refers to a market structure where...</td>\n","      <td>assistant</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Now explain it to a dog</td>\n","      <td>prompter</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Monopsony is a market structure in which there...</td>\n","      <td>assistant</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>How can one fight back when a monospony had be...</td>\n","      <td>prompter</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d622f6e-6936-4c84-8a72-d49ff28c49be')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9d622f6e-6936-4c84-8a72-d49ff28c49be button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9d622f6e-6936-4c84-8a72-d49ff28c49be');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# Separate the text based on the roles and create new columns\n","assistant_text = data4.loc[data4['role'] == 'assistant', 'text'].reset_index(drop=True)\n","prompter_text = data4.loc[data4['role'] == 'prompter', 'text'].reset_index(drop=True)\n","\n","# Create a new DataFrame with assistant and prompter columns\n","data4_formated = pd.DataFrame({\n","    'assistant': assistant_text,\n","    'prompter': prompter_text\n","})\n","data4_formated.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"yPlWKoaErAVj","executionInfo":{"status":"ok","timestamp":1688034415292,"user_tz":-330,"elapsed":13,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"53384f97-4b9a-406e-c01f-68580cd52d88"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                           assistant  \\\n","0  \"Monopsony\" refers to a market structure where...   \n","1  Monopsony is a market structure in which there...   \n","2  Monopsony refers to a market structure where t...   \n","3  Here are some potential regulatory options to ...   \n","4  Regulatory intervention can be used to address...   \n","\n","                                            prompter  \n","0  Can you write a short introduction about the r...  \n","1                            Now explain it to a dog  \n","2  How can one fight back when a monospony had be...  \n","3  What can be done at a regulatory level to ensu...  \n","4  I would imagine this is similar or even the sa...  "],"text/html":["\n","  <div id=\"df-1d586258-4869-4479-bfe9-02b52c877bdd\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>assistant</th>\n","      <th>prompter</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"Monopsony\" refers to a market structure where...</td>\n","      <td>Can you write a short introduction about the r...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Monopsony is a market structure in which there...</td>\n","      <td>Now explain it to a dog</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Monopsony refers to a market structure where t...</td>\n","      <td>How can one fight back when a monospony had be...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Here are some potential regulatory options to ...</td>\n","      <td>What can be done at a regulatory level to ensu...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Regulatory intervention can be used to address...</td>\n","      <td>I would imagine this is similar or even the sa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d586258-4869-4479-bfe9-02b52c877bdd')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1d586258-4869-4479-bfe9-02b52c877bdd button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1d586258-4869-4479-bfe9-02b52c877bdd');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["data4_formated = data4_formated.dropna()\n","data4_formated.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Be44ohXJsQ0k","executionInfo":{"status":"ok","timestamp":1688034484403,"user_tz":-330,"elapsed":4,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"29183c11-2fa3-47f8-f410-d298bc183b62"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15200, 2)"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Select relevant columns from the datasets\n","data1_selected = new_combined_data[[\"User\", \"Replies\"]]\n","data2_selected = data4_formated[[\"prompter\",\"assistant\"]]\n","\n","# Join text and assistant, response and prompter into single columns\n","data1_selected[\"text\"] = data1_selected[\"User\"]\n","data1_selected.loc[len(data1_selected)-1, \"User\"] += \" \" + data2_selected[\"prompter\"].values[0]\n","data1_selected[\"Reply\"] = data1_selected[\"Replies\"]\n","data1_selected.loc[len(data1_selected)-1, \"Replies\"] += \" \" + data2_selected[\"assistant\"].values[0]\n","\n","\n","# Concatenate the datasets\n","new_combined_data = pd.concat([data1_selected, data2_selected.iloc[1:]])\n","\n","# Reset the index of the combined dataset\n","new_combined_data.reset_index(drop=True, inplace=True)\n","\n","# Replace NaN values with empty string\n","new_combined_data.fillna(\"\", inplace=True)"],"metadata":{"id":"Q-6jINEAsdCB","executionInfo":{"status":"ok","timestamp":1688034870254,"user_tz":-330,"elapsed":429,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["new_combined_data['text'] = new_combined_data['text'] + new_combined_data['prompter']\n","new_combined_data['Reply'] = new_combined_data['Reply'] + new_combined_data['assistant']"],"metadata":{"id":"i33p_L8-uCB-","executionInfo":{"status":"ok","timestamp":1688034980104,"user_tz":-330,"elapsed":444,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["cols = [\"User\",\"Replies\",\"prompter\",\"assistant\"]\n","new_combined_data = new_combined_data.drop(cols,axis=1)"],"metadata":{"id":"1EHVPtBxuatp","executionInfo":{"status":"ok","timestamp":1688035033358,"user_tz":-330,"elapsed":3,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["new_combined_data.tail()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"OE6iHf5funr1","executionInfo":{"status":"ok","timestamp":1688035046266,"user_tz":-330,"elapsed":9,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}},"outputId":"5c1a79d8-5616-4486-8521-69f8b4fb2ed3"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                    text  \\\n","56004  I don't see any factor regarding the energy ef...   \n","56005  Is there something that I, as a home owner, ca...   \n","56006  How can I best stay informed about market trends?   \n","56007  That's not really that helpful, can you elabor...   \n","56008  What are the risks when you do not carry out A...   \n","\n","                                                   Reply  \n","56004  just don't open your pc, take shower and focus...  \n","56005  You're welcome! Remember to create a conducive...  \n","56006                                 You're welcome! :)  \n","56007  NASA's current major goals are:\\n\\n1. Return h...  \n","56008  NASA, like most federal agencies, has to opera...  "],"text/html":["\n","  <div id=\"df-2b6153e6-71cd-4f7c-b145-89304d4993e5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>Reply</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>56004</th>\n","      <td>I don't see any factor regarding the energy ef...</td>\n","      <td>just don't open your pc, take shower and focus...</td>\n","    </tr>\n","    <tr>\n","      <th>56005</th>\n","      <td>Is there something that I, as a home owner, ca...</td>\n","      <td>You're welcome! Remember to create a conducive...</td>\n","    </tr>\n","    <tr>\n","      <th>56006</th>\n","      <td>How can I best stay informed about market trends?</td>\n","      <td>You're welcome! :)</td>\n","    </tr>\n","    <tr>\n","      <th>56007</th>\n","      <td>That's not really that helpful, can you elabor...</td>\n","      <td>NASA's current major goals are:\\n\\n1. Return h...</td>\n","    </tr>\n","    <tr>\n","      <th>56008</th>\n","      <td>What are the risks when you do not carry out A...</td>\n","      <td>NASA, like most federal agencies, has to opera...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b6153e6-71cd-4f7c-b145-89304d4993e5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2b6153e6-71cd-4f7c-b145-89304d4993e5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2b6153e6-71cd-4f7c-b145-89304d4993e5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["new_combined_data.to_csv(\"Final_OUT_Data_2.csv\")"],"metadata":{"id":"caiTHhe-uq8P","executionInfo":{"status":"ok","timestamp":1688035162417,"user_tz":-330,"elapsed":1026,"user":{"displayName":"GUNA SHANKAR","userId":"05449004328458363898"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NaEQvHMvu3c4"},"execution_count":null,"outputs":[]}]}