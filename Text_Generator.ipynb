{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "FYIk8qFskmsQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JHwnS3KGknT3",
        "outputId": "ad6db716-bde1-45d3-b45f-4536dc23d3e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read conversation data from CSV file\n",
        "conversations_df = pd.read_csv('/content/drive/MyDrive/BIG_DATA/Language Model/personality.csv')\n",
        "conversations_df = conversations_df.sample(n=800, random_state=42)\n",
        "speakers = conversations_df['Persona'].tolist()\n",
        "texts = conversations_df['chat'].tolist()\n",
        "\n",
        "# Tokenize the texts\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences using sliding window approach\n",
        "input_sequences = []\n",
        "for text in texts:\n",
        "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "    for i in range(1, len(sequence)):\n",
        "        n_gram_sequence = sequence[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_length = max([len(sequence) for sequence in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre'))\n",
        "\n",
        "# Split input sequences into input and target\n",
        "xs = input_sequences[:, :-1]\n",
        "labels = input_sequences[:, -1]\n",
        "ys = to_categorical(labels, num_classes=total_words)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 10, input_length=max_sequence_length-1))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(xs, ys, epochs=100, verbose=1)\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"Hello\"\n",
        "next_words = 50\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(\"Generated Text:\", seed_text)\n"
      ],
      "metadata": {
        "id": "YInikYpGkn8K",
        "outputId": "8a0b1c9e-9ea6-4a2f-ee86-89d136a42ba0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3700/3700 [==============================] - 129s 33ms/step - loss: 5.8534 - accuracy: 0.1112\n",
            "Epoch 2/100\n",
            "3700/3700 [==============================] - 59s 16ms/step - loss: 5.1489 - accuracy: 0.1797\n",
            "Epoch 3/100\n",
            "3700/3700 [==============================] - 55s 15ms/step - loss: 4.8630 - accuracy: 0.2032\n",
            "Epoch 4/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 4.6593 - accuracy: 0.2133\n",
            "Epoch 5/100\n",
            "3700/3700 [==============================] - 54s 15ms/step - loss: 4.4973 - accuracy: 0.2208\n",
            "Epoch 6/100\n",
            "3700/3700 [==============================] - 54s 15ms/step - loss: 4.3408 - accuracy: 0.2292\n",
            "Epoch 7/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 4.2075 - accuracy: 0.2343\n",
            "Epoch 8/100\n",
            "3700/3700 [==============================] - 55s 15ms/step - loss: 4.0863 - accuracy: 0.2403\n",
            "Epoch 9/100\n",
            "3700/3700 [==============================] - 54s 15ms/step - loss: 3.9762 - accuracy: 0.2470\n",
            "Epoch 10/100\n",
            "3700/3700 [==============================] - 55s 15ms/step - loss: 3.8731 - accuracy: 0.2543\n",
            "Epoch 11/100\n",
            "3700/3700 [==============================] - 54s 14ms/step - loss: 3.7790 - accuracy: 0.2603\n",
            "Epoch 12/100\n",
            "3700/3700 [==============================] - 54s 15ms/step - loss: 3.6925 - accuracy: 0.2690\n",
            "Epoch 13/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 3.6103 - accuracy: 0.2757\n",
            "Epoch 14/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 3.5358 - accuracy: 0.2862\n",
            "Epoch 15/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 3.4664 - accuracy: 0.2938\n",
            "Epoch 16/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 3.4030 - accuracy: 0.3015\n",
            "Epoch 17/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 3.3440 - accuracy: 0.3100\n",
            "Epoch 18/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 3.2897 - accuracy: 0.3167\n",
            "Epoch 19/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 3.2399 - accuracy: 0.3229\n",
            "Epoch 20/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 3.1931 - accuracy: 0.3298\n",
            "Epoch 21/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 3.1515 - accuracy: 0.3352\n",
            "Epoch 22/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 3.1103 - accuracy: 0.3416\n",
            "Epoch 23/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 3.0736 - accuracy: 0.3472\n",
            "Epoch 24/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 3.0397 - accuracy: 0.3516\n",
            "Epoch 25/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 3.0080 - accuracy: 0.3571\n",
            "Epoch 26/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.9777 - accuracy: 0.3613\n",
            "Epoch 27/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.9489 - accuracy: 0.3653\n",
            "Epoch 28/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.9209 - accuracy: 0.3704\n",
            "Epoch 29/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.8977 - accuracy: 0.3735\n",
            "Epoch 30/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.8739 - accuracy: 0.3767\n",
            "Epoch 31/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.8515 - accuracy: 0.3808\n",
            "Epoch 32/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.8314 - accuracy: 0.3837\n",
            "Epoch 33/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.8110 - accuracy: 0.3868\n",
            "Epoch 34/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.7921 - accuracy: 0.3906\n",
            "Epoch 35/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.7755 - accuracy: 0.3914\n",
            "Epoch 36/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.7577 - accuracy: 0.3953\n",
            "Epoch 37/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.7432 - accuracy: 0.3973\n",
            "Epoch 38/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.7305 - accuracy: 0.3986\n",
            "Epoch 39/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.7144 - accuracy: 0.4025\n",
            "Epoch 40/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.7020 - accuracy: 0.4030\n",
            "Epoch 41/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.6870 - accuracy: 0.4052\n",
            "Epoch 42/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.6755 - accuracy: 0.4068\n",
            "Epoch 43/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.6639 - accuracy: 0.4095\n",
            "Epoch 44/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.6508 - accuracy: 0.4110\n",
            "Epoch 45/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.6413 - accuracy: 0.4120\n",
            "Epoch 46/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.6321 - accuracy: 0.4142\n",
            "Epoch 47/100\n",
            "3700/3700 [==============================] - 54s 15ms/step - loss: 2.6195 - accuracy: 0.4172\n",
            "Epoch 48/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.6123 - accuracy: 0.4171\n",
            "Epoch 49/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.6050 - accuracy: 0.4182\n",
            "Epoch 50/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.5927 - accuracy: 0.4210\n",
            "Epoch 51/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.5853 - accuracy: 0.4233\n",
            "Epoch 52/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.5781 - accuracy: 0.4216\n",
            "Epoch 53/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.5681 - accuracy: 0.4237\n",
            "Epoch 54/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.5630 - accuracy: 0.4255\n",
            "Epoch 55/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.5558 - accuracy: 0.4252\n",
            "Epoch 56/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.5503 - accuracy: 0.4278\n",
            "Epoch 57/100\n",
            "3700/3700 [==============================] - 54s 15ms/step - loss: 2.5408 - accuracy: 0.4283\n",
            "Epoch 58/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.5326 - accuracy: 0.4287\n",
            "Epoch 59/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.5290 - accuracy: 0.4308\n",
            "Epoch 60/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.5244 - accuracy: 0.4316\n",
            "Epoch 61/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.5187 - accuracy: 0.4319\n",
            "Epoch 62/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.5141 - accuracy: 0.4323\n",
            "Epoch 63/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.5086 - accuracy: 0.4323\n",
            "Epoch 64/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4991 - accuracy: 0.4361\n",
            "Epoch 65/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.4978 - accuracy: 0.4345\n",
            "Epoch 66/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4924 - accuracy: 0.4361\n",
            "Epoch 67/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.4877 - accuracy: 0.4364\n",
            "Epoch 68/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4813 - accuracy: 0.4369\n",
            "Epoch 69/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4930 - accuracy: 0.4353\n",
            "Epoch 70/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4694 - accuracy: 0.4393\n",
            "Epoch 71/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4681 - accuracy: 0.4387\n",
            "Epoch 72/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.4687 - accuracy: 0.4383\n",
            "Epoch 73/100\n",
            "3700/3700 [==============================] - 54s 15ms/step - loss: 2.4604 - accuracy: 0.4400\n",
            "Epoch 74/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4565 - accuracy: 0.4408\n",
            "Epoch 75/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4561 - accuracy: 0.4416\n",
            "Epoch 76/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4479 - accuracy: 0.4422\n",
            "Epoch 77/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4442 - accuracy: 0.4429\n",
            "Epoch 78/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4405 - accuracy: 0.4426\n",
            "Epoch 79/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.4394 - accuracy: 0.4434\n",
            "Epoch 80/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4405 - accuracy: 0.4436\n",
            "Epoch 81/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4368 - accuracy: 0.4441\n",
            "Epoch 82/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4275 - accuracy: 0.4459\n",
            "Epoch 83/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.4288 - accuracy: 0.4455\n",
            "Epoch 84/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4197 - accuracy: 0.4467\n",
            "Epoch 85/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4196 - accuracy: 0.4474\n",
            "Epoch 86/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4176 - accuracy: 0.4471\n",
            "Epoch 87/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.4162 - accuracy: 0.4470\n",
            "Epoch 88/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.4129 - accuracy: 0.4482\n",
            "Epoch 89/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.4098 - accuracy: 0.4474\n",
            "Epoch 90/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.4094 - accuracy: 0.4484\n",
            "Epoch 91/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.4075 - accuracy: 0.4479\n",
            "Epoch 92/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.4004 - accuracy: 0.4491\n",
            "Epoch 93/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.3948 - accuracy: 0.4501\n",
            "Epoch 94/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.3975 - accuracy: 0.4496\n",
            "Epoch 95/100\n",
            "3700/3700 [==============================] - 54s 15ms/step - loss: 2.3997 - accuracy: 0.4494\n",
            "Epoch 96/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.3925 - accuracy: 0.4503\n",
            "Epoch 97/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.3931 - accuracy: 0.4510\n",
            "Epoch 98/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.3893 - accuracy: 0.4517\n",
            "Epoch 99/100\n",
            "3700/3700 [==============================] - 52s 14ms/step - loss: 2.3893 - accuracy: 0.4516\n",
            "Epoch 100/100\n",
            "3700/3700 [==============================] - 53s 14ms/step - loss: 2.3865 - accuracy: 0.4516\n",
            "1/1 [==============================] - 0s 354ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "Generated Text: Hello how are you today i am good how are you i am doing great just got back from a run u are doing a accident person i am a factory at a club that is sweet my parents are happy i am a student i am sorry but i have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "seed_text = \"Hello, do you know me\"\n",
        "next_words = 50\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(\"Generated Text:\", seed_text)"
      ],
      "metadata": {
        "id": "-DJ3dfzLlueU",
        "outputId": "b74e8762-9ec6-41dc-de13-1c69ea169470",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Generated Text: Hello, do you know me just bring some place my cats and two kids hello i am a student i am in school i am a nerd i am sorry to hear that i have been to the way every day i am sorry i am not studying i am sorry i am a lawyer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('trained_model.h5')"
      ],
      "metadata": {
        "id": "m_Chx82-rvjq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbAOYDwuA87x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}